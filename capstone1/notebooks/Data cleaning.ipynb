{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.impute import SimpleImputer\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rc(\"text\", usetex=True)\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from re import search\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_singular_columns(df):\n",
    "    #Determine the number of samples with the most frequent value; if it equals the number of samples, then drop this column.\n",
    "    columns = df.columns.values\n",
    "    columns_to_drop = columns[[len(df)==df[col].value_counts().max() for col in columns]]\n",
    "    print('Dropping', columns_to_drop, 'column(s) from DataFrame.')\n",
    "    return df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-stage recommendation system for capital recovery: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to create two predictive models that combine to form a multi-stage recommendation system\n",
    "for the preservation and or recovery of capital. The first model is a classification model that predicts whether or not a loan\n",
    "will be \"Charged-off\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three stages of data cleaning\n",
    "\n",
    "    1. Ensure the integrity of the data for use in exploratory data analysis (EDA)\n",
    "    2. Subset and process the data relevant for the loan-outcome classification problem\n",
    "    3. Subset and process the data relevant for the recovery-amount regression problem\n",
    "    \n",
    "This notebook is focused on cleaning and selecting subsets of the data for different purposes. Any preprocessing\n",
    "required for application of ML algorithms is left for separate notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data cleaning for use in exploratory data analysis (EDA)\n",
    "\n",
    "This doesn't require much as we simply want to ensure the integrity of the data; the data comes in '.csv' format\n",
    "so let us see if it can be imported into a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3051: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loan_data = pd.read_csv('loan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas allows for features (columns) to have non-uniform data types, represented by the general category 'object' dtype.\n",
    "Still, we want the dtypes to be uniform to ensure that any manipulations or transformations later on do not cause errors.\n",
    "\n",
    "An example of a problem that could arise: imagine a feature includes strings and lists of strings. Attempting to apply\n",
    "a string based operation such as Python's \"split()\" method would return an error, as lists do not have the aforementioned attribute. \n",
    "\n",
    "Need to deal with mixed dtype columns to make the data consistent and  but first let's see if any of these columns survive the initial processing to remove\n",
    "columns with majority of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names from DTypeWarning. Need to clean by converting to one data type. \n",
    "mixed_dtype_column_names = loan_data.columns[[19,47,55,112,123,124,125,128,129,130,133,139,140,141]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to ascertain the cause of this mixed dtype error, let's look at some of the values in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['desc', 'next_pymnt_d', 'verification_status_joint',\n",
       "       'sec_app_earliest_cr_line', 'hardship_type', 'hardship_reason',\n",
       "       'hardship_status', 'hardship_start_date', 'hardship_end_date',\n",
       "       'payment_plan_start_date', 'hardship_loan_status',\n",
       "       'debt_settlement_flag_date', 'settlement_status', 'settlement_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_data.columns[[19,47,55,112,123,124,125,128,129,130,133,139,140,141]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>verification_status_joint</th>\n",
       "      <th>sec_app_earliest_cr_line</th>\n",
       "      <th>hardship_type</th>\n",
       "      <th>hardship_reason</th>\n",
       "      <th>hardship_status</th>\n",
       "      <th>hardship_start_date</th>\n",
       "      <th>hardship_end_date</th>\n",
       "      <th>payment_plan_start_date</th>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2260658</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260659</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260660</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260661</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260662</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260663</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Apr-2003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260665</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260666</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260667</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar-2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        desc next_pymnt_d verification_status_joint sec_app_earliest_cr_line  \\\n",
       "2260658  NaN          NaN                       NaN                      NaN   \n",
       "2260659  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260660  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260661  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260662  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260663  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260664  NaN          NaN              Not Verified                 Apr-2003   \n",
       "2260665  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260666  NaN     Mar-2019                       NaN                      NaN   \n",
       "2260667  NaN     Mar-2019                       NaN                      NaN   \n",
       "\n",
       "        hardship_type hardship_reason hardship_status hardship_start_date  \\\n",
       "2260658           NaN             NaN             NaN                 NaN   \n",
       "2260659           NaN             NaN             NaN                 NaN   \n",
       "2260660           NaN             NaN             NaN                 NaN   \n",
       "2260661           NaN             NaN             NaN                 NaN   \n",
       "2260662           NaN             NaN             NaN                 NaN   \n",
       "2260663           NaN             NaN             NaN                 NaN   \n",
       "2260664           NaN             NaN             NaN                 NaN   \n",
       "2260665           NaN             NaN             NaN                 NaN   \n",
       "2260666           NaN             NaN             NaN                 NaN   \n",
       "2260667           NaN             NaN             NaN                 NaN   \n",
       "\n",
       "        hardship_end_date payment_plan_start_date hardship_loan_status  \\\n",
       "2260658               NaN                     NaN                  NaN   \n",
       "2260659               NaN                     NaN                  NaN   \n",
       "2260660               NaN                     NaN                  NaN   \n",
       "2260661               NaN                     NaN                  NaN   \n",
       "2260662               NaN                     NaN                  NaN   \n",
       "2260663               NaN                     NaN                  NaN   \n",
       "2260664               NaN                     NaN                  NaN   \n",
       "2260665               NaN                     NaN                  NaN   \n",
       "2260666               NaN                     NaN                  NaN   \n",
       "2260667               NaN                     NaN                  NaN   \n",
       "\n",
       "        debt_settlement_flag_date settlement_status settlement_date  \n",
       "2260658                       NaN               NaN             NaN  \n",
       "2260659                       NaN               NaN             NaN  \n",
       "2260660                       NaN               NaN             NaN  \n",
       "2260661                       NaN               NaN             NaN  \n",
       "2260662                       NaN               NaN             NaN  \n",
       "2260663                       NaN               NaN             NaN  \n",
       "2260664                       NaN               NaN             NaN  \n",
       "2260665                       NaN               NaN             NaN  \n",
       "2260666                       NaN               NaN             NaN  \n",
       "2260667                       NaN               NaN             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_data.loc[:, mixed_dtype_column_names].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the mixed dtypes are likely due to a mixture of NumPy's NaN value with strings. \n",
    "Because the features are in essence categorical, the NaN values can be replaced with a new category, 'Missing';\n",
    "this applies to all object dtype columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to eventually reorder the data by date and it doesn't affect any calculations so it's done now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "issued_datetime = pd.to_datetime(loan_data.issue_d)\n",
    "loan_data = loan_data.loc[issued_datetime.sort_values().index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = loan_data.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the guess for the reason for mixed dtypes was right,\n",
    "check and see that all of these columns with NaN being replaced can undergo a string\n",
    "concatenation operation; I believe this ensure that all elements are strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_status</th>\n",
       "      <th>hardship_start_date</th>\n",
       "      <th>hardship_end_date</th>\n",
       "      <th>payment_plan_start_date</th>\n",
       "      <th>hardship_loan_status</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2142001</th>\n",
       "      <td>36 months</td>\n",
       "      <td>C</td>\n",
       "      <td>C3</td>\n",
       "      <td>Stanford University Libraries, LOCKSS Project</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Jun-2007</td>\n",
       "      <td>Does not meet the credit policy. Status:Fully ...</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142017</th>\n",
       "      <td>36 months</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Jun-2007</td>\n",
       "      <td>Does not meet the credit policy. Status:Fully ...</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142016</th>\n",
       "      <td>36 months</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>Halping hands company inc.</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Jun-2007</td>\n",
       "      <td>Does not meet the credit policy. Status:Fully ...</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142015</th>\n",
       "      <td>36 months</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Jun-2007</td>\n",
       "      <td>Does not meet the credit policy. Status:Fully ...</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142014</th>\n",
       "      <td>36 months</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Jun-2007</td>\n",
       "      <td>Does not meet the credit policy. Status:Charge...</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               term grade sub_grade  \\\n",
       "2142001   36 months     C        C3   \n",
       "2142017   36 months     B        B3   \n",
       "2142016   36 months     B        B4   \n",
       "2142015   36 months     C        C1   \n",
       "2142014   36 months     B        B4   \n",
       "\n",
       "                                             emp_title emp_length  \\\n",
       "2142001  Stanford University Libraries, LOCKSS Project   < 1 year   \n",
       "2142017                                            NaN   < 1 year   \n",
       "2142016                     Halping hands company inc.   < 1 year   \n",
       "2142015                                            NaN   < 1 year   \n",
       "2142014                                      Air Force   < 1 year   \n",
       "\n",
       "        home_ownership verification_status   issue_d  \\\n",
       "2142001           RENT        Not Verified  Jun-2007   \n",
       "2142017           RENT        Not Verified  Jun-2007   \n",
       "2142016           RENT        Not Verified  Jun-2007   \n",
       "2142015           RENT        Not Verified  Jun-2007   \n",
       "2142014           RENT        Not Verified  Jun-2007   \n",
       "\n",
       "                                               loan_status pymnt_plan  ...  \\\n",
       "2142001  Does not meet the credit policy. Status:Fully ...          n  ...   \n",
       "2142017  Does not meet the credit policy. Status:Fully ...          n  ...   \n",
       "2142016  Does not meet the credit policy. Status:Fully ...          n  ...   \n",
       "2142015  Does not meet the credit policy. Status:Fully ...          n  ...   \n",
       "2142014  Does not meet the credit policy. Status:Charge...          n  ...   \n",
       "\n",
       "        hardship_status hardship_start_date hardship_end_date  \\\n",
       "2142001             NaN                 NaN               NaN   \n",
       "2142017             NaN                 NaN               NaN   \n",
       "2142016             NaN                 NaN               NaN   \n",
       "2142015             NaN                 NaN               NaN   \n",
       "2142014             NaN                 NaN               NaN   \n",
       "\n",
       "        payment_plan_start_date hardship_loan_status disbursement_method  \\\n",
       "2142001                     NaN                  NaN                Cash   \n",
       "2142017                     NaN                  NaN                Cash   \n",
       "2142016                     NaN                  NaN                Cash   \n",
       "2142015                     NaN                  NaN                Cash   \n",
       "2142014                     NaN                  NaN                Cash   \n",
       "\n",
       "        debt_settlement_flag debt_settlement_flag_date settlement_status  \\\n",
       "2142001                    N                       NaN               NaN   \n",
       "2142017                    N                       NaN               NaN   \n",
       "2142016                    N                       NaN               NaN   \n",
       "2142015                    N                       NaN               NaN   \n",
       "2142014                    N                       NaN               NaN   \n",
       "\n",
       "        settlement_date  \n",
       "2142001             NaN  \n",
       "2142017             NaN  \n",
       "2142016             NaN  \n",
       "2142015             NaN  \n",
       "2142014             NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_data.apply(lambda x : x+'').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we investigate any required manipulations of the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = loan_data.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 108666955 missing values\n",
      "This represents 33.15% of the data set.\n"
     ]
    }
   ],
   "source": [
    "# Summation of all missing values\n",
    "number_of_missing_values = loan_data.isna().sum().sum()\n",
    "print('There are {} missing values'.format(number_of_missing_values))\n",
    "print('This represents {0:.2f}% of the data set.'.format(100*number_of_missing_values/loan_data.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were not any importation warnings in regards to the numerical columns; but because the categorical variables were missing values it is likely that the numerical data is also missing values. First let's see how many values are indeed missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78158211 missing values\n",
      "This represents 31.72% of this data.\n"
     ]
    }
   ],
   "source": [
    "# Summation of all missing values\n",
    "number_of_missing_values = numerical_data.isna().sum().sum()\n",
    "print('There are {} missing values'.format(number_of_missing_values))\n",
    "print('This represents {0:.2f}% of this data.'.format(100*number_of_missing_values/numerical_data.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting question, how are these missing variables distributed? Is it uniform across all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of the percentage of values that are missing per feature as a monotonically decreasing sequence\n",
    "percentnan = numerical_data.isna().sum().sort_values(ascending=False) / len(numerical_data)\n",
    "plt.plot(100*percentnan.values, marker='.',linestyle='none')\n",
    "plt.title('Percentage of values that are missing per feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of features are missing values; this is a problem for some of scikit-learn's machine learning algorithms we\n",
    "want to use; therefore, we need to figure out how to deal with them. We can not simply drop all rows (samples) which have missing values as this would leave us with exactly 0 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Columns: 109 entries, id to settlement_term\n",
      "dtypes: float64(105), int64(4)\n",
      "memory usage: 0.0 bytes\n"
     ]
    }
   ],
   "source": [
    "numerical_data.dropna().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns missing literally every value are worthless; so we drop them. There would be an argument to make that perhaps this is representative of sampling bias but because the sample is so large this is almost surely not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           100.0\n",
       "member_id    100.0\n",
       "url          100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_percentages = 100 * numerical_data.isna().sum() / len(numerical_data)\n",
    "missing_value_percentages[missing_value_percentages==100.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = categorical_data.fillna(value='Missing')\n",
    "loan_data = pd.concat((numerical_data, categorical_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = loan_data.drop(columns=['id','member_id','url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A similar situation is the columns which only have a single value; note that Pandas does not consider missing values as a \"value\" so a distinction needs to be made between literally every sample having the same value features containing a single value and missing values.\n",
    "\n",
    "The distinction between these two types is motivated by the concept of sampling bias. When we train our models with columns which follow the same pattern as 'policy_code', the model will only train on a single value and as such won't be good models for new test data with different values. In other words, the training domain doesn't encapsulate the testing domain and as such the model will have trouble with such predictions.\n",
    "\n",
    "First, determine and drop the columns which only have a single unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping ['policy_code'] column(s) from DataFrame.\n"
     ]
    }
   ],
   "source": [
    "loan_data = drop_singular_columns(loan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also certain data whose purpose is to merely contribute noise to the data set. These features have a large number of personalized responses; creating categorical data with a large number of categories. This information could be\n",
    "encoded into a sparse matrix, or tools from natural language processing (NLP) could be applied, but we reject these choices as\n",
    "they do not seem very efficient or useful at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reason to do this is because later in the cross validation stage of the modeling process, we would like to only have variables where we know their categories a priori so that they can be encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idiosyncratic_columns = ['emp_title', 'desc', 'title']\n",
    "loan_data = loan_data.drop(columns=idiosyncratic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still missing values in the dataset; but the approach with which to replace these values depends on the calculation\n",
    "being performed and so these values are kept until they need to be replaced or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.to_csv('loan_data_eda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleaning and preprocessing for loan outcome classification problem.\n",
    "\n",
    "The current formulation of this problem is to predict whether or not a loan will be charged off by the time it matures. Therefore, we separate out the data for the loans that have already matured by comparing the most recent date in the dataset\n",
    "to the maturity date. The maturity date is equal to the date the loan was issued added to the length of the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to convert the loan status into a binary variable; label the loans which have been charged off as positive\n",
    "outcome and otherwise as negative. This begs the question; should the data be separated into loans that have past their\n",
    "maturity date? The outcome of a current loan is a nonsensical quantity. Because the answer is not known let's account for both cases. Save the index when subsetting the data so other dataframes can be sliced equivalently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Term' from string to integer years; strings are either ' 36 Months' or ' 60 Months' (note the leading whitespace)\n",
    "terms_num = categorical_data.term.str.split(' ').apply(lambda x: int(x[1])//12)\n",
    "\n",
    "issued_date = pd.to_datetime(categorical_data.issue_d)\n",
    "maturity_date = issued_date.copy()\n",
    "\n",
    "# Select the indices for 3 and 5 year loans\n",
    "year3index = terms_num[terms_num.values==3].index\n",
    "year5index = terms_num[terms_num.values==5].index\n",
    "\n",
    "# Maturity Date = Term + Issued date \n",
    "maturity_date.loc[year3index] = issued_date.loc[year3index] + pd.DateOffset(years=3)\n",
    "maturity_date.loc[year5index] = issued_date.loc[year5index] + pd.DateOffset(years=5)\n",
    "\n",
    "# The matured loans are those whose maturity date is less than the most recent date, December 1st, 2018.\n",
    "matured_loan_data = loan_data.loc[maturity_date[maturity_date < pd.to_datetime('2018-12-02')].index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = matured_loan_data[matured_loan_data.loan_status.isin(['Charged Off','Fully Paid'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can safely overwrite/transform values to binary that we want for logistic regression because we have made a copy specifically\n",
    "for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_loan_status = classification_data.loan_status.map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "classification_data = classification_data.assign(loan_status=binary_loan_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a subset of the data was taken; ensure that there are no features with a single value again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping ['out_prncp' 'out_prncp_inv' 'pymnt_plan' 'next_pymnt_d'\n",
      " 'sec_app_earliest_cr_line' 'hardship_flag' 'disbursement_method'] column(s) from DataFrame.\n"
     ]
    }
   ],
   "source": [
    "classification_data = drop_singular_columns(classification_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are trying to predict whether or not to *issue* a loan, only data that is available at the time of\n",
    "that decision can be used in the creation of the predictive model. This will exclude variables which aggregate data\n",
    "from the past X months from the present date. The metadata or explanation of the variables typically indicates when the quantity involves time, so we take advantage of this by keeping anything that doesn't include terms like \"current\" or \"past X months\", etc. \n",
    "\n",
    "The list of variables included in the modeling process:\n",
    "    \n",
    "    1. Loan amount\n",
    "    2. Funded amount\n",
    "    3. Funded amount from investors\n",
    "    4. Interest rate\n",
    "    5. Installment\n",
    "    6. Annual income\n",
    "    7. Term\n",
    "    8. Grade\n",
    "    9. Sub-grade\n",
    "    10. Length of time at current employer\n",
    "    11. Home ownership status\n",
    "    12. Verification status\n",
    "    13. Issuance date\n",
    "    14. Purpose\n",
    "    15. Zip code\n",
    "    16. State of residence\n",
    "    17. Earliest date of recorded credit line\n",
    "    18. Initial listing status\n",
    "    19. Application type\n",
    "    20. Joint verification status\n",
    "    \n",
    "And of course, the target variable; the outcome of the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_nan = classification_data.isna().sum().sort_values(ascending=False)[:10] / len(classification_data)\n",
    "percent_nan_to_remove = percent_nan[percent_nan > 0.99].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sec_app_inq_last_6mths', 'revol_bal_joint',\n",
       "       'sec_app_collections_12_mths_ex_med',\n",
       "       'sec_app_mths_since_last_major_derog', 'sec_app_open_act_il',\n",
       "       'sec_app_revol_util', 'sec_app_open_acc', 'sec_app_mort_acc',\n",
       "       'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_nan_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = classification_data.loc[:, ~classification_data.columns.isin(percent_nan_to_remove.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dti_joint                                     0.999648\n",
       "annual_inc_joint                              0.999647\n",
       "orig_projected_additional_accrued_interest    0.998427\n",
       "hardship_payoff_balance_amount                0.998053\n",
       "hardship_last_payment_amount                  0.998053\n",
       "hardship_length                               0.998053\n",
       "hardship_amount                               0.998053\n",
       "deferral_term                                 0.998053\n",
       "hardship_dpd                                  0.998053\n",
       "settlement_amount                             0.984423\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_data.isna().sum().sort_values(ascending=False)[:10] / len(classification_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metadata isn't exactly clear when these feature values are recorded.\n",
    "assumed_to_be_time_independent = ['total_rev_hi_lim','total_il_high_credit_limit','total_bc_limit',\n",
    "                         'total_bal_ex_mort','tax_liens','revol_util','revol_bal','pub_rec_bankruptcies',\n",
    "                         'pub_rec','percent_bc_gt_75','open_acc','num_sats','num_rev_accts','num_op_rev_tl',\n",
    "                         'num_il_tl','num_bc_tl','num_bc_sats','mort_acc','dti']\n",
    "                         \n",
    "# \n",
    "time_independent =  ['loan_amnt', 'funded_amnt','funded_amnt_inv', 'int_rate', 'installment',\n",
    "                        'annual_inc', 'term', 'grade', 'sub_grade', 'emp_length', 'home_ownership',\n",
    "                        'verification_status', 'issue_d', 'purpose', 'zip_code', 'addr_state',\n",
    "                        'earliest_cr_line', 'initial_list_status', 'application_type', 'verification_status_joint',\n",
    "                        'loan_status']\n",
    "                                  \n",
    "modeling_data_columns = assumed_to_be_time_independent + time_independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_data_modeling = classification_data[modeling_data_columns]\n",
    "classification_data_ind = classification_data[time_independent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be much merit in keeping the joint verification status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data_modeling.dropna(axis=0).to_csv('classification_loan_data.csv',index=False)\n",
    "classification_data_ind.dropna(axis=0).to_csv('classification_loan_data_ind.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I need to do: one hot encoding for categorical variables, after taking care of features with large number of categories.\n",
    "Modify hypothesis, predict charged off loans by maturity date. \n",
    "\n",
    "The problem stated earlier with missing values; turns out that with respect to matured loans are the ones missing all\n",
    "values with respect to some features. \n",
    "\n",
    "I.e. solving the problem of not wanting to drop data because its easier to justify when only looking at charged-off loans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data cleaning and preprocessing in capital recovery regression problem.\n",
    "\n",
    "The current formulation of this problem is to form a model which predicts the amount of money that can be recovered from charged off loans; this only requires slicing the charged off loans; Missing values will be handled in ML notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "charged_off_loans = loan_data[loan_data.loan_status=='Charged Off']\n",
    "charged_off_loans = charged_off_loans.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a subset of the data was taken; ensure that there are no features with a single value again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping ['out_prncp' 'out_prncp_inv' 'loan_status' 'pymnt_plan' 'next_pymnt_d'] column(s) from DataFrame.\n"
     ]
    }
   ],
   "source": [
    "charged_off_loans = drop_singular_columns(charged_off_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_loans.to_csv('regression_loan_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
