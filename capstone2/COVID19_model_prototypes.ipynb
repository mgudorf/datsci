{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prototyping\n",
    "\n",
    "The goal of this notebook is to get a feel for the performance of the two different types of models to be trained.\n",
    "Specifically, comparison will be made between a naive baseline model, a ridge regression model, and a convolutional neural network model. Because this notebook is simply prototyping, a very small subset of the available data will be used; a single \n",
    "feature's time series for a single country, the United States. This single feature is all that will be available to the CNN and Ridge regression models; because eventually the feature data used in the regression is much larger than that used in the CNN training, this might be an unfair comparison.\n",
    "\n",
    "The main issue for using a very small subset of data is that I believe it will affect the CNN more than the regression, because\n",
    "the small number of samples influences how well the specific architecture will perform. Regardless I will press on and continue this testing. \n",
    "\n",
    "Both will use mean squared error as their loss function, unfortunately this means that I have to use RidgeCV, which means I need to provide my own folds in order to respect time ordering.\n",
    "\n",
    "Much like how in regression we want to include multiple days of information for prediction,  it may also be benefitial to convolve multiple frames together for the CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    print(y_true.shape, y_naive.shape, y_predict.shape)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        ax1.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_naive+1))\n",
    "        ax2.plot(xrange, np.log(y_true+1)\n",
    "                 -np.log(y_predict+1))\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        ax1.plot(xrange, y_true-y_naive)\n",
    "        ax2.plot(xrange, y_true-y_predict)\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def normalize_Xy_splits(splits, feature_range=(0., 0.5), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 0.5)\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    for i in range(1, X_train.shape[0]+1):\n",
    "        # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "        # each time step in the frame. \n",
    "        up_to_current_frame_min = X_train[:i,:,:,:].min((0,1,2))\n",
    "        up_to_current_frame_max = X_train[:i,:,:,:].max((0,1,2))\n",
    "        latest_min_array = np.tile(up_to_current_frame_min[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "        latest_max_array = np.tile(up_to_current_frame_max[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "        if i == 1:\n",
    "            frame_min_array = latest_min_array\n",
    "            frame_max_array = latest_max_array\n",
    "        else:\n",
    "            frame_min_array = np.concatenate((frame_min_array, \n",
    "                                                   latest_min_array)\n",
    "                                                  ,axis=0)\n",
    "            frame_max_array = np.concatenate((frame_max_array, \n",
    "                                                   latest_max_array)\n",
    "                                                  ,axis=0)\n",
    "\n",
    "    # frame_min_array = np.tile(frame_min_array, (1, n_countries, 1, 1))\n",
    "    # frame_max_array = np.tile(frame_max_array, (1, n_countries, 1, 1))\n",
    "\n",
    "    minmax_denominator = (frame_max_array-frame_min_array)\n",
    "    minmax_denominator[np.where(minmax_denominator==0)]=1\n",
    "    X_train_scaled = (max_-min_)*(X_train - frame_min_array) / minmax_denominator\n",
    "    # Use the latest min and max for test scaling.\n",
    "\n",
    "    latest_minmax_denominator = latest_max_array - latest_min_array\n",
    "    latest_minmax_denominator[np.where(latest_minmax_denominator==0)] = 1\n",
    "    X_validate_scaled = (max_- min_)*((X_validate - np.tile(latest_min_array,(n_validation_frames,1,1,1)))\n",
    "                                        / np.tile(latest_minmax_denominator,(n_validation_frames,1,1,1)))\n",
    "    X_test_scaled = (max_- min_)*((X_test - np.tile(latest_min_array,(n_test_frames,1,1,1))) \n",
    "                                        / np.tile(latest_minmax_denominator,(n_test_frames,1,1,1)))\n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "\n",
    "    train_norm_arrays =  (frame_max_array, frame_min_array, minmax_denominator)\n",
    "    validate_and_test_norm_arrays = (latest_max_array,latest_min_array,latest_minmax_denominator)\n",
    "\n",
    "    return scaled_splits, train_norm_arrays, validate_and_test_norm_arrays\n",
    "\n",
    "def flatten_Xy_splits(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict,n_countries, title='',suptitle=''):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    mse_train_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_train_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_train_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_train_naive, r2_predict))\n",
    "\n",
    "    true_predict_plot(y_true.ravel(), y_naive.ravel(), y_predict, title=title, suptitle=suptitle)\n",
    "    residual_diff_plots(y_true.ravel(), y_naive.ravel(), y_predict , n_days_into_future, n_countries)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should have been average/taking min max over countries as well as time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "be sure to check the definition of y when multiple countries. I think this is the definition from when using chronological data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that have been discussed with mike that might need to be changed. predicting log(y) as opposed to y. \n",
    "using MAE instead of MSE. \n",
    "using pooling or not. \n",
    "filter numbers. \n",
    "Using one country, one feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "global_data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prototype, see what kind of result we get with just the data on the United States, and only use the new_cases_per_million feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = global_data[global_data.location=='United States'].reset_index(drop=True)\n",
    "# data = global_data\n",
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(columns=column_search(data, 'log'))\n",
    "# data.loc[:, 'time_index'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO29eXhkZ3Xn/zm1aSvtW6ul3hdv7b2N28YQFgO2WewsJCYkmATGk0CYEJJfAkN+ITMJv0D28PwSMiaQOBNjwuIE47AZYw+Y2G233Yt773ar1Vpau0pLlVTrO3/ce0tVUpV21abzeR49qrr3Vt2jq6pvnfq+5z2vGGNQFEVRSgtXvgNQFEVR1h4Vd0VRlBJExV1RFKUEUXFXFEUpQVTcFUVRShBPvgMAaGpqMtu3b893GIqiKEXFSy+9NGyMac60ryDEffv27Rw6dCjfYSiKohQVItKVbZ/aMoqiKCWIiruiKEoJouKuKIpSgqi4K4qilCAq7oqiKCWIiruiKEoJouKuKIpSgqi4K0oB0T8+w/He8XyHoZQAKu6KUkB87ofn+PCXX853GEoJoOKuKAVEKBwjGI7nOwylBFBxV5QCIpYwROOJfIehlAAq7opSQMTihpiKu7IGqLgrSgFhZe66rrGyelTcFaWAiCcSRBMJdOF6ZbUsKu4i8iURGRSR4xn2/Y6IGBFpsu+LiHxORM6LyDERuWk9glaUUiWWMBgD8YSKu7I6lpK5/xNw19yNIrIFeAtwKWXz3cAe++dB4POrD1FRNg4x25KJqbgrq2RRcTfG/AgYzbDrr4DfBVJfhfcC/2wsngfqRKRtTSJVlA1ALGENpkZ0UFVZJSvy3EXkXUCvMebonF3tQHfK/R57W6bneFBEDonIoaGhoZWEoSglh5Oxx3RQVVklyxZ3EakEPgn8QabdGbZlfJUaYx4yxuw3xuxvbs64BKCibDgcr11r3ZXVspLMfRewAzgqIheBDuBlEdmElalvSTm2A+hbbZCKslFwyiBV3EuLmWicqXAsp+dctrgbY14xxrQYY7YbY7ZjCfpNxph+4HHgfXbVzAFg3BhzeW1DVpTSJW577lrrXlp86psneP+XXsjpOZdSCvko8BxwhYj0iMgHFjj828AF4DzwBeBDaxKlomwQktUymrmXFGcHJznWO57TElfPYgcYY96zyP7tKbcN8OHVh6UoGxNnQFWrZUqLwYkwkViCvsA0Wxoqc3JOnaGqKAXE7ICq2jKlQiJhGJycAeD80FTOzqvirigFhDOQqrZM6TAWiiQ/rC8MBXN2XhV3RSkg4mrLlBwDE+Hk7QuauSvKxmQ2c1dbplQYsC0Zn8elmbuibFR0ElPpMTBuifvNW+t5VTN3RdmYxHRAteRwbJkDOxsZnAwzORPNyXlV3BWlgIhp5l5yDEzO0FDl48q2agA6h3Njzai4K0qBYIxJ2jJOd0il+BmcmKGluoxdzVUAObNmVNwVpUBI7eEejaktUyoMTIRprSlna0MVbpfkbFBVxV1RCoTUqelRzdxLhoGJGTbVlOPzuNhSX6HirigbjfTMXcW9FIjFEwxPhWmtKQNgW2MVl0ZDOTm3iruiFAips1J1mb3SYCQYIWGgpaYcgI76CnrGVNwVZUORKug6Q7U0GJiwatxbbXFvr69gLBQlFLF6uwfXsce7iruiFAips1J1hmpp4NS4O7ZMe10FAL1j0xhjOPD/PcWffOfUupx70Za/iqLkhtTyR61zLw3652TuHfVWu9+esWmqy71MhmNJwV9rVNwVpUBIq5bRzL0kGJyYwSXQWOUDLM8doCcwjddtGSe7W/zrcm4Vd0UpEFIFXTP30mB4KkxDVRkeW8ib/WX43C56xkLJAXQVd0UpcVIzd+3nXhoEQlHqK73J+y6XsLmunN6xaaZmYtRWeGn2l63LuZeyhuqXRGRQRI6nbPszETktIsdE5N9EpC5l3ydE5LyInBGRt61L1IpSgqRm6xG1ZUqCQChKXYq4g1Ux0zM2zfnBKXa3+BGRdTn3Uqpl/gm4a862J4F9xpjrgLPAJwBE5GrgfuAa+zF/JyLuNYtWUUoYzdxLj8B0lNoKX9q2jrpKegO2uDevjyUDSxB3Y8yPgNE5275vjHEKNJ8HOuzb9wJfMcaEjTGdwHngNWsYr6KULGkzVFXcS4LxUCRj5j40GWYkGGFPax7FfQn8KvAd+3Y70J2yr8feNg8ReVBEDonIoaGhoTUIQ1GKm9RsPaozVEuCwHSUuoo54p5S+rhrnQZTYZXiLiKfBGLAI86mDIdlfJUaYx4yxuw3xuxvbm5eTRiKUhLEtbdMSRGOxQlF4vMyd6ccEmDPOor7iqtlROQB4B3Am40xzquyB9iSclgH0Lfy8BRl45CarWtvmeJnfNpacam2Mt1zb7fFvcLrZnPt+kxgghVm7iJyF/B7wLuMMaldcB4H7heRMhHZAewBXlh9mIpS+sTtGapul6jnXgIEQpa4z7VlNtWU43YJu1v8uFzrUykDS8jcReRR4A1Ak4j0AJ/Cqo4pA560y3ieN8b8mjHmhIh8FTiJZdd82BgTX6/gFaWUcPrJVHjdKu4lQFLc59gyHreL3c1+ru2oXdfzLyruxpj3ZNj8xQWO/zTw6dUEpSgbEceKKfe6tf1ACRAIRQCom1MKCfDogwco965v30adoaooBYIj7hU+l2buJUBgOnPmDtBQNV/w1xpt+asoBYLjuVdo5l4SjGexZXKFiruiFAhR9dxLisB0BLdL8JflxyBRcVeUHBKOxfnLJ88yHZlfZxBP8dy1/UDxEwhZE5jWq3fMYqi4K0oOOXIpwOeeOsfBzpF5+xxB1wHV0iAwHaU2T5YMqLgrSk6ZstfMzCTeyQFVtWVKgvHQ/NYDuUTFXVFyyKy4zxfveLJaRsW9FAhMR6irXP+qmGyouCtKDgmGLa89k3g72bzluastU+wENHNXlI1D0M7cIxkag6WWQkY0cy96xkPquSvKhmEhz302c3dp47AiJxpPMBmOZZydmitU3BUlC8YYvnv8MjPRtWuPFFzEc3cJ+Dwu4glDQgW+aBlfYHZqrlBxV5QsvHhxjF/7l5f53on+NXvOYCS7uMcSBo/bhddtvS2jCbVmipVsTcNyiYq7omTh6TODAIwGI2v2nFP2gGomTz0WT+BxCT5H3HVQtWgZn7abhmm1jKIUHk+ftsTd+Yq9Fiw0oBpLGDwuweO2ZjTqLNXiJBiOZe3lnku0K6SiZKB/fIbT/ZMATEzHFjl66SxU5x5LJNJsGa2YKT6ePDnAg//7ELfuaADUllGUnDE+HeXcwOSixz1jWzIel6xL5p7JcoknDG6X4E1m7mrLFBtHuscwBp6/MApk7uWeK1TclQ3FZ75zmvd84eCixz1zZoi22nJ2t/hzZ8vEDV6XzA6oauZesIRjcd73pRc42h1I235xJMS2xkp+964ruGN3E9Xl+TNHVNyVDcVPzg8zGgwzu6b7fAKhCM+eH+YNV7RQW+FlYg3FfWqBGaqxhMHtFjxLHFDtDUxzw//8Pif7JtYsPmVpXBoJ8aOzQ7x4cTRte9dIkO2NVXzoDbv5lw/euq5rpC7GouIuIl8SkUEROZ6yrUFEnhSRc/bvenu7iMjnROS8iBwTkZvWM3hFWQ69gWkujYZIGJjOUrseiyf4yKOHicQS/OJrtlriPrMetkxmcfe6XPhsW2axzP1k3wSBUJTjfeNrFp+yNC6PzwAwOTM7HmOMoWs4xPbGynyFlcZSMvd/Au6as+3jwFPGmD3AU/Z9gLuBPfbPg8Dn1yZMRVk9z70622Z3aibzIOmffe8MPz43zB/ddw3XdtRSW+FdM1smnjDJD5WMXSHjCdwuweNy2fcXztwvj08DMDgxsybxKUvHufap4j4ajDAZjrGtsSpfYaWxqLgbY34EjM7ZfC/wsH37YeC+lO3/bCyeB+pEpG2tglWU1fD8hRRxD88X93Aszhef7eRnbmrnF27ZCrCm4u5MYIIsde7OgKpnadUyvQFb3CfDaxKfsnSczH0qPPvauDgSAmB7U/Fk7ploNcZcBrB/t9jb24HulON67G3zEJEHReSQiBwaGhpaYRiKsnSee3UkueRZJnG/MBQkljC84YqW5LaaCi+hSHxFg5ufeOyVtNmtwZRzRjM2DjN43S68rqXVufcFLIEZ0Mw95/QnxX32f9o1EgQonsx9mWQaPcj43dIY85AxZr8xZn9zc/Mah6Eo6XSPhugNTPNTe63XWiZxP2uXSF7RWp3cVmtPQlnuoOrwVJhHX7iULKmEOeKeseVvIi1zX2xA9bKduQ9MaOaeazJ57hdHQrgEOuor8hVWGisV9wHHbrF/O6/gHmBLynEdQN/Kw1OUteE525K582orK8/kuZ/pn8TrFnY0zWZejrgv15p5uWsMSH/zO5UykL3O3esWPHbmvlhvmT5b3IfUlsk5/RnEvWskSFttBWUed77CSmOl4v448IB9+wHgmynb32dXzRwAxh37RlHyydn+Sco8Lq7vqAPS/e/kMQOT7Gzy4/PMvi1WLO6XrPrn1G8ITubudkmW3jLOJCY7c89g3cwem2BgMowIDE7OLFjaqaw9zoBqui0TKhi/HZZWCvko8BxwhYj0iMgHgM8AbxGRc8Bb7PsA3wYuAOeBLwAfWpeoFWWZdI+F6KivoLrcEuuMmfvAJHs3Vadtq6mwPPq1ydyt23UV3qztB7wp7QcW6uk+OBkmnjDsafETjRvGQmtXrqksTDAcY8L+v07NydwLxW+HJfSWMca8J8uuN2c41gAfXm1QirLWdI9Os7WhMjljMNUiAesN2z06zS/s35K2Pem5ZymdTOVfX7zETDTBL966lWO9duY+Mz9zr6vMLO5z2w8sNIjrWDI3bqnn7MAUAxMzNFTlb6r7RqLfHsBu8vuYtOdAjIeijIWiBVPjDjpDVdkgdI+F2NJQSZnHhdslaSVsAOcGpwDY2zo3c1+aLRNPGP70u2f4n0+c5IljfcxEE1SXeZJvfpgV9/pKX8b2A9G41RUy2ThsAVumz/Z8b9hq2UxaMZM7HL99d4ufYCROPGHoGi2sShlQcVc2AOOhKJMzMbbUVyIi+Ms8yYWqHc70W1P4r5hry5QvrVrm8KUxRoIR4gnDf3/Mmsx9++5GJsPzB1TrKn1ZB1Q9rqXZMk7m7owhaK177nAqZZxEIBiJJf8f7XWFUSkDKu7KBuDSqDW5ZEuD9cbzl3nSvHCAM/1TlHtdbKlP/1pd7nVT5nEtmrk/eWoAr1v48Bt3MR2Ns6mmnL2t1UyFY8nBzmA4hkssHz9j5p5I2L1lFrdlLgemqS73sLPZyhR1lmru6LcHU3e3+AFrXGXEXtClyV+Wt7jmouKulDzdY5a4d9jCbWXulrgHQhGePjPI8xdG2NtanbHR01Kahz15coADOxv5yJv2sL2xktfaHQGNgWDEytinwjGqfB7KPK6snnt6V8jsmXtvYIb2ugrKvW5qK7xa655DLo9b4xuNVZaQT83EGJmyxL2Qxj10sQ6l5Om2M/et9mBXVZk7WbnykUcP8+NzwwB88I4dGR+/WAuCV4emuDAU5IHbtlPudfPEf3sdXrfwjZd6AevN73ygVJV58Lozi7tVCula8oDqZtsCaK0pY3BSM/dccXl8hrba8pTB+SijwQjV5Z60Mtp8o+KulDzdYyFqK7xJ/9xfPpuJd4+G+Km9zfzJz1xLW215xscvJu4/ODkAwJ1Xt1rPb7c48Ke8+aGcYCRGVZnbFvcMjcMSibQB1YXaD1wen+ZGezC1pbpcM/cccnl8hva68uT/17FlGgsoawe1ZZQNQPfodNJvB/CnZO7DUxF2NFWxua4Ckcy9t2sWEfcXL46xu8U/bzDNyeySNdHhOH47c880iSmeMHhSZqhGstgy05E4Y6FoMnNvqSnTWao5pH98mk215VSXpYj7VJjGAvLbQcVd2QB0j4XSBkodi2QmGmcqHKO5euE35WI93TuHp9jVPL8EznnzO7Xuji3jcwvReGLerFKnFFLEqnXPlrl3Dltld1sbrL+ptaacwckZEgtU1yhrw2gwwlgoas+ZsCfEhWOMBiMF5beDirtS4iQShp6xabY0zIp7VZmHqZlYMttt8i/8pqyt8DKeZQZoPGHoHp1me1MGcU958wNpnrsx1mPnPpezClM2Xx7gtF22eVWbVYrXUl1mz1KNLPh3KKvnSLc18/j6jrpZ220mxvCU2jKKklMGJ8NEYgm2pHTqqy7zMBWJJWvDF8vcayq8TIZjGTPjvsA0kXiCHRkmr8x6stYHw1TYGljN1vXR8dzBWpg7W7XM6f5JfB4X2+1zttZYYwXqu68/Ry4FcLuEaztqqfS6EYGJmShjoQiNiyQJuUbFXSlpnDLIuZm7MdBj71usNrnGLmmcWxsPsxZJpszdn+LJAoQi8eSAKsxfjMNpHAbgy1IuCXDq8gR7W/3JLH9W3LViZr053B1gb2s1lT4PLpfg93noHZsmnjA0VKnnrig5o3Mo3Z8GS9wBLg4vTdwX6gx50V6gYccSxH0qxXOH9FJHYwyxFFvG43JlXWbvdP8kV26qSd53qnz6VdzXlUTCcLQ7wA1b6pLbqss9ydeA2jKKkkOeuzBCY5UvaWHAbBVL8k25BM8dyDio2jkcpNLnpiWDteN2CVU+qzInGk8QiSXw+zwpk5Rmxd3x3x1bxuuRjJn78FSYockwV6a0SWiuLkNkdlp8NgYmZhiZUutmpXSOBJmYiXFjirj7yz3JGdBqyyhKjkgkDD8+N8wde5rSZp5W+WbFvabcs+jiCk7JoTPZKZWLw1ab12xllNXlXqZmYskZsVVlsxNdorHZzNzpI+O0HvC6XEQzePxn+q3Voq5qm83cvW4Xzf6y5LT4bHz4kZf59UdeXvAYJTtH7B79TrM2sL6dDRfg7FRQcVdKmNP9kwxPhbljd1Padmeg8+JwkKZFBlMB9rXXcudVrXzuqXPJBlEOF0dC7FhggQZ/uYfJcDRpzTh17pDuuc/L3N2ujIt1nLpsVcpcOafBWVttOf2LDKheHAnyQufovL9BWRpHugP4yzzsavYntzkVUUCyHUGhoOKulCzPnrcWXn/dnvQ1eh0vfCwUpXmJE08+9c6rMRj+6ImTyW2xeILu0VCa5TMXp0lZry2obXXlGVv6Ov6622V77m4hlmGZvVOXJ2muLps3YWZTbfmCmXs4Fk9mmN9+RRdHWwlHugNc11GbHPSG2UQBNHNXlJzx43PD7G31s2lOWwFH3IElZe5gVdt86A27+c7xfl4dsnq/94xNE0uYjJUyDtXlHqbCMS6N2P1tGirxeeYPqDpC7vSVsWaxzrdlTvdPzMvaATbVlC/ouQ+Mz2b13zqm4r5cZqJxTl2eSBtMhdmJaoXWVwZU3JUSZSYa52DnKHfsbp63rypF3JeauQO8xe4dc6LPskY6F6iUcagutzL3S6Mh3C5hc11FxgFVx3N3J22Z+TNU4wnDucGpNL/dYVNtBZMp3v5cnDU/79jdxNHuQLKZ2mLMROPJD7ONzIm+cWIJM1/c7cy90CplYJXiLiK/JSInROS4iDwqIuUiskNEDorIORH5VxEpvL9aKXkOXRwjEkvwuj1N8/ZVp3yVXmx2aiq7mv14XMJp2/e+6NS4L2DLVJdZA6pdoyE225ZMJs/dEXevK/sM1d6xaSKxBLtTPF+Hxcohne0feJ3V+fI/lmjN/MvzXdz91z/e8FU2hzMMpgL4yyzPvdD6ysAqxF1E2oH/Buw3xuwD3MD9wGeBvzLG7AHGgA+sRaCKshxe6hpDBG7eXj9vn7PUHixvcQWfx8XuFj+n7YqVM/2T1JR7FvyA8Du2zGiIbQ3Wh0Cmfu3xeHrm7vO4CM8ZUL0wbGXQmWwgZyJTfxZrxrFsbtnewPVb6njiWN8if61FX2CGSDzBj84NLen4UuVId4D2ugpaqudYfHaiUGh+O6zelvEAFSLiASqBy8CbgK/b+x8G7lvlORRl2RzuHmNPiz/Z5jcVZ6k9WP7KOVduqk5m7gc7R7lle0PWMkiw/P2pcIyLw8HkLFmfI+6xBEe7A3zw4ReZjloLejilkI1VZQzP6fTofFPIZAM5mXs2371/fIbqcg/+Mg/vvK6N470TyedbiMC0NQj7zBkV97mWDMx67iVlyxhjeoE/By5hifo48BIQMMY4xl8P0J7p8SLyoIgcEpFDQ0Mb+4WjrC2JhOHwpQA3bZ2ftTs44r5YX5m5XNlWQ9/4DOcGJukcDnLrzoYFj3csoPHpKNvsxUK8KQOqBztH+MGpQbps/95j2zJtteUMTIbTmot1Dgfxl2X+puAMGmdrQXB5fDr5AXDPtW3A0qyZgN0w7Udnh+Y1OtsoDE+F6RmbzizujudeYBOYYHW2TD1wL7AD2AxUAXdnODTjK8IY85AxZr8xZn9z8/xBL0VZKReGg4xPR5ck7kutlnFwKlX++bkuAG7d0bjg8an+vtMCIdVzd9oBD9hZumPLbKotJ54wDKd43Z0jIbY3VWb8plDudVNf6U0OnM6lf3yGTbXWZKzNdRXcvK2ebx1d3JoZC0VwiVU2eqwnsOjxpUimyUsOs7ZMCXnuwJ1ApzFmyBgTBR4DbgfqbJsGoANYmrmnKGvE4UtWW9YbM7wZHarKrFmpy/067VSqfOPlHvxlHq7ZPL9yJRVnwA1mxd2X4rlPhS07ZsC2U5xSyE01822WzuEpdjTNH0x1aK0pz+q5943P0FYz6xe/47o2TvdPcn5w4UqYQCjK7buaECk9a8YYw+Xx6QVXvALLknG7hH2ba+ft85eiLYNlxxwQkUqxUok3AyeBp4Gfs495APjm6kJUlOXx8qUANeXpMwnn4i/3Ul3uody7cOuBubRUl1Ff6SUUiXPztvpko69spGXujemZezSesJfgmxXx1MwdZgdII7EEvWPT7GjMPhvWmqU6X9wjsQTDU2Ha6mbF3bFmvn+yf8H4x0LWSlU3bKnj6TODCx6byktdYwVv4zz36gi3/ckPufYPv8+v/OMLWbtwHukOcOWmaip8818rV2+u4f23b+f1ewvPfViN534Qa+D0ZeAV+7keAn4P+JiInAcagS+uQZyKsmQOXxrjhq31af1k5tJSXZa2OtNSEZFkR8bF/HaY/dpeXzm7hmvqAthBJ3O3RTnVcweSs04vjYZIGNiRYcUnh021FRkz98HJGYwhbY3Y1ppyGqp8C7YiSCQM49NR6iq93HXNJo71jHOib3zRv/lk3wQ/+/n/5MsHuxY9NhO/+k8v8tUXu5f1mPFQNDkLeKkc7rbsltt3NfL0maGM1yIaT/BS1xg3b8ts8ZV53Pzhu64pvWoZY8ynjDFXGmP2GWN+2RgTNsZcMMa8xhiz2xjzbmPMxi6QVXLK5EyUMwOT3LSAJQPwyXuu4h8e2L+iczjWzGJ+O1i94CG95bCzWEcklkiu0uR45U61TEOVD5/bxWVb9DuXUFO/qaac4akI4Vg8bbsj+I7n7tBY5WN4MvvqTRMzUYyBukof99+ylUqfmy/86MIifzG81DUKwNdf7l302LmEIjF+eHqQfz+yvMd+5run+cUvPL+sx5zpn6S9roJfOrANsJbQm8uxngDT0Ti37Vz8f11o6AxVpaQ4OzCJMXBt+3x/NJX6Kl+y2+Nyeft1bdy9bxPXdSx8Dpj13LemiHK6524PqNpNv5zGYSJCa21Z0otfqAzSocNebWru7FPH8mmb04ahyV/GSDB77jVmV8rUV3qprfRy/y1b+daxy4tmyM6En6PdgWXPbu0etZ77SHdgUS88lQtDU3SNhJKrXi2FswOT7G31U1dp/Y8yLVP4/AXrg+pWFXdFyS/OAhwLieBquXlbPZ//pZuT3vlCVCcz99kPklTP3WkX4Ih8qoffVlORFOYLw0HqK73UVWb/+n9Nu/WNwmmP4DCbuaeLe6Pfx8hU9szdEbt6+5zO7NYvPduZ9TFgCfP1W+pwCfz74eVl4E5v9FAkzqnLk0t+XJ/9zefC0OK1+2A1fbswFGTvpuqkpTIanP/B8NyrI1yZckwxoeKulBRdI0FcAh0r8NPXg6oyD3/x7uv55QPbk9vcLsElli0zd+k+T8o4QWvKAOnF4eCiH1i7m/2UeVwc7033xfvGp6nyuZMTbhya/GUMLdBWIGCLu5PZttdV8MYrmnn6dPaB1UAowoXhIG+7ppXX7m7i3w73YszSB1YvpXzrcOydxUgkTPIDbLHqH4eLIyEi8QR7W6qpt4V7bI4tE4klONQ1yoEizNpBxV0pMS6OhGivryioDn0/e3PHvKzZWSM1GEkX99R2sm21VmmjMYbzQ1MLdp8EK+u/sq2GV1LE/Wh3gB+eHmRzXcW8+vgmv4/Jmdg8j97BmcCU+m2hva4irfZ+LkfsQcobttTx0ze20zM2zbGexQdhHbpHQ/jLPLTVlnOoa2xJjxmeCidbOSzVBjo3YH0r2NtaTXWZB49LGJ1jyxztCTATTai4K0oh0DUSXHDQsVDwuq3eMcFwLE3QnUoasAZIw7EEz18YZWgyzC3bF6/O2be5hhO9EyQShq+8cIn7/u4nTEfi/L/vuHresU6zq0wDiZDuuac+ZmImltaLPpXDlwK4BK7rqGP/NiteZ4GRpXBpNMTWhkpu3lbPy0sU99QxgKVm7mcHphCB3S1+RIT6Kt+8zP35V0cQgQNLqIoqRFTclZLi4kgoOc2/kPG5XQTDMaJxw5b6WT/eWawDZj3yL79wCYA3XtGy6PNe217LZNjqQvm3z5zn+o46nvrtn8pYh+1MvMlWMROwZ6em9udxvOdMg49glRfuba3GX+ahvb6CMo9ryYILlrhvabBm0PaNzyxp1ShnXGJbY+WSM/ezA5NsbahM1q43VPrmfci9fGmMvS3VC45zFDIq7krJEAhFGJ+OFk3m7mTGqXZLqufuiPv3jvdzzeaaedZOJvbZVUJferaT7tFp3n/79rSl4FJxWi8MZ6mYGQtFqK3wps0XcD4QMg3EJhKGo92B5Mxgt0vY2ezn/BIFN5EwdNuZu5P1L8WacT4AXrenia6RUNbJSKmcHZhkT8vsoif1Vd55H1jdY9PrOjC/3qi4KyXDRXu1o23FIO4eSR9Oo0UAABy0SURBVIpJ6oeRx53uuYPVg+ZNVy6etQPsafXjdQtffuES/jIPb7tmU9Zjm+x+KNkqZsZC0WSljINj5WQqobw8McP4dJRrUqbp727xLzlzH5oKE44l2NpQyVVt1Wm98xeiN2ANGN+0tZ5YwtA1svBCJJFYgs7hIFdsmp3B3FCVnrkbY+gZCyXLS4sRFXelZHA6K24vAlvGytwdcZ+NN9V/b/aX4dx94xLFvczjZm9rNfGE4e3XtmWcMu/gdDLMNkA6HopSW5me9c+WDc7/QOiya/F3pmS7u5v99AammY5kHrRNxamU2dJQicftor2+Iq16JhuXAzNsrqtgd4sl1ot9mHQOB4klDHtbUzL3Sl9yABlgJBhhJppQcVeUQuDicAgRkn3TCxmf25UUk9RvGt4Uz93jdtFcXUZDlY/rOxaecZuKM4Hr5/Z3LHhcVZmHCq876ypLY6HI/Mx9AVsm+c0pVdxb/BiztCqW1HVmnd9LWQ6wb3yatroKdtq9hBY7l1NNlNr0raHKx1goQsLuh9MzZlk9hVJSuxI8ix+iKMVB10iQtpryZTcDywdetytZR15f5aOm3MPETAy3O71c8c6rWmn0l6Vl9Ivx87dsodLnYX+WfiipLDSRKRCKcsWcxbhrK7y4XZLRlukaCeLzuNK6TzrZ9KtDU8nxgGx0j1kfzu12try1oZJvL6HnfF9gmms21yRLKJ8+Pch0JM7r9zbzmh3zK12O9QSo8rnTOmzWV/pIGKvlQl2lj54x60Olo6F4M3cVd6VkuDgSLAq/HaySR6dpor/MTVO1VWKYmrkDfPqnr132c9+0tX7BXvapNM6ZyDQTjfPIwUv80oGtGTN3l0uoz1BZAtb139pQmTYAu72pErdLluS7XxoN0VZTTpnH+nDe2lDJWCjKxEw044paTrzDUxHa7L4512yu4QenBjnUNcbh7jEe+eCBeY851jPOvvbatA/MVLuprtJHr525t6+wRUUhoLaMUjJ02YtZFAOprQuqyjzJ5f6Wk6GvBc1zMvdnzgzxR0+c5GuHeghF4mk17g6NVbOP+cefdCZnrHaNhOaNd5R53GxrqFySuHePhtIsNceeubTAAKkzM9XpE/QXP38D3/+t1/P2a9sy+vWRWIKTlye4fs6qSvVzSjx7xqapq/RmrTQqBlTclZJgYibKSDBSNJl76gxaf5mHZlvcPTkW98aq9OZhjh3xjz+x+sdkqvFu9PsYCUYwxvAX3z/L///0eYwxWb857VpixUzncCite6Yj9Av57k5Pmc12r/raCi97W6vZ2VxFX2BmXlnk2YFJIrHEvMZyDZXp/WV6xkJFnbWDirtSIjjZXTFUysCczN1nrYvqEhbsQb8eOJ67M5DYF7Ay4VftBlx1GTJ3p2xwYCLMVDjG0e4AF0dCzEQTGa//7hY/F0eCC9afT8xEGZ4KJwdFYXZxk4UqZpx4N89pZ7yloZJ4wsybBHXUXipw7gB1fZXdGTI4m7kXc6UMqLgrJcJFuwyyWDJ3p81Apc+NyyXcfW0b7799R87jaPKXEUsYJuxWub2BUFoHxLmeOzi2TDhZlRJLGL7xUg+Q+fpfuamaaNzw1KmBrHF02h8mO1MWI6kp91Jf6aVrNEQwHONvfnCOwcn0xUicbxpzJ3glLZ05Hwyv9IxTV+lly5yB0qTnHorYNe7TRV0pAyruSonQlZzAVBxvSCdzd9bgPLCzkT945/z+L+vNbK27lbH2BWa4tr022U8lU+bu9JdJ7RnztZeslZMyzQ6+e18bV7fV8Ml/O87QZOayywvD1gfFrjkrTTnlkF9/qYe/+sFZPvjwIUIpzdZ+cn6YKzdVz6uQ2pYl6z/aM8617bXzmqhVeN2UeVyMBSOMBiNMR+OauStKIXBxOEhLdRmVvuIoAPPNEfd84QzkOhOZ+gLTbK6r4H23bafC687oOztZ7qGLY/jLPNywpY6BiTBetyS971R8Hhd/ff8NTIZjfPwbxzK2AO4cslo1z52jsKWhkkujIR473EuTv4zjveP85leOkEgYBidnONQ1xl375s/Cba0ux+d2pQ3GzkTjnB2YzDhnQGS2CqgUatxhleIuInUi8nUROS0ip0TkNhFpEJEnReSc/XtpNVmKsgqsSo3isGRgNnOvKhBxH5mKMB2JMxKM0FFfwT3XtnH0U2/NPKBqi/uLF0fZ1VzFbbuslrhb6iuzLhi+t7Waj965h6dOD3Iuw+Dqq8NBtjRUJssgHbba4n60O8B/ff1Ofv/tV/PkyQEeO9zL904MYIz1zWAuLpfQ0ZA+w/VId4B4wnDDlswTwurtiUyz4r6xM/e/Ab5rjLkSuB44BXwceMoYswd4yr6vKGtOOBYnbg8EWpUaxZNpOdUy+c7cm+3mYZfHp+dVnmTriT/bXybCrhZ/st/5Ytf/Z2+yZsw+eXK+935hKPNiJFsbKjEGXAL33rCZX3ntdq7vqOUvvn+Gbx7uZWdTFXtb/fMe5zw2Vdxf7BxFhKytkxuqvHbmbj2mfaOKu4jUAK8HvghgjIkYYwLAvcDD9mEPA/etNkhFmUsiYXj7557lj//jJKFIjMHJ8KKLWRQShZK5N1T52FxbztGe8eTEnbmVJ5ke47Cr2c/+bfV43ZJW6ZKJ1ppyrt9Sx/dP9KdtTyQMF4eD7Gya/3inYuaOPc201JQjIvz3e67i8visJTPXP3fY1lDJpZFQ0gZ64eIoV7RWz+uX41Bf6WNgIsxTpwapq/RmnThVLKwmc98JDAH/KCKHReQfRKQKaDXGXAawf2fseCQiD4rIIRE5NDQ0tIowlI3Iwc5Rzg9O8R/HLifXTS0qW8ZjCZK/LP+tEm7cWs/hS2PJssHFMtbGOeJeVebh0f9ygF9/w65Fz/XWq1s52jOenHwE0D8xw3Q0nlYp43BFazVVPje/fGBbctutOxt569WtQGZLxmFLQyWT4RiBUJRYPMHLXWMLLnjSUOWjNzDNoa5RPn7XlYv+LYXOasTdA9wEfN4YcyMQZBkWjDHmIWPMfmPM/ubm+QsJKMpC/Nthq/RucDLMd49b/UeKypZxBlTL8z8AfMOWOnrGpjnaY62i1FqzcN94p78MzPaO2b+9IenfL4Qjyk+mlEU6i1rvzPDNq9Ffxit/+DbeYj/O4Y/v28dnf/Za9rXXzHuMQ2o55MnLEwQj8Yy9ZlKPL/e6+Ptfupn7X7N10b+l0FmNuPcAPcaYg/b9r2OJ/YCItAHYv7OvpqsoK2AmGuc7r/TzU/bqQo8ctFYqKiZxLxRbBkgurvHd4/1sqilPm2CVCae/jMcly77mu1v87Giq4ltH+wiGrZLGTrsMMputk2liV0tNOb9wy9aslgzM1tx3jYZ4odNabHshcf/V1+7ghU/eyVsX6IFfTKz4lWWM6ReRbhG5whhzBngzcNL+eQD4jP37m2sSqaLYPHlygMlwjAdfv5PRYIRXesdp8vuKqg9Iss69AEo397XX4nEJY6Eou7Yt7Js7NFb5qKnwLPpBMBcR4WdvaufPv3+Wm//4Sd58VStTMzEqfW5aaxbP/JeDM1HpeO84ncPWgPtC30pcLil6nz2V1b6yPgI8IiI+4ALwK1jfBr4qIh8ALgHvXuU5FCWNbx7pZVNNOQd2NvLGK5p5pXe8aGamOjgzVAvBlin3url6cw3HesaTDbgW4+5rNy1b2B0+9Ibd3LK9gW+/cpnHj/YxFoqyr71mwSx8JVT6POxsruKhH10A4OduXri/famxqleWMeYIsD/Drjev5nkVJRvReIL/fHWEn7mpHbdLeMOVLXzuh+eLypKB2TLDQrBlAG7cUsexnvEll/999M69Kz6XyyXcurORW3c28ol7ruLJkwPrVlP+zQ+/lqdODfLMmUHee2vx++jLoTBeWYqyRE70TRCKxJO11dd31HHT1jpet6cpz5Etj7ntB/LNjVvrefi5riVn7mtFudfNO6/fvG7PX13u5b4b27nvxvZ1O0ehUhivLEVZIgcvjACzA2Nul/DYh16bz5BWRKGJ++27G9nZXLWk1ZuU4qAwXlmKskQOdo6ys7mKluqFy/UKHcdzLxRbpqW6nB/+9hvyHYayhmjjMKVoiCcML3aOcuuOxnyHsmq2NFTi87iKvn+JUrgURtqgKEvg1OUJJsOxZDvaYuamrfWc+B9vW3HFiaIshr6ylKLhedtvL4XMHVBhV9YVfXUpRcNLXWNsbaict+qOoijzUXFXioYzA5Nc3Za9l4iiKLOouCtFQTgWp2sklLV3t6Io6ai4K0VB53CQeMKwu7U636EoSlGg4q4UBecGrM6Be1o0c1eUpaDirhQF5wancAkZl2JTFGU+Ku5KUXBuYJJtjVWUe/O/cpGiFAMq7kpRcG5wSi0ZRVkGKu5KwROJJbg4HGSPVsooypJRcVcKnq6RILGEYU+LVsooylJRcVcKnrN2pcxutWUUZcmouCsFz7nBSURU3BVlOaxa3EXELSKHReQJ+/4OETkoIudE5F/t9VUVZcVcGg2xqaZcK2UUZRmsReb+m8CplPufBf7KGLMHGAM+sAbnUDYww1MRmqvL8h2GohQVqxJ3EekA3g78g31fgDcBX7cPeRi4bzXnUJThyTBNfhV3RVkOq83c/xr4XSBh328EAsaYmH2/B8i4Mq2IPCgih0Tk0NDQ0CrDUEqZ4akwzSruirIsVizuIvIOYNAY81Lq5gyHmkyPN8Y8ZIzZb4zZ39zcvNIwlBInkTCMBCM0VevQjaIsh9Uss/da4F0icg9QDtRgZfJ1IuKxs/cOoG/1YSoblcB0lHjCqC2jKMtkxZm7MeYTxpgOY8x24H7gh8aY9wJPAz9nH/YA8M1VR6lsWIanwgAq7oqyTNajzv33gI+JyHksD/6L63AOZYMwPKnirigrYTW2TBJjzDPAM/btC8Br1uJ5FWXIztyb1XNXlGWhM1SVgmZ4KgJo5q4oy0XFXSk4xqejfOOlHowxDE+F8bqF2gpvvsNSlKJCxV0pOL52qJvf/tpRzg5MMTwZprGqDGt+nKIoS0XFXSk4zvRPAnCib5zhqbDWuCvKClBxVwqOs4NWi98TfRMMTWnrAUVZCSruSkFhjOH8QErmPhnR1gOKsgLWpBRSUdaK3sA0wUicSp+bk30TTEfjNGlHSEVZNpq5KwXFOXvVpbdds4mJmRjRuLYeUJSVoOKuFBRnbUvm3hs2J7c1+XVAVVGWi4q7UlCcHZiipbqMW3c04rKrH9VzV5Tlo+KuFBTnBifZ21pNhc/NrmZrzVT13BVl+ai4KwVDImE4NzDFnlZL1K/ZXANo6wFFWQlaLaMUDD1j00xH4+xtrQasQdWesWnqtPWAoiwbFXelYDjWGwDgik2WuN99bRt3X9uWz5AUpWhRW0YpGP7j2GWa/GVc116b71AUpehRcVcKgomZKE+dHuQd17XhcevLUlFWi76LlILg+ycGiMQSvPP6zYsfrCjKoqi4KwXB40f76Kiv4KatdfkORVFKghWLu4hsEZGnReSUiJwQkd+0tzeIyJMics7+Xb924SqlyPBUmJ+cH+Zd12/Wvu2KskasJnOPAb9tjLkKOAB8WESuBj4OPGWM2QM8Zd9XlKz8+NwQ8YTh7n1aGaMoa8WKxd0Yc9kY87J9exI4BbQD9wIP24c9DNy32iCV0ubZcyPUV3qTk5YURVk9a+K5i8h24EbgINBqjLkM1gcA0JLlMQ+KyCEROTQ0NLQWYShFiDGGZ88PcfvuJlwutWQUZa1YtbiLiB/4BvBRY8zEUh9njHnIGLPfGLO/ubl5tWEoRcqrQ1MMTIS5Y3dTvkNRlJJiVeIuIl4sYX/EGPOYvXlARNrs/W3A4OpCVEqZZ88NA6i4K8oas5pqGQG+CJwyxvxlyq7HgQfs2w8A31x5eEqp8+z5YbY1VrKloTLfoShKSbGazP21wC8DbxKRI/bPPcBngLeIyDngLfZ9RZnHTDTO8xdGNWtXlHVgxY3DjDHPAtlGwN680udVNgaJhOFjXz1CMBLTWamKsg7oDFUlL3z2e6f59iv9fPKeqziwszHf4ShKyaHiruSc0WCEh350gZ/f38EH7tiR73AUpSRRcVdyzsELIxgDv3DLVm03oCjrhIq7knOeuzBCpc/NdR3at11R1gsVdyXnPPfqCPu3N+DVvu2Ksm7ou0vJKcNTYc4NTnGbDqIqyrqi4q7klOcvjABw2y4Vd0VZT1TclZzy3Ksj+Ms87NMOkIqyrqi4KznDGMN/vjrCLdvrdZ1URVln9B2m5IxjPeN0Dgd5y9Wb8h2KopQ8Ku5KzvjqoW7KvS7eeb2uuKQo642Ku5ITpiNxHj/Sxz3XtlFd7s13OIpS8qi4Kznheyf6mQzHePfNW/IdiqJsCFTclZzw5YOX2NpQya07GvIdiqJsCFTclXXn2XPDvHBxlPffvl3XSVWUHKHirqwrxhg++93TtNdV8N4DW/MdjqJsGFa8WIeiLMTETJS+wDT/58wQr/SO8xfvvp4yjzvfYSnKhkHFXVkzhibD/OWTZ3niaB+T4Vhy+772Gu67sT2PkSnKxmPdxF1E7gL+BnAD/2CM0bVUS5CpcIwTveP8xyuXeezlXmaice69oZ0rNvlpr6tkc105V7XV4FavXVFyyrqIu4i4gb/FWiC7B3hRRB43xpxcj/Mpa4MxhslwjEAwSiQex+NyEY0nmJiJ0T0a4vzgFOcHp+gcDjIVjhGMxAiEogCUe1285epNfPTOPexq9uf5L1EUZb0y99cA540xFwBE5CvAvcCaivvzF0b4mx+cS943mNnbJv1Yk/XO0h9n5uxM37fQ+ZbzuAViManHzd03d0u2x6UflzAQiSUIReIEQhFiiezP43YJ2xor2dnkp7bCS4XPRXtdJTubq7hjdxNVZeryKUqhsF7vxnagO+V+D3Br6gEi8iDwIMDWrSurokgYQ3yuGEnGmwiQXNFNQObsTV3tbe7Kb6nHLrQq3Nwl49LOMO85V/Y4Fohl4fNlfpwI+NwuKnxu6it91k+VD5/HRTSWwOtxUV3uob2ugu2NVfg8WmClKMXAeol7Jgmck8yah4CHAPbv3589XVyA23c1cfuuppU8VFEUpaRZrzSsB0idZ94B9K3TuRRFUZQ5rJe4vwjsEZEdIuID7gceX6dzKYqiKHNYF1vGGBMTkd8AvodVCvklY8yJ9TiXoiiKMp91K28wxnwb+PZ6Pb+iKIqSHS19UBRFKUFU3BVFUUoQFXdFUZQSRMVdURSlBJGFpq3nLAiRIaBrhQ9vAobXMJy1olDjgsKNTeNaHhrX8ijFuLYZY5oz7SgIcV8NInLIGLM/33HMpVDjgsKNTeNaHhrX8thocaktoyiKUoKouCuKopQgpSDuD+U7gCwUalxQuLFpXMtD41oeGyquovfcFUVRlPmUQuauKIqizEHFXVEUpQQpanEXkbtE5IyInBeRj+cxji0i8rSInBKREyLym/b2BhF5UkTO2b/r8xSfW0QOi8gT9v0dInLQjutf7bbMuY6pTkS+LiKn7et2WyFcLxH5Lft/eFxEHhWR8nxcLxH5kogMisjxlG0Zr49YfM5+HxwTkZtyHNef2f/HYyLybyJSl7LvE3ZcZ0TkbbmMK2Xf74iIEZEm+35er5e9/SP2NTkhIn+asn3trpcxpih/sFoJvwrsBHzAUeDqPMXSBtxk364GzgJXA38KfNze/nHgs3mK72PAl4En7PtfBe63b/898Ot5iOlh4IP2bR9Ql+/rhbU8ZCdQkXKd3p+P6wW8HrgJOJ6yLeP1Ae4BvoO1AtoB4GCO43or4LFvfzYlrqvt92UZsMN+v7pzFZe9fQtW6/EuoKlArtcbgR8AZfb9lvW4Xjl506zTRbsN+F7K/U8An8h3XHYs3wTeApwB2uxtbcCZPMTSATwFvAl4wn5BD6e8GdOuY45iqrFFVOZsz+v1Ynbt3wasdthPAG/L1/UCts8RhYzXB/hfwHsyHZeLuObs+2ngEft22nvSFtnbchkX8HXgeuBiirjn9XphJQt3ZjhuTa9XMdsymRbhbs9TLElEZDtwI3AQaDXGXAawf7fkIaS/Bn4XSNj3G4GAMSZm38/HddsJDAH/aNtF/yAiVeT5ehljeoE/By4Bl4Fx4CXyf70csl2fQnov/CpWVgx5jktE3gX0GmOOztmV7+u1F3idbfX9HxG5ZT3iKmZxX3QR7lwjIn7gG8BHjTET+YzFjucdwKAx5qXUzRkOzfV182B9Vf28MeZGIIhlM+QV28O+F+sr8WagCrg7w6GFVj9cCP9TROSTQAx4xNmU4bCcxCUilcAngT/ItDvDtlxeLw9Qj2UJ/T/AV0VE1jquYhb3glqEW0S8WML+iDHmMXvzgIi02fvbgMEch/Va4F0ichH4CpY189dAnYg4q3Dl47r1AD3GmIP2/a9jiX2+r9edQKcxZsgYEwUeA24n/9fLIdv1yft7QUQeAN4BvNfYnkKe49qF9SF91H79dwAvi8imPMeFff7HjMULWN+qm9Y6rmIW94JZhNv+1P0icMoY85cpux4HHrBvP4DlxecMY8wnjDEdxpjtWNfnh8aY9wJPAz+Xx7j6gW4RucLe9GbgJHm+Xlh2zAERqbT/p05ceb1eKWS7Po8D77OrQA4A4459kwtE5C7g94B3GWNCc+K9X0TKRGQHsAd4IRcxGWNeMca0GGO226//Hqyih37yfL2Af8dKtBCRvVgFBcOs9fVar0GEXPxgjXqfxRpV/mQe47gD6+vTMeCI/XMPlr/9FHDO/t2QxxjfwGy1zE77RXMe+Br2qH2O47kBOGRfs3/H+pqa9+sF/A/gNHAc+N9YlQs5v17Ao1i+fxRLmD6Q7fpgfZ3/W/t98AqwP8dxncfyip3X/t+nHP9JO64zwN25jGvO/ovMDqjm+3r5gH+xX2MvA29aj+ul7QcURVFKkGK2ZRRFUZQsqLgriqKUICruiqIoJYiKu6IoSgmi4q4oilKCqLgriqKUICruStFhtwv+kH17s4h8fR3P9Wsi8r5lPuYZEVnz1ewVZTlonbtSdNjN2Z4wxuzLcygZEZFngN8xxhzKdyzKxkUzd6UY+QywS0SOiMjXnIUQROT9IvLvIvItEekUkd8QkY/ZnSefF5EG+7hdIvJdEXlJRH4sIldmO5GI/KGI/I59+xkR+ayIvCAiZ0Xkdfb2ChH5ir3ww78CFSmPf6uIPCciL9ux+kWk1l6M4Qr7mEdF5L+s3+VSNiIq7kox8nHgVWPMDVhd9VLZB/wi8Brg00DIWJ0nnwMce+Uh4CPGmJuB3wH+bhnn9hhjXgN8FPiUve3X7fNcZ5/zZgB75Z/fx+rdfRNWu4WPGWPGgd8A/klE7gfqjTFfWEYMirIonsUPUZSi4mljzCQwKSLjwLfs7a8A19ltmW8Hvmb1BgOs/jFLxen4+RLWIgxgrbbzOQBjzDEROWZvP4C1us5P7HP5sD5kMMY8KSLvxupxcv1y/kBFWQoq7kqpEU65nUi5n8B6vbuwFt+4YZXPHyf9/ZNp8EqAJ40x75m3Q8QFXAVMY6381LPCeBQlI2rLKMXIJNZatcvGWIuodNpZs7NY8moz5x8B77Wfbx9wnb39eeC1IrLb3ldpt3gF+C3gFPAe4Ev2egCKsmaouCtFhzFmBMvqOA782Qqe4r3AB0TkKHACa/Wl1fB5wG/bMb+L3YPbGDOEtcD2o/a+54ErbYH/IPDbxpgfY304/P4qY1CUNLQUUlEUpQTRzF1RFKUE0QFVRSG5uPO752z+mjHm0/mIR1FWi9oyiqIoJYjaMoqiKCWIiruiKEoJouKuKIpSgqi4K4qilCD/Fwdb/VUHTk6NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_country_plot(data, 'new_cases_per_million',legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late to the party of why CNN model misbehaves sometimes; problem lies in the activation functions + dense layers; most common complication is that the output layer's weight is w<=0, meaning that upon multiplication with ReLU, it can only provide 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.time_index>=40]\n",
    "model_data = data.copy().iloc[:, 1:]\n",
    "model_data =  model_data#.apply(lambda x :np.log(x+1))\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "model_data = model_data.iloc[:, new_cases_index].to_frame(); new_cases_index=0\n",
    "n_countries = data.location.nunique()\n",
    "target_data = model_data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 28\n",
    "start_date = frame_size\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 132 159\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,0.5), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "(latest_max, latest_min, latest_denom) = latest_minmax\n",
    "(frame_max, frame_min, frame_denom) = frame_minmax\n",
    "(X_cnn_train, y_cnn_train, X_cnn_validate, y_cnn_validate, X_cnn_test, y_cnn_test) = splits\n",
    "\n",
    "X_cnn_train_model = np.concatenate(X_cnn_train.reshape(X_cnn_train.shape[0], X_cnn_train.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_cnn_validate_model = np.concatenate(X_cnn_validate.reshape(X_cnn_validate.shape[0], X_cnn_validate.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_cnn_test_model = np.concatenate(X_cnn_test.reshape(X_cnn_test.shape[0], X_cnn_test.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "y_cnn_train_model = y_cnn_train.ravel()\n",
    "y_cnn_validate_model = y_cnn_validate.ravel()\n",
    "y_cnn_test_model = y_cnn_test.ravel()\n",
    "\n",
    "\n",
    "\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()\n",
    "# y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "# ax.scatter(range(len(X)), X[:, 0, -1, new_cases_index], s=40, color='r')\n",
    "ax.scatter(range(len(X_cnn_train)), X_cnn_train[:,0,-1,new_cases_index], s=20, label='train')\n",
    "ax.scatter(range(len(X_cnn_train), len(X_cnn_train)+len(X_cnn_validate)), X_cnn_validate[:,0,-1,new_cases_index], s=20, label='validate')\n",
    "ax.scatter(range(len(X_cnn_train)+len(X_cnn_validate), len(X)), X_cnn_test[:,0,-1,new_cases_index], s=20, label='test')\n",
    "# ax.plot(np.log(data.new_cases_per_million.values[start_date-1:-1]+1), color='k', alpha=0.2)\n",
    "# ax.plot(y, color='k')\n",
    "plt.legend()\n",
    "_ = plt.show()\n",
    "\n",
    "# Rescale to 0.5 to 1 to account for new maximums. How does this even help?        \n",
    "\n",
    "# Rescale to 0.5 to 1 to account for new maximums. How does this even help?        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 0.0001\n",
    "kernel = 4\n",
    "N = 8\n",
    "FC = 8\n",
    "batch_size = X_cnn_train.shape[0]\n",
    "\n",
    "f1, f2 = 64, 8\n",
    "k1, k2 = 4, 4\n",
    "\n",
    "cnn_model = Sequential()\n",
    "# kernel_initializer0=RandomUniform(minval=0.0, \n",
    "#                                  maxval= 1.0,\n",
    "#                                  seed=0,\n",
    "#                                 dtype=float)\n",
    "\n",
    "cnn_model.add(Conv1D(filters=int(f1), kernel_size=int(k1),\n",
    "                 padding='valid',\n",
    "                 input_shape=X_cnn_train.shape[2:],\n",
    "                 use_bias=False,\n",
    "#                 kernel_constraint=non_neg(),\n",
    "#                  activation='relu',\n",
    "#                 kernel_initializer=kernel_initializer0\n",
    "                )\n",
    "         )\n",
    "\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Activation('relu'))\n",
    "# kernel_initializer1=RandomUniform(minval=0.0, \n",
    "#                                  maxval= 1.0,\n",
    "#                                  seed=0,\n",
    "#                                 dtype=float)\n",
    "\n",
    "cnn_model.add(Conv1D(filters=int(f2), \n",
    "                 kernel_size=int(k2), \n",
    "                 padding='valid',\n",
    "                 use_bias=False,\n",
    "#                  kernel_constraint=non_neg(),\n",
    "\n",
    "#                  activation='relu',\n",
    "#                 kernel_initializer=kernel_initializer1\n",
    "#                   use_bias=False\n",
    "                )\n",
    "         )\n",
    "\n",
    "\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Activation('relu'))\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# kernel_initializer2=RandomUniform(minval=0.0, \n",
    "#                                  maxval= 1.0,\n",
    "#                                  seed=2,\n",
    "#                                 dtype=float)\n",
    "\n",
    "cnn_model.add(Dense(cnn_model.output.shape[1], \n",
    "                activation='relu',\n",
    "                use_bias=False,\n",
    "#                  kernel_constraint=non_neg(),\n",
    "#                 kernel_initializer=kernel_initializer2\n",
    "               )\n",
    "         )\n",
    "\n",
    "kernel_initializer3=RandomUniform(minval=0.0, \n",
    "                                 maxval= 1.0,\n",
    "                                 seed=0,\n",
    "                                dtype=float)\n",
    "\n",
    "cnn_model.add(Dense(1, \n",
    "                    activation='relu',\n",
    "                    use_bias=False,\n",
    "                     kernel_constraint=non_neg(),\n",
    "#                     kernel_initializer=kernel_initializer3\n",
    "                   ))\n",
    "cnn_model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5))\n",
    "(ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "_ = ax1.hist(cnn_model.get_weights()[0].ravel())\n",
    "_ = ax2.hist(cnn_model.get_weights()[1].ravel())\n",
    "_ = ax3.hist(cnn_model.get_weights()[2].ravel())\n",
    "_ = ax4.hist(cnn_model.get_weights()[3].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125 samples, validate on 7 samples\n",
      "Epoch 1/2000\n",
      "125/125 [==============================] - 0s 2ms/sample - loss: 4287.0986 - val_loss: 12585.6357\n",
      "Epoch 2/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3848.5242 - val_loss: 12560.1553\n",
      "Epoch 3/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3837.7410 - val_loss: 12530.6650\n",
      "Epoch 4/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3825.2107 - val_loss: 12499.0361\n",
      "Epoch 5/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3811.8755 - val_loss: 12466.0566\n",
      "Epoch 6/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3798.0503 - val_loss: 12432.1895\n",
      "Epoch 7/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3783.7441 - val_loss: 12397.9238\n",
      "Epoch 8/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3769.2256 - val_loss: 12363.8857\n",
      "Epoch 9/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3754.5095 - val_loss: 12329.6016\n",
      "Epoch 10/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3739.8677 - val_loss: 12294.8848\n",
      "Epoch 11/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3725.3169 - val_loss: 12259.8955\n",
      "Epoch 12/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3710.7954 - val_loss: 12225.8662\n",
      "Epoch 13/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3696.3140 - val_loss: 12192.1982\n",
      "Epoch 14/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3681.9683 - val_loss: 12158.6494\n",
      "Epoch 15/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3667.5869 - val_loss: 12125.9131\n",
      "Epoch 16/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3653.2681 - val_loss: 12094.2041\n",
      "Epoch 17/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3638.8381 - val_loss: 12062.3643\n",
      "Epoch 18/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3624.3557 - val_loss: 12030.3438\n",
      "Epoch 19/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3609.8059 - val_loss: 11998.1875\n",
      "Epoch 20/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3595.2212 - val_loss: 11965.8994\n",
      "Epoch 21/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3580.6074 - val_loss: 11933.4326\n",
      "Epoch 22/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 3565.9150 - val_loss: 11900.7480\n",
      "Epoch 23/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3551.1848 - val_loss: 11867.7393\n",
      "Epoch 24/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3536.3953 - val_loss: 11834.4814\n",
      "Epoch 25/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3521.5498 - val_loss: 11800.9990\n",
      "Epoch 26/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3506.7332 - val_loss: 11767.2881\n",
      "Epoch 27/2000\n",
      "125/125 [==============================] - 0s 112us/sample - loss: 3491.8496 - val_loss: 11733.3447\n",
      "Epoch 28/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3476.9287 - val_loss: 11699.2285\n",
      "Epoch 29/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3461.9390 - val_loss: 11664.8779\n",
      "Epoch 30/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3446.8599 - val_loss: 11630.2354\n",
      "Epoch 31/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3431.7009 - val_loss: 11595.3145\n",
      "Epoch 32/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3416.4578 - val_loss: 11560.0869\n",
      "Epoch 33/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3401.1345 - val_loss: 11524.6582\n",
      "Epoch 34/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3385.7263 - val_loss: 11488.9932\n",
      "Epoch 35/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3370.2290 - val_loss: 11453.1338\n",
      "Epoch 36/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3354.6440 - val_loss: 11416.9717\n",
      "Epoch 37/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3338.9729 - val_loss: 11380.4922\n",
      "Epoch 38/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3323.2178 - val_loss: 11343.7598\n",
      "Epoch 39/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3307.4102 - val_loss: 11306.7646\n",
      "Epoch 40/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3291.4771 - val_loss: 11269.4990\n",
      "Epoch 41/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3275.4150 - val_loss: 11231.9463\n",
      "Epoch 42/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3259.2610 - val_loss: 11194.1162\n",
      "Epoch 43/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3242.9827 - val_loss: 11156.0205\n",
      "Epoch 44/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3226.6021 - val_loss: 11117.6514\n",
      "Epoch 45/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3210.1338 - val_loss: 11079.0215\n",
      "Epoch 46/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3193.5510 - val_loss: 11040.1016\n",
      "Epoch 47/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3176.8704 - val_loss: 11000.8965\n",
      "Epoch 48/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3160.0869 - val_loss: 10961.3604\n",
      "Epoch 49/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3143.1992 - val_loss: 10921.4932\n",
      "Epoch 50/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 3126.2122 - val_loss: 10881.3408\n",
      "Epoch 51/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3109.1235 - val_loss: 10840.9014\n",
      "Epoch 52/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3091.9458 - val_loss: 10800.1709\n",
      "Epoch 53/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3074.6768 - val_loss: 10759.1016\n",
      "Epoch 54/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3057.3064 - val_loss: 10717.7207\n",
      "Epoch 55/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3039.8494 - val_loss: 10676.0322\n",
      "Epoch 56/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3022.3552 - val_loss: 10634.0527\n",
      "Epoch 57/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 3004.7737 - val_loss: 10591.8154\n",
      "Epoch 58/2000\n",
      "125/125 [==============================] - 0s 80us/sample - loss: 2987.1082 - val_loss: 10549.2061\n",
      "Epoch 59/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 2969.3665 - val_loss: 10506.0371\n",
      "Epoch 60/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2951.5310 - val_loss: 10462.5967\n",
      "Epoch 61/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2933.5996 - val_loss: 10418.8369\n",
      "Epoch 62/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 2915.5620 - val_loss: 10374.7646\n",
      "Epoch 63/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2897.4275 - val_loss: 10330.4131\n",
      "Epoch 64/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2879.2009 - val_loss: 10285.7686\n",
      "Epoch 65/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2860.8870 - val_loss: 10240.8389\n",
      "Epoch 66/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2842.4885 - val_loss: 10195.6240\n",
      "Epoch 67/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2823.9973 - val_loss: 10149.9287\n",
      "Epoch 68/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2805.4136 - val_loss: 10103.8262\n",
      "Epoch 69/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2786.7185 - val_loss: 10057.3740\n",
      "Epoch 70/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2767.9246 - val_loss: 10010.5752\n",
      "Epoch 71/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2749.0405 - val_loss: 9963.4092\n",
      "Epoch 72/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2730.0369 - val_loss: 9915.8750\n",
      "Epoch 73/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2710.9343 - val_loss: 9868.0459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2691.7329 - val_loss: 9819.9268\n",
      "Epoch 75/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2672.4338 - val_loss: 9771.4893\n",
      "Epoch 76/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2653.0437 - val_loss: 9722.7451\n",
      "Epoch 77/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2633.5532 - val_loss: 9673.6631\n",
      "Epoch 78/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2613.9773 - val_loss: 9624.2324\n",
      "Epoch 79/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2594.3174 - val_loss: 9574.4111\n",
      "Epoch 80/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2574.5842 - val_loss: 9524.3076\n",
      "Epoch 81/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2554.7861 - val_loss: 9473.8389\n",
      "Epoch 82/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2534.9141 - val_loss: 9423.0986\n",
      "Epoch 83/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2514.9514 - val_loss: 9371.9834\n",
      "Epoch 84/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2494.9055 - val_loss: 9320.5234\n",
      "Epoch 85/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2474.7898 - val_loss: 9268.4170\n",
      "Epoch 86/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2454.5869 - val_loss: 9215.7002\n",
      "Epoch 87/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2434.3000 - val_loss: 9162.5576\n",
      "Epoch 88/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 2413.9368 - val_loss: 9109.0498\n",
      "Epoch 89/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2393.4878 - val_loss: 9055.1074\n",
      "Epoch 90/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2372.9517 - val_loss: 9000.8613\n",
      "Epoch 91/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2352.3286 - val_loss: 8946.3232\n",
      "Epoch 92/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2331.6240 - val_loss: 8891.3838\n",
      "Epoch 93/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2310.8401 - val_loss: 8836.2119\n",
      "Epoch 94/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2289.9932 - val_loss: 8780.7949\n",
      "Epoch 95/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2269.0791 - val_loss: 8725.0859\n",
      "Epoch 96/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2248.0901 - val_loss: 8669.0781\n",
      "Epoch 97/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2227.0525 - val_loss: 8612.7480\n",
      "Epoch 98/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2205.9602 - val_loss: 8556.0439\n",
      "Epoch 99/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2184.8323 - val_loss: 8499.0166\n",
      "Epoch 100/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2163.6604 - val_loss: 8441.7207\n",
      "Epoch 101/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2142.4507 - val_loss: 8384.0967\n",
      "Epoch 102/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2121.2046 - val_loss: 8326.2275\n",
      "Epoch 103/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2099.9338 - val_loss: 8268.1572\n",
      "Epoch 104/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2078.6418 - val_loss: 8209.8408\n",
      "Epoch 105/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2057.3323 - val_loss: 8151.1616\n",
      "Epoch 106/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 2036.0006 - val_loss: 8092.2256\n",
      "Epoch 107/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 2014.6716 - val_loss: 8033.1172\n",
      "Epoch 108/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1993.3280 - val_loss: 7973.7822\n",
      "Epoch 109/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1971.9709 - val_loss: 7914.1587\n",
      "Epoch 110/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1950.5847 - val_loss: 7854.0522\n",
      "Epoch 111/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1929.1649 - val_loss: 7793.5815\n",
      "Epoch 112/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1907.7375 - val_loss: 7732.8091\n",
      "Epoch 113/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1886.2993 - val_loss: 7671.6826\n",
      "Epoch 114/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1864.8759 - val_loss: 7610.2437\n",
      "Epoch 115/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1843.4738 - val_loss: 7548.5679\n",
      "Epoch 116/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1822.0759 - val_loss: 7486.5791\n",
      "Epoch 117/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1800.6798 - val_loss: 7424.4336\n",
      "Epoch 118/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1779.3029 - val_loss: 7362.0552\n",
      "Epoch 119/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1757.9548 - val_loss: 7299.4375\n",
      "Epoch 120/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1736.6646 - val_loss: 7236.6812\n",
      "Epoch 121/2000\n",
      "125/125 [==============================] - 0s 54us/sample - loss: 1715.4592 - val_loss: 7173.8604\n",
      "Epoch 122/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1694.3225 - val_loss: 7110.9932\n",
      "Epoch 123/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1673.2587 - val_loss: 7048.1245\n",
      "Epoch 124/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1652.3016 - val_loss: 6985.2358\n",
      "Epoch 125/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1631.4537 - val_loss: 6922.2959\n",
      "Epoch 126/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1610.7166 - val_loss: 6859.3320\n",
      "Epoch 127/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1590.0702 - val_loss: 6796.2900\n",
      "Epoch 128/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1569.4961 - val_loss: 6733.2812\n",
      "Epoch 129/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1549.0535 - val_loss: 6670.2300\n",
      "Epoch 130/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1528.7399 - val_loss: 6607.0967\n",
      "Epoch 131/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1508.5538 - val_loss: 6543.9756\n",
      "Epoch 132/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1488.5065 - val_loss: 6480.3872\n",
      "Epoch 133/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1468.5662 - val_loss: 6416.8247\n",
      "Epoch 134/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1448.7571 - val_loss: 6353.2612\n",
      "Epoch 135/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1429.0629 - val_loss: 6289.7725\n",
      "Epoch 136/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1409.5601 - val_loss: 6226.2603\n",
      "Epoch 137/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1390.2476 - val_loss: 6162.3447\n",
      "Epoch 138/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1371.1398 - val_loss: 6098.3350\n",
      "Epoch 139/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1352.2024 - val_loss: 6034.2578\n",
      "Epoch 140/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1333.4700 - val_loss: 5970.3516\n",
      "Epoch 141/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1314.8993 - val_loss: 5906.6680\n",
      "Epoch 142/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1296.5082 - val_loss: 5843.2456\n",
      "Epoch 143/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1278.3053 - val_loss: 5779.6016\n",
      "Epoch 144/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1260.3247 - val_loss: 5715.8091\n",
      "Epoch 145/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1242.5931 - val_loss: 5652.3237\n",
      "Epoch 146/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1225.0854 - val_loss: 5589.1260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1207.8517 - val_loss: 5526.2505\n",
      "Epoch 148/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1190.9132 - val_loss: 5463.6362\n",
      "Epoch 149/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1174.2653 - val_loss: 5401.0366\n",
      "Epoch 150/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1157.9142 - val_loss: 5338.4297\n",
      "Epoch 151/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1141.8584 - val_loss: 5275.5938\n",
      "Epoch 152/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1126.1155 - val_loss: 5211.8101\n",
      "Epoch 153/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1110.6981 - val_loss: 5148.5503\n",
      "Epoch 154/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1095.6012 - val_loss: 5085.8403\n",
      "Epoch 155/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1080.8507 - val_loss: 5023.6460\n",
      "Epoch 156/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1066.4421 - val_loss: 4962.0557\n",
      "Epoch 157/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1052.3824 - val_loss: 4901.0269\n",
      "Epoch 158/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 1038.6558 - val_loss: 4840.6304\n",
      "Epoch 159/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1025.2631 - val_loss: 4780.7856\n",
      "Epoch 160/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 1012.2264 - val_loss: 4721.5464\n",
      "Epoch 161/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 999.5426 - val_loss: 4662.9854\n",
      "Epoch 162/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 987.2014 - val_loss: 4605.1553\n",
      "Epoch 163/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 975.2153 - val_loss: 4548.0005\n",
      "Epoch 164/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 963.5798 - val_loss: 4491.6045\n",
      "Epoch 165/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 952.3054 - val_loss: 4435.9951\n",
      "Epoch 166/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 941.3904 - val_loss: 4381.1934\n",
      "Epoch 167/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 930.8307 - val_loss: 4327.2139\n",
      "Epoch 168/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 920.6202 - val_loss: 4274.0518\n",
      "Epoch 169/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 910.7593 - val_loss: 4221.7407\n",
      "Epoch 170/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 901.2462 - val_loss: 4170.1836\n",
      "Epoch 171/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 892.0731 - val_loss: 4119.4653\n",
      "Epoch 172/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 883.2213 - val_loss: 4069.5273\n",
      "Epoch 173/2000\n",
      "125/125 [==============================] - 0s 112us/sample - loss: 874.6772 - val_loss: 4020.4236\n",
      "Epoch 174/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 866.4589 - val_loss: 3972.0659\n",
      "Epoch 175/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 858.5411 - val_loss: 3924.3513\n",
      "Epoch 176/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 850.9289 - val_loss: 3877.5452\n",
      "Epoch 177/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 843.6222 - val_loss: 3831.5352\n",
      "Epoch 178/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 836.6169 - val_loss: 3786.3430\n",
      "Epoch 179/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 829.9049 - val_loss: 3742.1169\n",
      "Epoch 180/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 823.4736 - val_loss: 3698.7583\n",
      "Epoch 181/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 817.3289 - val_loss: 3656.1392\n",
      "Epoch 182/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 811.4501 - val_loss: 3614.1770\n",
      "Epoch 183/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 805.8345 - val_loss: 3572.9709\n",
      "Epoch 184/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 800.4326 - val_loss: 3532.6863\n",
      "Epoch 185/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 795.2164 - val_loss: 3493.2668\n",
      "Epoch 186/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 790.1695 - val_loss: 3454.7192\n",
      "Epoch 187/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 785.3629 - val_loss: 3417.1692\n",
      "Epoch 188/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 780.8456 - val_loss: 3380.6941\n",
      "Epoch 189/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 776.5273 - val_loss: 3344.8628\n",
      "Epoch 190/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 772.4507 - val_loss: 3311.6384\n",
      "Epoch 191/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 768.5712 - val_loss: 3279.1282\n",
      "Epoch 192/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 764.8434 - val_loss: 3247.5862\n",
      "Epoch 193/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 761.2610 - val_loss: 3215.4866\n",
      "Epoch 194/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 757.8321 - val_loss: 3183.3123\n",
      "Epoch 195/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 754.5693 - val_loss: 3151.4810\n",
      "Epoch 196/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 751.4768 - val_loss: 3120.6853\n",
      "Epoch 197/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 748.5464 - val_loss: 3091.3479\n",
      "Epoch 198/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 745.7494 - val_loss: 3063.6047\n",
      "Epoch 199/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 743.0803 - val_loss: 3037.2698\n",
      "Epoch 200/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 740.5399 - val_loss: 3012.5251\n",
      "Epoch 201/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 738.1454 - val_loss: 2988.3855\n",
      "Epoch 202/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 735.8605 - val_loss: 2964.8528\n",
      "Epoch 203/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 733.6741 - val_loss: 2941.4097\n",
      "Epoch 204/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 731.5801 - val_loss: 2918.3840\n",
      "Epoch 205/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 729.5826 - val_loss: 2896.1216\n",
      "Epoch 206/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 727.6794 - val_loss: 2874.9395\n",
      "Epoch 207/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 725.8601 - val_loss: 2855.0603\n",
      "Epoch 208/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 724.1160 - val_loss: 2836.6365\n",
      "Epoch 209/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 722.4225 - val_loss: 2819.8157\n",
      "Epoch 210/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 720.7746 - val_loss: 2805.0574\n",
      "Epoch 211/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 719.2137 - val_loss: 2792.0977\n",
      "Epoch 212/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 717.7051 - val_loss: 2780.0833\n",
      "Epoch 213/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 716.2335 - val_loss: 2768.8411\n",
      "Epoch 214/2000\n",
      "125/125 [==============================] - 0s 80us/sample - loss: 714.7984 - val_loss: 2759.0120\n",
      "Epoch 215/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 713.3786 - val_loss: 2750.4197\n",
      "Epoch 216/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 711.9725 - val_loss: 2743.0833\n",
      "Epoch 217/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 710.5771 - val_loss: 2737.0369\n",
      "Epoch 218/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 709.2385 - val_loss: 2731.4763\n",
      "Epoch 219/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 707.9144 - val_loss: 2727.0720\n",
      "Epoch 220/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 56us/sample - loss: 706.5825 - val_loss: 2723.3599\n",
      "Epoch 221/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 705.2850 - val_loss: 2719.9524\n",
      "Epoch 222/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 703.9959 - val_loss: 2716.9470\n",
      "Epoch 223/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 702.7134 - val_loss: 2714.4021\n",
      "Epoch 224/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 701.4374 - val_loss: 2712.1060\n",
      "Epoch 225/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 700.2370 - val_loss: 2708.1926\n",
      "Epoch 226/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 699.0638 - val_loss: 2701.8533\n",
      "Epoch 227/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 697.9272 - val_loss: 2693.9519\n",
      "Epoch 228/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 696.8305 - val_loss: 2684.4055\n",
      "Epoch 229/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 695.7359 - val_loss: 2673.6467\n",
      "Epoch 230/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 694.6432 - val_loss: 2661.6519\n",
      "Epoch 231/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 693.5629 - val_loss: 2648.8308\n",
      "Epoch 232/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 692.4984 - val_loss: 2635.5630\n",
      "Epoch 233/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 691.4579 - val_loss: 2622.1985\n",
      "Epoch 234/2000\n",
      "125/125 [==============================] - 0s 112us/sample - loss: 690.4461 - val_loss: 2609.0559\n",
      "Epoch 235/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 689.4630 - val_loss: 2596.3940\n",
      "Epoch 236/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 688.5105 - val_loss: 2584.5105\n",
      "Epoch 237/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 687.5843 - val_loss: 2573.6523\n",
      "Epoch 238/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 686.6843 - val_loss: 2563.8567\n",
      "Epoch 239/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 685.8046 - val_loss: 2555.2747\n",
      "Epoch 240/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 684.9384 - val_loss: 2548.0715\n",
      "Epoch 241/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 684.0838 - val_loss: 2541.3884\n",
      "Epoch 242/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 683.2412 - val_loss: 2535.6274\n",
      "Epoch 243/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 682.4005 - val_loss: 2530.8970\n",
      "Epoch 244/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 681.5552 - val_loss: 2527.3931\n",
      "Epoch 245/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 680.6905 - val_loss: 2525.3472\n",
      "Epoch 246/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 679.7846 - val_loss: 2524.5298\n",
      "Epoch 247/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 678.7978 - val_loss: 2525.3169\n",
      "Epoch 248/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 677.6759 - val_loss: 2527.8511\n",
      "Epoch 249/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 676.5453 - val_loss: 2528.8474\n",
      "Epoch 250/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 675.5806 - val_loss: 2529.1472\n",
      "Epoch 251/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 674.6358 - val_loss: 2529.0483\n",
      "Epoch 252/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 673.7346 - val_loss: 2527.9617\n",
      "Epoch 253/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 672.8556 - val_loss: 2525.9543\n",
      "Epoch 254/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 671.9740 - val_loss: 2522.9785\n",
      "Epoch 255/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 671.0908 - val_loss: 2519.0198\n",
      "Epoch 256/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 670.2003 - val_loss: 2514.0691\n",
      "Epoch 257/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 669.2996 - val_loss: 2508.1621\n",
      "Epoch 258/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 668.3909 - val_loss: 2501.4685\n",
      "Epoch 259/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 667.4783 - val_loss: 2493.9099\n",
      "Epoch 260/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 666.5578 - val_loss: 2485.6685\n",
      "Epoch 261/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 665.6302 - val_loss: 2476.8972\n",
      "Epoch 262/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 664.7019 - val_loss: 2467.6255\n",
      "Epoch 263/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 663.7742 - val_loss: 2458.0168\n",
      "Epoch 264/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 662.8796 - val_loss: 2448.6340\n",
      "Epoch 265/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 662.0054 - val_loss: 2439.5798\n",
      "Epoch 266/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 661.1450 - val_loss: 2430.9612\n",
      "Epoch 267/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 660.2975 - val_loss: 2422.8879\n",
      "Epoch 268/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 659.4620 - val_loss: 2415.4038\n",
      "Epoch 269/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 658.6943 - val_loss: 2410.0496\n",
      "Epoch 270/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 657.9198 - val_loss: 2406.5642\n",
      "Epoch 271/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 657.0958 - val_loss: 2404.7517\n",
      "Epoch 272/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 656.2219 - val_loss: 2404.4363\n",
      "Epoch 273/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 655.3974 - val_loss: 2403.9431\n",
      "Epoch 274/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 654.6039 - val_loss: 2403.0759\n",
      "Epoch 275/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 653.8656 - val_loss: 2401.4290\n",
      "Epoch 276/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 653.1202 - val_loss: 2399.0881\n",
      "Epoch 277/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 652.3633 - val_loss: 2396.1389\n",
      "Epoch 278/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 651.5974 - val_loss: 2392.6936\n",
      "Epoch 279/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 650.8278 - val_loss: 2388.7637\n",
      "Epoch 280/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 650.0556 - val_loss: 2384.4253\n",
      "Epoch 281/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 649.2804 - val_loss: 2379.7617\n",
      "Epoch 282/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 648.5040 - val_loss: 2374.8533\n",
      "Epoch 283/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 647.7284 - val_loss: 2369.8149\n",
      "Epoch 284/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 646.9554 - val_loss: 2364.7209\n",
      "Epoch 285/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 646.2286 - val_loss: 2361.2996\n",
      "Epoch 286/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 645.5029 - val_loss: 2359.4128\n",
      "Epoch 287/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 644.7448 - val_loss: 2358.8098\n",
      "Epoch 288/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 643.9716 - val_loss: 2358.6956\n",
      "Epoch 289/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 643.2257 - val_loss: 2358.0173\n",
      "Epoch 290/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 642.4921 - val_loss: 2356.7732\n",
      "Epoch 291/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 641.7546 - val_loss: 2354.9075\n",
      "Epoch 292/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 641.0172 - val_loss: 2352.4910\n",
      "Epoch 293/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 640.2745 - val_loss: 2349.5681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 639.5265 - val_loss: 2346.2214\n",
      "Epoch 295/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 638.7740 - val_loss: 2342.5247\n",
      "Epoch 296/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 638.0242 - val_loss: 2339.6511\n",
      "Epoch 297/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 637.2936 - val_loss: 2337.5598\n",
      "Epoch 298/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 636.5538 - val_loss: 2336.1992\n",
      "Epoch 299/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 635.8047 - val_loss: 2335.5154\n",
      "Epoch 300/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 635.0558 - val_loss: 2334.3167\n",
      "Epoch 301/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 634.3147 - val_loss: 2332.6584\n",
      "Epoch 302/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 633.5684 - val_loss: 2330.5886\n",
      "Epoch 303/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 632.8174 - val_loss: 2328.1729\n",
      "Epoch 304/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 632.0632 - val_loss: 2325.4663\n",
      "Epoch 305/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 631.3125 - val_loss: 2323.6902\n",
      "Epoch 306/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 630.5593 - val_loss: 2322.7754\n",
      "Epoch 307/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 629.8042 - val_loss: 2321.4700\n",
      "Epoch 308/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 629.0507 - val_loss: 2319.8142\n",
      "Epoch 309/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 628.2939 - val_loss: 2317.8513\n",
      "Epoch 310/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 627.5344 - val_loss: 2315.5938\n",
      "Epoch 311/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 626.7723 - val_loss: 2313.0896\n",
      "Epoch 312/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 626.0139 - val_loss: 2310.8799\n",
      "Epoch 313/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 625.2553 - val_loss: 2309.3401\n",
      "Epoch 314/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 624.4899 - val_loss: 2308.4263\n",
      "Epoch 315/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 623.7210 - val_loss: 2307.6958\n",
      "Epoch 316/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 622.9570 - val_loss: 2306.6128\n",
      "Epoch 317/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 622.1912 - val_loss: 2305.2156\n",
      "Epoch 318/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 621.4225 - val_loss: 2303.5398\n",
      "Epoch 319/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 620.6516 - val_loss: 2301.6216\n",
      "Epoch 320/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 619.8784 - val_loss: 2299.5051\n",
      "Epoch 321/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 619.1013 - val_loss: 2297.2380\n",
      "Epoch 322/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 618.3220 - val_loss: 2294.8601\n",
      "Epoch 323/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 617.5406 - val_loss: 2292.4038\n",
      "Epoch 324/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 616.7615 - val_loss: 2290.4429\n",
      "Epoch 325/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 615.9797 - val_loss: 2288.9478\n",
      "Epoch 326/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 615.1933 - val_loss: 2287.8860\n",
      "Epoch 327/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 614.4042 - val_loss: 2286.6487\n",
      "Epoch 328/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 613.6161 - val_loss: 2285.2065\n",
      "Epoch 329/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 612.8256 - val_loss: 2283.6448\n",
      "Epoch 330/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 612.0326 - val_loss: 2281.9832\n",
      "Epoch 331/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 611.2377 - val_loss: 2280.2498\n",
      "Epoch 332/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 610.4421 - val_loss: 2278.3899\n",
      "Epoch 333/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 609.6440 - val_loss: 2276.4602\n",
      "Epoch 334/2000\n",
      "125/125 [==============================] - 0s 48us/sample - loss: 608.8420 - val_loss: 2274.5176\n",
      "Epoch 335/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 608.0378 - val_loss: 2272.5222\n",
      "Epoch 336/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 607.2313 - val_loss: 2270.4517\n",
      "Epoch 337/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 606.4203 - val_loss: 2268.5139\n",
      "Epoch 338/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 605.6067 - val_loss: 2266.7207\n",
      "Epoch 339/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 604.7906 - val_loss: 2265.0789\n",
      "Epoch 340/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 603.9725 - val_loss: 2263.4387\n",
      "Epoch 341/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 603.1522 - val_loss: 2261.8376\n",
      "Epoch 342/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 602.3295 - val_loss: 2260.2759\n",
      "Epoch 343/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 601.5044 - val_loss: 2258.7874\n",
      "Epoch 344/2000\n",
      "125/125 [==============================] - 0s 88us/sample - loss: 600.6768 - val_loss: 2257.3647\n",
      "Epoch 345/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 599.8469 - val_loss: 2256.0005\n",
      "Epoch 346/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 599.0138 - val_loss: 2254.6833\n",
      "Epoch 347/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 598.1782 - val_loss: 2253.4016\n",
      "Epoch 348/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 597.3397 - val_loss: 2252.1428\n",
      "Epoch 349/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 596.5052 - val_loss: 2250.8840\n",
      "Epoch 350/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 595.6741 - val_loss: 2249.5803\n",
      "Epoch 351/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 594.8408 - val_loss: 2248.2534\n",
      "Epoch 352/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 594.0056 - val_loss: 2246.9094\n",
      "Epoch 353/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 593.1693 - val_loss: 2245.5591\n",
      "Epoch 354/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 592.3240 - val_loss: 2244.1851\n",
      "Epoch 355/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 591.4739 - val_loss: 2242.7815\n",
      "Epoch 356/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 590.6210 - val_loss: 2241.3464\n",
      "Epoch 357/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 589.7688 - val_loss: 2239.8547\n",
      "Epoch 358/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 588.9159 - val_loss: 2238.3096\n",
      "Epoch 359/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 588.0546 - val_loss: 2236.7241\n",
      "Epoch 360/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 587.1877 - val_loss: 2235.1028\n",
      "Epoch 361/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 586.3160 - val_loss: 2233.4453\n",
      "Epoch 362/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 585.4421 - val_loss: 2231.2637\n",
      "Epoch 363/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 584.5641 - val_loss: 2228.6169\n",
      "Epoch 364/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 583.6797 - val_loss: 2225.5667\n",
      "Epoch 365/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 582.7902 - val_loss: 2222.1978\n",
      "Epoch 366/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 581.8953 - val_loss: 2218.5811\n",
      "Epoch 367/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 580.9958 - val_loss: 2214.7859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 580.0947 - val_loss: 2210.8748\n",
      "Epoch 369/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 579.1912 - val_loss: 2206.9104\n",
      "Epoch 370/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 578.2859 - val_loss: 2202.9490\n",
      "Epoch 371/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 577.3782 - val_loss: 2199.0413\n",
      "Epoch 372/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 576.4677 - val_loss: 2195.7559\n",
      "Epoch 373/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 575.5508 - val_loss: 2193.0674\n",
      "Epoch 374/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 574.6304 - val_loss: 2190.2778\n",
      "Epoch 375/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 573.7145 - val_loss: 2187.4939\n",
      "Epoch 376/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 572.7985 - val_loss: 2184.9902\n",
      "Epoch 377/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 571.8750 - val_loss: 2182.7776\n",
      "Epoch 378/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 570.9523 - val_loss: 2180.6235\n",
      "Epoch 379/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 570.0258 - val_loss: 2178.5508\n",
      "Epoch 380/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 569.0952 - val_loss: 2176.6401\n",
      "Epoch 381/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 568.1593 - val_loss: 2174.8879\n",
      "Epoch 382/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 567.2133 - val_loss: 2173.4751\n",
      "Epoch 383/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 566.2626 - val_loss: 2172.3611\n",
      "Epoch 384/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 565.3080 - val_loss: 2171.4714\n",
      "Epoch 385/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 564.3514 - val_loss: 2170.9573\n",
      "Epoch 386/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 563.4036 - val_loss: 2170.5959\n",
      "Epoch 387/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 562.4568 - val_loss: 2170.3113\n",
      "Epoch 388/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 561.5073 - val_loss: 2170.0527\n",
      "Epoch 389/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 560.5552 - val_loss: 2169.7334\n",
      "Epoch 390/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 559.6027 - val_loss: 2169.3164\n",
      "Epoch 391/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 558.6516 - val_loss: 2168.7827\n",
      "Epoch 392/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 557.6981 - val_loss: 2168.1067\n",
      "Epoch 393/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 556.7425 - val_loss: 2167.2649\n",
      "Epoch 394/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 555.7839 - val_loss: 2166.2441\n",
      "Epoch 395/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 554.8252 - val_loss: 2165.0356\n",
      "Epoch 396/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 553.8647 - val_loss: 2163.6372\n",
      "Epoch 397/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 552.8999 - val_loss: 2162.1169\n",
      "Epoch 398/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 551.9378 - val_loss: 2160.4885\n",
      "Epoch 399/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 550.9750 - val_loss: 2158.7769\n",
      "Epoch 400/2000\n",
      "125/125 [==============================] - 0s 54us/sample - loss: 550.0121 - val_loss: 2156.9744\n",
      "Epoch 401/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 549.0488 - val_loss: 2155.1106\n",
      "Epoch 402/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 548.0798 - val_loss: 2153.1973\n",
      "Epoch 403/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 547.1094 - val_loss: 2151.2422\n",
      "Epoch 404/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 546.1365 - val_loss: 2149.3696\n",
      "Epoch 405/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 545.1661 - val_loss: 2147.4941\n",
      "Epoch 406/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 544.1796 - val_loss: 2145.6072\n",
      "Epoch 407/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 543.1935 - val_loss: 2143.6870\n",
      "Epoch 408/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 542.2048 - val_loss: 2141.7778\n",
      "Epoch 409/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 541.2126 - val_loss: 2139.8833\n",
      "Epoch 410/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 540.2173 - val_loss: 2137.9998\n",
      "Epoch 411/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 539.2195 - val_loss: 2136.1321\n",
      "Epoch 412/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 538.2193 - val_loss: 2134.2502\n",
      "Epoch 413/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 537.2161 - val_loss: 2132.3955\n",
      "Epoch 414/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 536.2109 - val_loss: 2130.5659\n",
      "Epoch 415/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 535.2065 - val_loss: 2128.6956\n",
      "Epoch 416/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 534.1999 - val_loss: 2126.8157\n",
      "Epoch 417/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 533.1910 - val_loss: 2124.9292\n",
      "Epoch 418/2000\n",
      "125/125 [==============================] - 0s 168us/sample - loss: 532.1795 - val_loss: 2123.0364\n",
      "Epoch 419/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 531.1649 - val_loss: 2121.1431\n",
      "Epoch 420/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 530.1481 - val_loss: 2119.2234\n",
      "Epoch 421/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 529.1285 - val_loss: 2117.2803\n",
      "Epoch 422/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 528.1066 - val_loss: 2115.4148\n",
      "Epoch 423/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 527.0828 - val_loss: 2113.5679\n",
      "Epoch 424/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 526.0533 - val_loss: 2111.8035\n",
      "Epoch 425/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 525.0197 - val_loss: 2110.1470\n",
      "Epoch 426/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 523.9813 - val_loss: 2108.5825\n",
      "Epoch 427/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 522.9407 - val_loss: 2107.0657\n",
      "Epoch 428/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 521.8975 - val_loss: 2105.5789\n",
      "Epoch 429/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 520.8514 - val_loss: 2104.1040\n",
      "Epoch 430/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 519.8016 - val_loss: 2102.6348\n",
      "Epoch 431/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 518.7495 - val_loss: 2101.0151\n",
      "Epoch 432/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 517.6885 - val_loss: 2099.2859\n",
      "Epoch 433/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 516.6229 - val_loss: 2097.3384\n",
      "Epoch 434/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 515.5534 - val_loss: 2095.4084\n",
      "Epoch 435/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 514.4807 - val_loss: 2093.4514\n",
      "Epoch 436/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 513.4044 - val_loss: 2091.4512\n",
      "Epoch 437/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 512.3233 - val_loss: 2089.4248\n",
      "Epoch 438/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 511.2352 - val_loss: 2087.3582\n",
      "Epoch 439/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 510.1446 - val_loss: 2085.2322\n",
      "Epoch 440/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 509.0541 - val_loss: 2083.0444\n",
      "Epoch 441/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 507.9608 - val_loss: 2080.8481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 506.8598 - val_loss: 2078.6553\n",
      "Epoch 443/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 505.7601 - val_loss: 2076.3853\n",
      "Epoch 444/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 504.6640 - val_loss: 2073.7947\n",
      "Epoch 445/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 503.5812 - val_loss: 2070.8855\n",
      "Epoch 446/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 502.5030 - val_loss: 2067.4558\n",
      "Epoch 447/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 501.4308 - val_loss: 2063.6440\n",
      "Epoch 448/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 500.3520 - val_loss: 2059.7739\n",
      "Epoch 449/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 499.2693 - val_loss: 2055.8364\n",
      "Epoch 450/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 498.1832 - val_loss: 2051.7754\n",
      "Epoch 451/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 497.0941 - val_loss: 2047.7704\n",
      "Epoch 452/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 496.0055 - val_loss: 2043.8298\n",
      "Epoch 453/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 494.9259 - val_loss: 2039.8859\n",
      "Epoch 454/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 493.8474 - val_loss: 2035.9412\n",
      "Epoch 455/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 492.7702 - val_loss: 2031.9857\n",
      "Epoch 456/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 491.6962 - val_loss: 2028.0248\n",
      "Epoch 457/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 490.6285 - val_loss: 2023.9941\n",
      "Epoch 458/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 489.5833 - val_loss: 2018.9587\n",
      "Epoch 459/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 488.5415 - val_loss: 2013.0713\n",
      "Epoch 460/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 487.5002 - val_loss: 2006.5334\n",
      "Epoch 461/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 486.4587 - val_loss: 1999.5491\n",
      "Epoch 462/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 485.4176 - val_loss: 1992.3416\n",
      "Epoch 463/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 484.3802 - val_loss: 1985.1366\n",
      "Epoch 464/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 483.3470 - val_loss: 1978.1127\n",
      "Epoch 465/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 482.3183 - val_loss: 1971.4095\n",
      "Epoch 466/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 481.2932 - val_loss: 1965.1521\n",
      "Epoch 467/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 480.2689 - val_loss: 1959.4919\n",
      "Epoch 468/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 479.2446 - val_loss: 1954.4869\n",
      "Epoch 469/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 478.2182 - val_loss: 1950.2142\n",
      "Epoch 470/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 477.1893 - val_loss: 1946.6515\n",
      "Epoch 471/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 476.1609 - val_loss: 1943.6896\n",
      "Epoch 472/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 475.1436 - val_loss: 1941.0758\n",
      "Epoch 473/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 474.1311 - val_loss: 1938.7588\n",
      "Epoch 474/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 473.1257 - val_loss: 1936.5927\n",
      "Epoch 475/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 472.1201 - val_loss: 1934.5992\n",
      "Epoch 476/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 471.1128 - val_loss: 1932.9143\n",
      "Epoch 477/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 470.0992 - val_loss: 1931.4352\n",
      "Epoch 478/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 469.0882 - val_loss: 1929.9889\n",
      "Epoch 479/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 468.0815 - val_loss: 1928.4622\n",
      "Epoch 480/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 467.0768 - val_loss: 1926.7991\n",
      "Epoch 481/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 466.0740 - val_loss: 1924.9244\n",
      "Epoch 482/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 465.0708 - val_loss: 1922.8359\n",
      "Epoch 483/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 464.0637 - val_loss: 1920.6481\n",
      "Epoch 484/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 463.0500 - val_loss: 1918.3658\n",
      "Epoch 485/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 462.0349 - val_loss: 1915.9965\n",
      "Epoch 486/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 461.0190 - val_loss: 1913.4655\n",
      "Epoch 487/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 460.0062 - val_loss: 1910.7673\n",
      "Epoch 488/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 458.9916 - val_loss: 1907.9177\n",
      "Epoch 489/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 457.9765 - val_loss: 1904.9265\n",
      "Epoch 490/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 456.9628 - val_loss: 1901.8401\n",
      "Epoch 491/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 455.9492 - val_loss: 1898.6833\n",
      "Epoch 492/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 454.9356 - val_loss: 1895.5410\n",
      "Epoch 493/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 453.9254 - val_loss: 1892.3588\n",
      "Epoch 494/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 452.9168 - val_loss: 1889.0110\n",
      "Epoch 495/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 451.8957 - val_loss: 1885.8173\n",
      "Epoch 496/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 450.8702 - val_loss: 1882.8051\n",
      "Epoch 497/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 449.8417 - val_loss: 1879.9728\n",
      "Epoch 498/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 448.8124 - val_loss: 1877.3123\n",
      "Epoch 499/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 447.7813 - val_loss: 1874.8395\n",
      "Epoch 500/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 446.7477 - val_loss: 1872.5822\n",
      "Epoch 501/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 445.7151 - val_loss: 1870.3695\n",
      "Epoch 502/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 444.6970 - val_loss: 1868.0929\n",
      "Epoch 503/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 443.6889 - val_loss: 1865.6366\n",
      "Epoch 504/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 442.6881 - val_loss: 1863.0178\n",
      "Epoch 505/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 441.6896 - val_loss: 1860.2402\n",
      "Epoch 506/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 440.6949 - val_loss: 1857.2858\n",
      "Epoch 507/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 439.7046 - val_loss: 1854.1973\n",
      "Epoch 508/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 438.7172 - val_loss: 1850.9481\n",
      "Epoch 509/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 437.7312 - val_loss: 1847.5975\n",
      "Epoch 510/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 436.7459 - val_loss: 1844.2035\n",
      "Epoch 511/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 435.7628 - val_loss: 1840.7380\n",
      "Epoch 512/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 434.7814 - val_loss: 1837.2559\n",
      "Epoch 513/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 433.8006 - val_loss: 1833.8177\n",
      "Epoch 514/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 432.8197 - val_loss: 1830.4110\n",
      "Epoch 515/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 431.8397 - val_loss: 1827.0841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 430.8605 - val_loss: 1823.8435\n",
      "Epoch 517/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 429.8817 - val_loss: 1820.7201\n",
      "Epoch 518/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 428.9036 - val_loss: 1817.7528\n",
      "Epoch 519/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 427.9260 - val_loss: 1814.9463\n",
      "Epoch 520/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 426.9487 - val_loss: 1812.3453\n",
      "Epoch 521/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 425.9716 - val_loss: 1809.9144\n",
      "Epoch 522/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 424.9945 - val_loss: 1807.6569\n",
      "Epoch 523/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 424.0176 - val_loss: 1805.5148\n",
      "Epoch 524/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 423.0417 - val_loss: 1803.4835\n",
      "Epoch 525/2000\n",
      "125/125 [==============================] - 0s 88us/sample - loss: 422.0699 - val_loss: 1801.5924\n",
      "Epoch 526/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 421.1015 - val_loss: 1799.7256\n",
      "Epoch 527/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 420.1324 - val_loss: 1797.8541\n",
      "Epoch 528/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 419.1645 - val_loss: 1795.9357\n",
      "Epoch 529/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 418.1971 - val_loss: 1793.9469\n",
      "Epoch 530/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 417.2304 - val_loss: 1791.8787\n",
      "Epoch 531/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 416.2645 - val_loss: 1789.7135\n",
      "Epoch 532/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 415.3009 - val_loss: 1786.5273\n",
      "Epoch 533/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 414.3413 - val_loss: 1782.4724\n",
      "Epoch 534/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 413.3802 - val_loss: 1777.7682\n",
      "Epoch 535/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 412.4207 - val_loss: 1772.6163\n",
      "Epoch 536/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 411.4611 - val_loss: 1767.2435\n",
      "Epoch 537/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 410.5080 - val_loss: 1762.8190\n",
      "Epoch 538/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 409.5587 - val_loss: 1759.3445\n",
      "Epoch 539/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 408.6076 - val_loss: 1756.7970\n",
      "Epoch 540/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 407.6566 - val_loss: 1754.1510\n",
      "Epoch 541/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 406.7100 - val_loss: 1751.4374\n",
      "Epoch 542/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 405.7637 - val_loss: 1748.6853\n",
      "Epoch 543/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 404.8218 - val_loss: 1745.8303\n",
      "Epoch 544/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 403.8828 - val_loss: 1742.9088\n",
      "Epoch 545/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 402.9454 - val_loss: 1739.9789\n",
      "Epoch 546/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 402.0090 - val_loss: 1737.0677\n",
      "Epoch 547/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 401.0719 - val_loss: 1734.2050\n",
      "Epoch 548/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 400.1361 - val_loss: 1731.4144\n",
      "Epoch 549/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 399.2014 - val_loss: 1728.7345\n",
      "Epoch 550/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 398.2678 - val_loss: 1726.1378\n",
      "Epoch 551/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 397.3348 - val_loss: 1723.6418\n",
      "Epoch 552/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 396.4065 - val_loss: 1721.2478\n",
      "Epoch 553/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 395.4980 - val_loss: 1718.8800\n",
      "Epoch 554/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 394.5896 - val_loss: 1717.5123\n",
      "Epoch 555/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 393.6793 - val_loss: 1716.0167\n",
      "Epoch 556/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 392.7685 - val_loss: 1714.3253\n",
      "Epoch 557/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 391.8576 - val_loss: 1712.4076\n",
      "Epoch 558/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 390.9499 - val_loss: 1710.2365\n",
      "Epoch 559/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 390.0468 - val_loss: 1707.8147\n",
      "Epoch 560/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 389.1444 - val_loss: 1705.2235\n",
      "Epoch 561/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 388.2427 - val_loss: 1702.4844\n",
      "Epoch 562/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 387.3422 - val_loss: 1699.6288\n",
      "Epoch 563/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 386.4433 - val_loss: 1696.6963\n",
      "Epoch 564/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 385.5556 - val_loss: 1694.7177\n",
      "Epoch 565/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 384.6675 - val_loss: 1693.5653\n",
      "Epoch 566/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 383.7820 - val_loss: 1692.0787\n",
      "Epoch 567/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 382.8979 - val_loss: 1690.1892\n",
      "Epoch 568/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 382.0189 - val_loss: 1687.8920\n",
      "Epoch 569/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 381.1406 - val_loss: 1685.2107\n",
      "Epoch 570/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 380.2633 - val_loss: 1682.2308\n",
      "Epoch 571/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 379.3832 - val_loss: 1679.0234\n",
      "Epoch 572/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 378.5056 - val_loss: 1675.6676\n",
      "Epoch 573/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 377.6341 - val_loss: 1672.2277\n",
      "Epoch 574/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 376.7658 - val_loss: 1668.7871\n",
      "Epoch 575/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 375.8989 - val_loss: 1665.4127\n",
      "Epoch 576/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 375.0337 - val_loss: 1663.1633\n",
      "Epoch 577/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 374.1700 - val_loss: 1660.8192\n",
      "Epoch 578/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 373.3094 - val_loss: 1658.3600\n",
      "Epoch 579/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 372.4562 - val_loss: 1655.7715\n",
      "Epoch 580/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 371.6051 - val_loss: 1653.0670\n",
      "Epoch 581/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 370.7550 - val_loss: 1650.2654\n",
      "Epoch 582/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 369.9063 - val_loss: 1647.3909\n",
      "Epoch 583/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 369.0590 - val_loss: 1644.4769\n",
      "Epoch 584/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 368.2126 - val_loss: 1641.5565\n",
      "Epoch 585/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 367.3724 - val_loss: 1638.6688\n",
      "Epoch 586/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 366.5351 - val_loss: 1635.8284\n",
      "Epoch 587/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 365.6998 - val_loss: 1633.0648\n",
      "Epoch 588/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 364.8675 - val_loss: 1630.3627\n",
      "Epoch 589/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 364.0380 - val_loss: 1627.7382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 363.2113 - val_loss: 1625.2058\n",
      "Epoch 591/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 362.3872 - val_loss: 1622.7335\n",
      "Epoch 592/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 361.5660 - val_loss: 1620.2805\n",
      "Epoch 593/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 360.7481 - val_loss: 1617.8279\n",
      "Epoch 594/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 359.9328 - val_loss: 1615.3658\n",
      "Epoch 595/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 359.1202 - val_loss: 1612.8829\n",
      "Epoch 596/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 358.3118 - val_loss: 1610.4315\n",
      "Epoch 597/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 357.5039 - val_loss: 1608.0182\n",
      "Epoch 598/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 356.6985 - val_loss: 1605.6222\n",
      "Epoch 599/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 355.8973 - val_loss: 1603.2191\n",
      "Epoch 600/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 355.0986 - val_loss: 1600.7927\n",
      "Epoch 601/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 354.3033 - val_loss: 1598.3453\n",
      "Epoch 602/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 353.5106 - val_loss: 1595.8787\n",
      "Epoch 603/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 352.7206 - val_loss: 1593.4015\n",
      "Epoch 604/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 351.9334 - val_loss: 1590.9131\n",
      "Epoch 605/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 351.1488 - val_loss: 1588.4180\n",
      "Epoch 606/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 350.3667 - val_loss: 1585.9213\n",
      "Epoch 607/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 349.5875 - val_loss: 1583.4191\n",
      "Epoch 608/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 348.8107 - val_loss: 1580.9547\n",
      "Epoch 609/2000\n",
      "125/125 [==============================] - 0s 104us/sample - loss: 348.0338 - val_loss: 1578.5245\n",
      "Epoch 610/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 347.2586 - val_loss: 1576.1169\n",
      "Epoch 611/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 346.4855 - val_loss: 1573.7209\n",
      "Epoch 612/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 345.7152 - val_loss: 1571.3239\n",
      "Epoch 613/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 344.9464 - val_loss: 1568.9209\n",
      "Epoch 614/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 344.1811 - val_loss: 1566.4967\n",
      "Epoch 615/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 343.4182 - val_loss: 1564.0447\n",
      "Epoch 616/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 342.6581 - val_loss: 1561.5081\n",
      "Epoch 617/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 341.9012 - val_loss: 1558.8912\n",
      "Epoch 618/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 341.1465 - val_loss: 1556.2155\n",
      "Epoch 619/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 340.3959 - val_loss: 1553.3800\n",
      "Epoch 620/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 339.6515 - val_loss: 1550.4220\n",
      "Epoch 621/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 338.9119 - val_loss: 1547.4000\n",
      "Epoch 622/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 338.1752 - val_loss: 1544.3698\n",
      "Epoch 623/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 337.4407 - val_loss: 1541.3867\n",
      "Epoch 624/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 336.7096 - val_loss: 1538.4941\n",
      "Epoch 625/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 335.9814 - val_loss: 1535.7148\n",
      "Epoch 626/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 335.2558 - val_loss: 1533.0748\n",
      "Epoch 627/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 334.5333 - val_loss: 1530.5997\n",
      "Epoch 628/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 333.8136 - val_loss: 1528.2455\n",
      "Epoch 629/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 333.0968 - val_loss: 1525.9930\n",
      "Epoch 630/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 332.3830 - val_loss: 1523.8177\n",
      "Epoch 631/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 331.6721 - val_loss: 1521.7100\n",
      "Epoch 632/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 330.9629 - val_loss: 1519.6936\n",
      "Epoch 633/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 330.2545 - val_loss: 1517.7302\n",
      "Epoch 634/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 329.5471 - val_loss: 1515.7682\n",
      "Epoch 635/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 328.8479 - val_loss: 1513.6858\n",
      "Epoch 636/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 328.1532 - val_loss: 1511.4573\n",
      "Epoch 637/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 327.4615 - val_loss: 1509.1077\n",
      "Epoch 638/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 326.7709 - val_loss: 1506.6505\n",
      "Epoch 639/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 326.0807 - val_loss: 1504.1425\n",
      "Epoch 640/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 325.3927 - val_loss: 1501.6078\n",
      "Epoch 641/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 324.7068 - val_loss: 1499.0695\n",
      "Epoch 642/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 324.0233 - val_loss: 1496.5442\n",
      "Epoch 643/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 323.3420 - val_loss: 1494.0339\n",
      "Epoch 644/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 322.6628 - val_loss: 1491.6556\n",
      "Epoch 645/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 321.9909 - val_loss: 1489.3752\n",
      "Epoch 646/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 321.3198 - val_loss: 1487.1815\n",
      "Epoch 647/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 320.6482 - val_loss: 1484.9475\n",
      "Epoch 648/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 319.9809 - val_loss: 1482.6722\n",
      "Epoch 649/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 319.3156 - val_loss: 1480.3313\n",
      "Epoch 650/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 318.6533 - val_loss: 1478.0192\n",
      "Epoch 651/2000\n",
      "125/125 [==============================] - 0s 88us/sample - loss: 317.9942 - val_loss: 1475.6530\n",
      "Epoch 652/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 317.3376 - val_loss: 1473.2440\n",
      "Epoch 653/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 316.6848 - val_loss: 1470.9377\n",
      "Epoch 654/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 316.0312 - val_loss: 1468.7148\n",
      "Epoch 655/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 315.3790 - val_loss: 1466.4797\n",
      "Epoch 656/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 314.7294 - val_loss: 1464.2296\n",
      "Epoch 657/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 314.0819 - val_loss: 1461.9680\n",
      "Epoch 658/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 313.4385 - val_loss: 1459.6604\n",
      "Epoch 659/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 312.7994 - val_loss: 1457.3270\n",
      "Epoch 660/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 312.1628 - val_loss: 1455.0887\n",
      "Epoch 661/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 311.5287 - val_loss: 1452.9171\n",
      "Epoch 662/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 310.8981 - val_loss: 1450.7488\n",
      "Epoch 663/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 310.2696 - val_loss: 1448.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 309.6449 - val_loss: 1446.3480\n",
      "Epoch 665/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 309.0209 - val_loss: 1444.1326\n",
      "Epoch 666/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 308.3978 - val_loss: 1442.0051\n",
      "Epoch 667/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 307.7759 - val_loss: 1439.8219\n",
      "Epoch 668/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 307.1568 - val_loss: 1437.5865\n",
      "Epoch 669/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 306.5401 - val_loss: 1435.4279\n",
      "Epoch 670/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 305.9259 - val_loss: 1433.3080\n",
      "Epoch 671/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 305.3143 - val_loss: 1431.1257\n",
      "Epoch 672/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 304.7057 - val_loss: 1428.8698\n",
      "Epoch 673/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 304.0998 - val_loss: 1426.6501\n",
      "Epoch 674/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 303.4961 - val_loss: 1424.4746\n",
      "Epoch 675/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 302.8949 - val_loss: 1422.2540\n",
      "Epoch 676/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 302.2961 - val_loss: 1419.9778\n",
      "Epoch 677/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 301.7003 - val_loss: 1417.5929\n",
      "Epoch 678/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 301.0993 - val_loss: 1415.1028\n",
      "Epoch 679/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 300.4957 - val_loss: 1412.5918\n",
      "Epoch 680/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 299.8922 - val_loss: 1409.7960\n",
      "Epoch 681/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 299.2894 - val_loss: 1406.7550\n",
      "Epoch 682/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 298.6892 - val_loss: 1403.4156\n",
      "Epoch 683/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 298.0878 - val_loss: 1399.9685\n",
      "Epoch 684/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 297.4846 - val_loss: 1396.5897\n",
      "Epoch 685/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 296.8631 - val_loss: 1393.4127\n",
      "Epoch 686/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 296.2336 - val_loss: 1390.6879\n",
      "Epoch 687/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 295.5947 - val_loss: 1388.5706\n",
      "Epoch 688/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 294.9545 - val_loss: 1387.0363\n",
      "Epoch 689/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 294.3226 - val_loss: 1385.1174\n",
      "Epoch 690/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 293.6919 - val_loss: 1382.7823\n",
      "Epoch 691/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 293.0551 - val_loss: 1380.2600\n",
      "Epoch 692/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 292.4191 - val_loss: 1377.4736\n",
      "Epoch 693/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 291.7808 - val_loss: 1374.6018\n",
      "Epoch 694/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 291.1428 - val_loss: 1371.7559\n",
      "Epoch 695/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 290.5088 - val_loss: 1369.2444\n",
      "Epoch 696/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 289.8729 - val_loss: 1367.0566\n",
      "Epoch 697/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 289.2330 - val_loss: 1364.2605\n",
      "Epoch 698/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 288.5718 - val_loss: 1359.6729\n",
      "Epoch 699/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 287.8993 - val_loss: 1354.0432\n",
      "Epoch 700/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 287.2282 - val_loss: 1347.4183\n",
      "Epoch 701/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 286.5280 - val_loss: 1340.0579\n",
      "Epoch 702/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 285.8273 - val_loss: 1333.0948\n",
      "Epoch 703/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 285.1477 - val_loss: 1326.9680\n",
      "Epoch 704/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 284.4717 - val_loss: 1321.7830\n",
      "Epoch 705/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 283.7757 - val_loss: 1317.6777\n",
      "Epoch 706/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 283.1256 - val_loss: 1315.0066\n",
      "Epoch 707/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 282.4843 - val_loss: 1313.1173\n",
      "Epoch 708/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 281.8499 - val_loss: 1312.1758\n",
      "Epoch 709/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 281.2412 - val_loss: 1311.1169\n",
      "Epoch 710/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 280.6433 - val_loss: 1309.6526\n",
      "Epoch 711/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 280.0440 - val_loss: 1307.4674\n",
      "Epoch 712/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 279.4337 - val_loss: 1304.6222\n",
      "Epoch 713/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 278.8169 - val_loss: 1301.2887\n",
      "Epoch 714/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 278.2007 - val_loss: 1297.2041\n",
      "Epoch 715/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 277.5893 - val_loss: 1292.9132\n",
      "Epoch 716/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 276.9918 - val_loss: 1288.7548\n",
      "Epoch 717/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 276.4045 - val_loss: 1284.5594\n",
      "Epoch 718/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 275.8112 - val_loss: 1280.9061\n",
      "Epoch 719/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 275.2479 - val_loss: 1277.9808\n",
      "Epoch 720/2000\n",
      "125/125 [==============================] - 0s 128us/sample - loss: 274.7005 - val_loss: 1275.6198\n",
      "Epoch 721/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 274.1649 - val_loss: 1273.7491\n",
      "Epoch 722/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 273.6340 - val_loss: 1272.1764\n",
      "Epoch 723/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 273.1051 - val_loss: 1270.9396\n",
      "Epoch 724/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 272.5787 - val_loss: 1269.8490\n",
      "Epoch 725/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 272.0551 - val_loss: 1268.7499\n",
      "Epoch 726/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 271.5357 - val_loss: 1267.7240\n",
      "Epoch 727/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 271.0265 - val_loss: 1265.8488\n",
      "Epoch 728/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 270.5243 - val_loss: 1263.2030\n",
      "Epoch 729/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 270.0237 - val_loss: 1259.9447\n",
      "Epoch 730/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 269.5314 - val_loss: 1256.2208\n",
      "Epoch 731/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 269.0418 - val_loss: 1252.3538\n",
      "Epoch 732/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 268.5626 - val_loss: 1248.9520\n",
      "Epoch 733/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 268.0851 - val_loss: 1246.4967\n",
      "Epoch 734/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 267.6063 - val_loss: 1244.9264\n",
      "Epoch 735/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 267.1332 - val_loss: 1244.0734\n",
      "Epoch 736/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 266.6583 - val_loss: 1243.7426\n",
      "Epoch 737/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 266.1858 - val_loss: 1243.1848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 265.7196 - val_loss: 1242.2904\n",
      "Epoch 739/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 265.2557 - val_loss: 1240.9902\n",
      "Epoch 740/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 264.7954 - val_loss: 1239.3650\n",
      "Epoch 741/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 264.3405 - val_loss: 1237.4023\n",
      "Epoch 742/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 263.8898 - val_loss: 1234.8795\n",
      "Epoch 743/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 263.4419 - val_loss: 1232.0050\n",
      "Epoch 744/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 262.9962 - val_loss: 1229.0372\n",
      "Epoch 745/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 262.5556 - val_loss: 1226.3663\n",
      "Epoch 746/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 262.1147 - val_loss: 1224.2233\n",
      "Epoch 747/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 261.6768 - val_loss: 1222.6277\n",
      "Epoch 748/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 261.2464 - val_loss: 1221.2843\n",
      "Epoch 749/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 260.8220 - val_loss: 1219.9869\n",
      "Epoch 750/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 260.4026 - val_loss: 1218.9325\n",
      "Epoch 751/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 259.9852 - val_loss: 1218.1378\n",
      "Epoch 752/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 259.5703 - val_loss: 1217.4755\n",
      "Epoch 753/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 259.1579 - val_loss: 1216.8279\n",
      "Epoch 754/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 258.7528 - val_loss: 1215.8684\n",
      "Epoch 755/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 258.3526 - val_loss: 1214.6521\n",
      "Epoch 756/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 257.9529 - val_loss: 1213.1058\n",
      "Epoch 757/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 257.5551 - val_loss: 1211.2676\n",
      "Epoch 758/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 257.1595 - val_loss: 1209.2037\n",
      "Epoch 759/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 256.7672 - val_loss: 1207.1263\n",
      "Epoch 760/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 256.3770 - val_loss: 1205.0809\n",
      "Epoch 761/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 255.9924 - val_loss: 1203.5090\n",
      "Epoch 762/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 255.5995 - val_loss: 1201.9586\n",
      "Epoch 763/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 255.2130 - val_loss: 1200.1053\n",
      "Epoch 764/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 254.8339 - val_loss: 1197.9745\n",
      "Epoch 765/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 254.4609 - val_loss: 1195.7103\n",
      "Epoch 766/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 254.0901 - val_loss: 1193.4728\n",
      "Epoch 767/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 253.7211 - val_loss: 1191.2665\n",
      "Epoch 768/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 253.3539 - val_loss: 1189.2408\n",
      "Epoch 769/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 252.9884 - val_loss: 1187.4707\n",
      "Epoch 770/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 252.6243 - val_loss: 1185.9894\n",
      "Epoch 771/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 252.2608 - val_loss: 1184.7903\n",
      "Epoch 772/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 251.8994 - val_loss: 1183.7102\n",
      "Epoch 773/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 251.5411 - val_loss: 1182.6877\n",
      "Epoch 774/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 251.1850 - val_loss: 1181.6267\n",
      "Epoch 775/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 250.8309 - val_loss: 1180.2673\n",
      "Epoch 776/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 250.4842 - val_loss: 1178.7637\n",
      "Epoch 777/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 250.1364 - val_loss: 1177.1173\n",
      "Epoch 778/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 249.7928 - val_loss: 1175.1204\n",
      "Epoch 779/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 249.4498 - val_loss: 1172.9915\n",
      "Epoch 780/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 249.1082 - val_loss: 1170.8466\n",
      "Epoch 781/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 248.7701 - val_loss: 1168.3395\n",
      "Epoch 782/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 248.4354 - val_loss: 1165.3593\n",
      "Epoch 783/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 248.1043 - val_loss: 1162.1797\n",
      "Epoch 784/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 247.7753 - val_loss: 1159.1318\n",
      "Epoch 785/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 247.4490 - val_loss: 1156.4235\n",
      "Epoch 786/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 247.1236 - val_loss: 1154.2089\n",
      "Epoch 787/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 246.7994 - val_loss: 1152.5459\n",
      "Epoch 788/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 246.4761 - val_loss: 1151.3127\n",
      "Epoch 789/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 246.1547 - val_loss: 1149.6671\n",
      "Epoch 790/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 245.8363 - val_loss: 1147.6981\n",
      "Epoch 791/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 245.5190 - val_loss: 1145.4978\n",
      "Epoch 792/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 245.2027 - val_loss: 1143.2223\n",
      "Epoch 793/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 244.8887 - val_loss: 1141.1559\n",
      "Epoch 794/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 244.5751 - val_loss: 1139.2650\n",
      "Epoch 795/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 244.2636 - val_loss: 1137.6774\n",
      "Epoch 796/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 243.9531 - val_loss: 1136.3744\n",
      "Epoch 797/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 243.6445 - val_loss: 1135.1390\n",
      "Epoch 798/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 243.3370 - val_loss: 1133.9133\n",
      "Epoch 799/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 243.0312 - val_loss: 1132.8245\n",
      "Epoch 800/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 242.7250 - val_loss: 1131.5581\n",
      "Epoch 801/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 242.4216 - val_loss: 1129.6375\n",
      "Epoch 802/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 242.1196 - val_loss: 1127.2714\n",
      "Epoch 803/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 241.8197 - val_loss: 1124.8163\n",
      "Epoch 804/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 241.5217 - val_loss: 1122.5254\n",
      "Epoch 805/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 241.2243 - val_loss: 1120.6371\n",
      "Epoch 806/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 240.9266 - val_loss: 1119.2673\n",
      "Epoch 807/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 240.6290 - val_loss: 1118.3430\n",
      "Epoch 808/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 240.3322 - val_loss: 1117.7238\n",
      "Epoch 809/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 240.0359 - val_loss: 1117.1696\n",
      "Epoch 810/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 239.7405 - val_loss: 1116.5197\n",
      "Epoch 811/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 239.4459 - val_loss: 1115.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 239.1516 - val_loss: 1114.4731\n",
      "Epoch 813/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 238.8580 - val_loss: 1113.0651\n",
      "Epoch 814/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 238.5657 - val_loss: 1111.6096\n",
      "Epoch 815/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 238.2731 - val_loss: 1109.9579\n",
      "Epoch 816/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 237.9819 - val_loss: 1108.1665\n",
      "Epoch 817/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 237.6912 - val_loss: 1106.3219\n",
      "Epoch 818/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 237.4012 - val_loss: 1104.5116\n",
      "Epoch 819/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 237.1117 - val_loss: 1102.9203\n",
      "Epoch 820/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 236.8216 - val_loss: 1101.5574\n",
      "Epoch 821/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 236.5316 - val_loss: 1100.3116\n",
      "Epoch 822/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 236.2400 - val_loss: 1099.6783\n",
      "Epoch 823/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 235.9318 - val_loss: 1099.3464\n",
      "Epoch 824/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 235.6197 - val_loss: 1099.1825\n",
      "Epoch 825/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 235.3045 - val_loss: 1098.9207\n",
      "Epoch 826/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 234.9870 - val_loss: 1098.3363\n",
      "Epoch 827/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 234.6682 - val_loss: 1097.3540\n",
      "Epoch 828/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 234.3476 - val_loss: 1095.8992\n",
      "Epoch 829/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 234.0250 - val_loss: 1093.9866\n",
      "Epoch 830/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 233.7007 - val_loss: 1091.7080\n",
      "Epoch 831/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 233.3757 - val_loss: 1089.3547\n",
      "Epoch 832/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 233.0503 - val_loss: 1086.9976\n",
      "Epoch 833/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 232.7241 - val_loss: 1084.7606\n",
      "Epoch 834/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 232.3973 - val_loss: 1082.7968\n",
      "Epoch 835/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 232.0711 - val_loss: 1080.9091\n",
      "Epoch 836/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 231.7453 - val_loss: 1079.3077\n",
      "Epoch 837/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 231.4184 - val_loss: 1077.7218\n",
      "Epoch 838/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 231.0914 - val_loss: 1076.2581\n",
      "Epoch 839/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 230.7606 - val_loss: 1074.8884\n",
      "Epoch 840/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 230.4289 - val_loss: 1073.5281\n",
      "Epoch 841/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 230.0953 - val_loss: 1072.4489\n",
      "Epoch 842/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 229.7601 - val_loss: 1071.5295\n",
      "Epoch 843/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 229.4233 - val_loss: 1070.6455\n",
      "Epoch 844/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 229.1009 - val_loss: 1069.1775\n",
      "Epoch 845/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 228.7870 - val_loss: 1067.1892\n",
      "Epoch 846/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 228.4744 - val_loss: 1064.8333\n",
      "Epoch 847/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 228.1640 - val_loss: 1061.3877\n",
      "Epoch 848/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 227.8630 - val_loss: 1057.4679\n",
      "Epoch 849/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 227.5638 - val_loss: 1053.4720\n",
      "Epoch 850/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 227.2675 - val_loss: 1049.8513\n",
      "Epoch 851/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 226.9772 - val_loss: 1046.6816\n",
      "Epoch 852/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 226.6879 - val_loss: 1044.2642\n",
      "Epoch 853/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 226.3969 - val_loss: 1042.5687\n",
      "Epoch 854/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 226.1042 - val_loss: 1041.4949\n",
      "Epoch 855/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 225.8104 - val_loss: 1040.9706\n",
      "Epoch 856/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 225.5162 - val_loss: 1040.6781\n",
      "Epoch 857/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 225.2251 - val_loss: 1040.2216\n",
      "Epoch 858/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 224.9376 - val_loss: 1039.4612\n",
      "Epoch 859/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 224.6517 - val_loss: 1038.1992\n",
      "Epoch 860/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 224.3668 - val_loss: 1036.3596\n",
      "Epoch 861/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 224.0825 - val_loss: 1033.8387\n",
      "Epoch 862/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 223.7990 - val_loss: 1030.9529\n",
      "Epoch 863/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 223.5166 - val_loss: 1027.9265\n",
      "Epoch 864/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 223.2353 - val_loss: 1025.0261\n",
      "Epoch 865/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 222.9553 - val_loss: 1022.4356\n",
      "Epoch 866/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 222.6764 - val_loss: 1020.2656\n",
      "Epoch 867/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 222.3992 - val_loss: 1018.3654\n",
      "Epoch 868/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 222.1236 - val_loss: 1016.7316\n",
      "Epoch 869/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 221.8487 - val_loss: 1015.4530\n",
      "Epoch 870/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 221.5746 - val_loss: 1014.4040\n",
      "Epoch 871/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 221.3013 - val_loss: 1013.2838\n",
      "Epoch 872/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 221.0291 - val_loss: 1012.1257\n",
      "Epoch 873/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 220.7579 - val_loss: 1010.8100\n",
      "Epoch 874/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 220.4878 - val_loss: 1009.2770\n",
      "Epoch 875/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 220.2185 - val_loss: 1007.3417\n",
      "Epoch 876/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 219.9496 - val_loss: 1005.2367\n",
      "Epoch 877/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 219.6814 - val_loss: 1003.1024\n",
      "Epoch 878/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 219.4137 - val_loss: 1001.0332\n",
      "Epoch 879/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 219.1469 - val_loss: 999.1017\n",
      "Epoch 880/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 218.8807 - val_loss: 997.2059\n",
      "Epoch 881/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 218.6152 - val_loss: 995.5350\n",
      "Epoch 882/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 218.3504 - val_loss: 994.0412\n",
      "Epoch 883/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 218.0862 - val_loss: 992.6793\n",
      "Epoch 884/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 217.8228 - val_loss: 991.2382\n",
      "Epoch 885/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 217.5598 - val_loss: 989.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 217.2980 - val_loss: 988.1847\n",
      "Epoch 887/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 217.0369 - val_loss: 986.6879\n",
      "Epoch 888/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 216.7765 - val_loss: 985.1764\n",
      "Epoch 889/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 216.5166 - val_loss: 983.6302\n",
      "Epoch 890/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 216.2573 - val_loss: 982.0397\n",
      "Epoch 891/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 215.9987 - val_loss: 980.4035\n",
      "Epoch 892/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 215.7529 - val_loss: 978.5016\n",
      "Epoch 893/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 215.5099 - val_loss: 976.3309\n",
      "Epoch 894/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 215.2672 - val_loss: 973.9245\n",
      "Epoch 895/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 215.0257 - val_loss: 971.5875\n",
      "Epoch 896/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 214.7863 - val_loss: 969.3839\n",
      "Epoch 897/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 214.5484 - val_loss: 967.4228\n",
      "Epoch 898/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 214.3118 - val_loss: 965.6087\n",
      "Epoch 899/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 214.0763 - val_loss: 963.9689\n",
      "Epoch 900/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 213.8412 - val_loss: 962.6443\n",
      "Epoch 901/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 213.6072 - val_loss: 961.5521\n",
      "Epoch 902/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 213.3737 - val_loss: 960.5699\n",
      "Epoch 903/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 213.1413 - val_loss: 959.5722\n",
      "Epoch 904/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 212.9096 - val_loss: 958.3011\n",
      "Epoch 905/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 212.6796 - val_loss: 956.8350\n",
      "Epoch 906/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 212.4507 - val_loss: 955.1511\n",
      "Epoch 907/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 212.2225 - val_loss: 953.3094\n",
      "Epoch 908/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 211.9951 - val_loss: 951.3969\n",
      "Epoch 909/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 211.7686 - val_loss: 949.5103\n",
      "Epoch 910/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 211.5426 - val_loss: 947.7306\n",
      "Epoch 911/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 211.3173 - val_loss: 946.1401\n",
      "Epoch 912/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 211.0927 - val_loss: 944.6442\n",
      "Epoch 913/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 210.8678 - val_loss: 943.3256\n",
      "Epoch 914/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 210.6439 - val_loss: 941.9787\n",
      "Epoch 915/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 210.4199 - val_loss: 940.5738\n",
      "Epoch 916/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 210.1966 - val_loss: 939.0262\n",
      "Epoch 917/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 209.9743 - val_loss: 937.4503\n",
      "Epoch 918/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 209.7526 - val_loss: 936.2508\n",
      "Epoch 919/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 209.5326 - val_loss: 935.1923\n",
      "Epoch 920/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 209.3132 - val_loss: 934.1681\n",
      "Epoch 921/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 209.0943 - val_loss: 933.0760\n",
      "Epoch 922/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 208.8758 - val_loss: 931.4515\n",
      "Epoch 923/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 208.6577 - val_loss: 929.7714\n",
      "Epoch 924/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 208.4400 - val_loss: 928.0943\n",
      "Epoch 925/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 208.2228 - val_loss: 926.4705\n",
      "Epoch 926/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 208.0064 - val_loss: 924.8393\n",
      "Epoch 927/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 207.7897 - val_loss: 922.8448\n",
      "Epoch 928/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 207.5737 - val_loss: 921.1523\n",
      "Epoch 929/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 207.3584 - val_loss: 919.3847\n",
      "Epoch 930/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 207.1435 - val_loss: 917.6265\n",
      "Epoch 931/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 206.9289 - val_loss: 915.5304\n",
      "Epoch 932/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 206.7197 - val_loss: 913.2726\n",
      "Epoch 933/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 206.5111 - val_loss: 911.0441\n",
      "Epoch 934/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 206.3030 - val_loss: 909.0153\n",
      "Epoch 935/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 206.0958 - val_loss: 907.5905\n",
      "Epoch 936/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 205.8892 - val_loss: 906.7016\n",
      "Epoch 937/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 205.6825 - val_loss: 906.3027\n",
      "Epoch 938/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 205.4764 - val_loss: 906.1185\n",
      "Epoch 939/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 205.2722 - val_loss: 904.9966\n",
      "Epoch 940/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 205.0679 - val_loss: 903.4049\n",
      "Epoch 941/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 204.8639 - val_loss: 901.4205\n",
      "Epoch 942/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 204.6611 - val_loss: 899.6202\n",
      "Epoch 943/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 204.4565 - val_loss: 898.3486\n",
      "Epoch 944/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 204.2467 - val_loss: 897.5981\n",
      "Epoch 945/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 204.0362 - val_loss: 897.1419\n",
      "Epoch 946/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 203.8258 - val_loss: 896.2665\n",
      "Epoch 947/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 203.6161 - val_loss: 894.8641\n",
      "Epoch 948/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 203.4057 - val_loss: 892.9654\n",
      "Epoch 949/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 203.1949 - val_loss: 891.1854\n",
      "Epoch 950/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 202.9842 - val_loss: 889.5807\n",
      "Epoch 951/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 202.7730 - val_loss: 888.1864\n",
      "Epoch 952/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 202.5615 - val_loss: 886.9820\n",
      "Epoch 953/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 202.3507 - val_loss: 885.7884\n",
      "Epoch 954/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 202.1395 - val_loss: 884.3428\n",
      "Epoch 955/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 201.9278 - val_loss: 882.8234\n",
      "Epoch 956/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 201.7164 - val_loss: 881.2386\n",
      "Epoch 957/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 201.5050 - val_loss: 879.6207\n",
      "Epoch 958/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 201.2935 - val_loss: 878.0058\n",
      "Epoch 959/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 201.0816 - val_loss: 876.9873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 960/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 200.8687 - val_loss: 876.2578\n",
      "Epoch 961/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 200.6556 - val_loss: 875.2345\n",
      "Epoch 962/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 200.4443 - val_loss: 873.7466\n",
      "Epoch 963/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 200.2322 - val_loss: 871.8268\n",
      "Epoch 964/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 200.0199 - val_loss: 870.2050\n",
      "Epoch 965/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 199.8087 - val_loss: 868.9101\n",
      "Epoch 966/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 199.5980 - val_loss: 867.4354\n",
      "Epoch 967/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 199.3872 - val_loss: 865.8126\n",
      "Epoch 968/2000\n",
      "125/125 [==============================] - 0s 53us/sample - loss: 199.1768 - val_loss: 864.1888\n",
      "Epoch 969/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 198.9664 - val_loss: 862.1559\n",
      "Epoch 970/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 198.7579 - val_loss: 859.7672\n",
      "Epoch 971/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 198.5504 - val_loss: 857.2491\n",
      "Epoch 972/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 198.3430 - val_loss: 855.3802\n",
      "Epoch 973/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 198.1367 - val_loss: 854.2793\n",
      "Epoch 974/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 197.9297 - val_loss: 853.3379\n",
      "Epoch 975/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 197.7232 - val_loss: 852.4178\n",
      "Epoch 976/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 197.5169 - val_loss: 851.3350\n",
      "Epoch 977/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 197.3110 - val_loss: 850.0268\n",
      "Epoch 978/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 197.1051 - val_loss: 848.4912\n",
      "Epoch 979/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 196.8994 - val_loss: 846.6620\n",
      "Epoch 980/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 196.6938 - val_loss: 844.6743\n",
      "Epoch 981/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 196.4884 - val_loss: 842.8175\n",
      "Epoch 982/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 196.2835 - val_loss: 841.1949\n",
      "Epoch 983/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 196.0803 - val_loss: 839.7462\n",
      "Epoch 984/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 195.8757 - val_loss: 838.4608\n",
      "Epoch 985/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 195.6708 - val_loss: 837.2627\n",
      "Epoch 986/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 195.4672 - val_loss: 836.0688\n",
      "Epoch 987/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 195.2638 - val_loss: 834.8179\n",
      "Epoch 988/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 195.0607 - val_loss: 833.5245\n",
      "Epoch 989/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 194.8578 - val_loss: 832.1646\n",
      "Epoch 990/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 194.6550 - val_loss: 830.8641\n",
      "Epoch 991/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 194.4525 - val_loss: 829.5982\n",
      "Epoch 992/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 194.2501 - val_loss: 828.2899\n",
      "Epoch 993/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 194.0481 - val_loss: 826.9268\n",
      "Epoch 994/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 193.8463 - val_loss: 825.5191\n",
      "Epoch 995/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 193.6458 - val_loss: 823.9710\n",
      "Epoch 996/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 193.4443 - val_loss: 822.3331\n",
      "Epoch 997/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 193.2428 - val_loss: 820.7787\n",
      "Epoch 998/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 193.0426 - val_loss: 819.7596\n",
      "Epoch 999/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 192.8420 - val_loss: 818.6967\n",
      "Epoch 1000/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 192.6423 - val_loss: 817.4966\n",
      "Epoch 1001/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 192.4434 - val_loss: 816.1190\n",
      "Epoch 1002/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 192.2448 - val_loss: 814.5991\n",
      "Epoch 1003/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 192.0468 - val_loss: 813.1621\n",
      "Epoch 1004/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 191.8491 - val_loss: 811.8120\n",
      "Epoch 1005/2000\n",
      "125/125 [==============================] - 0s 72us/sample - loss: 191.6513 - val_loss: 810.6722\n",
      "Epoch 1006/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 191.4565 - val_loss: 809.3593\n",
      "Epoch 1007/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 191.2660 - val_loss: 806.9136\n",
      "Epoch 1008/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 191.0758 - val_loss: 805.0768\n",
      "Epoch 1009/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 190.8870 - val_loss: 803.1152\n",
      "Epoch 1010/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 190.6995 - val_loss: 801.3794\n",
      "Epoch 1011/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 190.5125 - val_loss: 799.9551\n",
      "Epoch 1012/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 190.3263 - val_loss: 798.4988\n",
      "Epoch 1013/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 190.1402 - val_loss: 797.1567\n",
      "Epoch 1014/2000\n",
      "125/125 [==============================] - 0s 56us/sample - loss: 189.9550 - val_loss: 795.9332\n",
      "Epoch 1015/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 189.7701 - val_loss: 794.7765\n",
      "Epoch 1016/2000\n",
      "125/125 [==============================] - 0s 64us/sample - loss: 189.5855 - val_loss: 793.6396\n",
      "Epoch 1017/2000\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "          batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5),sharex=True)\n",
    "(ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "_ = ax1.hist(cnn_model.get_weights()[0].ravel())\n",
    "_ = ax2.hist(cnn_model.get_weights()[1].ravel())\n",
    "_ = ax3.hist(cnn_model.get_weights()[2].ravel())\n",
    "_ = ax4.hist(cnn_model.get_weights()[3].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(cnn_model.get_weights())[0].min(),\n",
    "np.array(cnn_model.get_weights())[1].min(),\n",
    "np.array(cnn_model.get_weights())[2].min(),\n",
    "np.array(cnn_model.get_weights())[3].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_train_model.ravel()\n",
    "y_predict = cnn_model.predict(X_cnn_train_model).ravel()\n",
    "model_analysis(y_true, y_train_naive, y_predict, n_countries, title='CNN model', suptitle='Performance on training set')\n",
    "print(y_true.max(),y_train_naive.max(), y_predict.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_cnn_train_model.max(), y_train_naive.max(), y_predict.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,1030.1168956946265]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5442352024240946]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_validate_model.ravel()\n",
    "y_predict = cnn_model.predict(X_cnn_validate_model).ravel()\n",
    "model_analysis(y_true, y_validate_naive, y_predict, n_countries, title='CNN model', suptitle='Performance on validation set')\n",
    "print(y_true.max(),y_train_naive.max(), y_predict.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tile_shape = np.array(np.array(X_cnn_train.shape)/np.array(frame_denom.shape),int)\n",
    "minmax_inverse_train = X_train - (((X_cnn_train / 0.5) * np.tile(frame_denom, train_tile_shape)) + np.tile(frame_min, train_tile_shape))\n",
    "\n",
    "\n",
    "validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "minmax_inverse_validate = X_validate - (((X_cnn_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "\n",
    "\n",
    "test_tile_shape = np.array(np.array(X_cnn_test.shape)/np.array(latest_denom.shape),int)\n",
    "minmax_inverse_test = X_test - (((X_cnn_test / 0.5) * np.tile(latest_denom, test_tile_shape)) + np.tile(latest_min, test_tile_shape))\n",
    "print(np.linalg.norm(minmax_inverse_train),np.linalg.norm(minmax_inverse_validate),np.linalg.norm(minmax_inverse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time index to X relationship.\n",
    "\n",
    "    Last frame, last day of X is time_index() - n_days_into_future\n",
    "    last frame, first day is time_index() - n_days_into_future - frame_size\n",
    "    first frame, first day is start_date-frame_size\n",
    "    first frame, last day is start_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots of train, validate, test sets, by plotting all values in black first I can ensure that the points\n",
    "are correctly ordered and at the correct values because it makes the rest of the points look like they have black borders.\n",
    "The \"missing\" data point at the end is because it is in the prediction variable y, and not in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first the following plot looks wrong because y looks like it is 1-day behind but this is only because I am\n",
    "not providing the time values. I.e. the 5th value of y is actually the 6th value of X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.scatter(range(len(X)), X[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=30,color='k')\n",
    "ax.scatter(range(len(X_train)), X_train[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='train')\n",
    "ax.scatter(range(len(X_train), len(X_train)+len(X_validate)), X_validate[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='validate')\n",
    "ax.scatter(range(len(X_train)+len(X_validate), len(X)), X_test[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='test')\n",
    "ax.plot(data[data.location=='United States'].new_cases_per_million.values[frame_size-1:-1], color='k',alpha=0.2)\n",
    "plt.legend()\n",
    "_ = plt.show()\n",
    "# plt.plot(X_validate[:,0,-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_data = pd.read_csv('regression_data.csv', index_col=0)\n",
    "r_data = r_data[r_data.location == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only use 1 feature in the CNN model, I understand that regression and CNN are fundamentally different but does\n",
    "this mean I should only be using 1 feature for a fair baseline comparison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prune = 2\n",
    "mae_list_naive = []\n",
    "r2_list_naive = []\n",
    "mae_list_predict = []\n",
    "r2_list_predict = []\n",
    "# data = data[r_data.time_index >= first_day]\n",
    "r_model_data = r_data.new_cases_per_million.to_frame().copy(); new_cases_index=0\n",
    "#\n",
    "new_cases_index = column_search(r_model_data,'new_cases_per_million',threshold='match', return_style='iloc')[0]\n",
    "n_countries = r_data.location.nunique()\n",
    "target_data = r_data.new_cases_per_million\n",
    "time_index = r_data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size #+ time_index.min()\n",
    "# start_date = 50\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_Xy(r_model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "# y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()\n",
    "\n",
    "flat_splits = flatten_Xy_splits(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "X_validate - (((X_cnn_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "(latest_max, latest_min, latest_denom) = latest_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_regression_train.max(),X_regression_validate.max(),X_regression_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_regression_train.shape,X_regression_validate.shape,X_regression_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_regression_train.shape,y_regression_validate.shape,y_regression_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a frame of frame_size days of new_cases_per_million data for a single country. So the number of rows equals\n",
    "n_countries times frame_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation (slicing off last row) is because X_regression is both training and validation\n",
    "data to be sliced by the train, validation indices passed to Ridge CV. i.e. the holdout set is being held out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "regression_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = regression_model.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = regression_model.predict(X_regression_train).ravel()\n",
    "\n",
    "model_analysis( y_regression[train_indices].ravel(), y_train_naive, y_predict_train, \n",
    "               n_countries, title='Ridge regression model', suptitle='Performance on training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_model.predict(X_regression_validate).ravel()\n",
    "model_analysis( y_regression_validate.ravel(), y_validate_naive, y_predict_validate, n_countries,title='Ridge regression model', suptitle='Performance on validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaled single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prune = 2\n",
    "mae_list_naive = []\n",
    "r2_list_naive = []\n",
    "mae_list_predict = []\n",
    "r2_list_predict = []\n",
    "# data = data[r_data.time_index >= first_day]\n",
    "# r_model_data = r_data.new_cases_per_million.apply(lambda x : np.log(x+1)).to_frame().copy(); new_cases_index=0\n",
    "r_model_data = r_data.new_cases_per_million.to_frame().copy(); new_cases_index=0\n",
    "new_cases_index = column_search(r_model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = r_data.location.nunique()\n",
    "target_data = r_data.new_cases_per_million\n",
    "time_index = r_data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size #+ time_index.min()\n",
    "# start_date = 50\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_Xy(r_model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "(X_train, y_train, X_validate,\n",
    " y_validate, X_test, y_test) = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "# y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()\n",
    "\n",
    "# The normalization is alot easier if I split into train, validate, test but this makes other parts more annoying\n",
    "scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,0.5), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "\n",
    "(latest_max, latest_min, latest_denom) = latest_minmax\n",
    "(frame_max, frame_min, frame_denom) = frame_minmax\n",
    "\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = scaled_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tile_shape = np.array(np.array(X_regression_train.shape) / np.array(frame_denom.shape),int)\n",
    "minmax_inverse_train = X_train - (((X_regression_train / 0.5) * np.tile(frame_denom, train_tile_shape)) + np.tile(frame_min, train_tile_shape))\n",
    "np.linalg.norm(minmax_inverse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "minmax_inverse_validate = X_validate - (((X_regression_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "np.linalg.norm(minmax_inverse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_shape = np.array(np.array(X_regression_test.shape)/np.array(latest_denom.shape),int)\n",
    "minmax_inverse_test = X_test - (((X_regression_test / 0.5) * np.tile(latest_denom, test_tile_shape)) + np.tile(latest_min, test_tile_shape))\n",
    "np.linalg.norm(minmax_inverse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy_splits(scaled_splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits\n",
    "\n",
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "regression_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = regression_model.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = regression_model.predict(X_regression_train).ravel()\n",
    "model_analysis(y_regression_train, y_train_naive, y_predict_train, n_countries, title='Ridge', suptitle='Scaled predictor training set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_model.predict(X_regression_validate).ravel()\n",
    "model_analysis(y_regression_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"All\" feature prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_prune = 2\n",
    "# mae_list_naive = []\n",
    "# r2_list_naive = []\n",
    "# mae_list_predict = []\n",
    "# r2_list_predict = []\n",
    "# # data = data[r_data.time_index >= first_day]\n",
    "# new_cases_index = column_search(r_model_data,'new_cases_per_million',threshold='match', return_style='iloc')[0]\n",
    "# n_countries = r_data.location.nunique()\n",
    "# target_data = r_data.new_cases_per_million\n",
    "# time_index = r_data.time_index\n",
    "# frame_size = 28\n",
    "# start_date = 2*frame_size #+ time_index.min()\n",
    "# # start_date = 50\n",
    "# n_validation_frames = 7\n",
    "# n_test_frames = 1\n",
    "# n_days_into_future = 1\n",
    "# train_or_test = 'train'\n",
    "\n",
    "\n",
    "n_prune = 2\n",
    "mae_list_naive = []\n",
    "r2_list_naive = []\n",
    "mae_list_predict = []\n",
    "r2_list_predict = []\n",
    "r_model_data = r_data.iloc[:,2:].copy()#.apply(lambda x : np.log(x+1))\n",
    "new_cases_index = column_search(r_model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = r_data.location.nunique()\n",
    "target_data = r_data.new_cases_per_million\n",
    "time_index = r_data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size #+ time_index.min()\n",
    "# start_date = 50\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_Xy(r_model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "(X_train, y_train, X_validate,\n",
    " y_validate, X_test, y_test) = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "# y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()\n",
    "\n",
    "# The normalization is alot easier if I split into train, validate, test but this makes other parts more annoying\n",
    "scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,0.5), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "\n",
    "(latest_max, latest_min, latest_denom) = latest_minmax\n",
    "(frame_max, frame_min, frame_denom) = frame_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "new_cases_indices = np.ravel_multi_index([list(range(frame_size)),[new_cases_index]],(frame_size, n_features))\n",
    "\n",
    "new_cases_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_regression_train[:,new_cases_indices][:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that there is no time pollution going on, look at the first and last rows of the training data to ensure\n",
    "that they have the correct days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy_splits(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits\n",
    "\n",
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "regression_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = regression_model.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first frame should have time values from 0 to 27.\n",
    "r_model_data.new_cases_per_million.iloc[:28].values-X_regression_train[0, new_cases_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last row or last frame, should have values from time_index.max()-1-n_test_frames-n_validation_frames-frame_size to time_index.max()-1-n_test_frames-n_validation_frames\n",
    "(r_model_data.new_cases_per_million.iloc[-28-n_test_frames-n_validation_frames-1:-n_test_frames-n_validation_frames-1].values\n",
    " -X_regression_train[-1, new_cases_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = regression_model.predict(X_regression_train).ravel()\n",
    "model_analysis(y_regression_train, y_train_naive, y_predict_train, n_countries, title='Ridge', suptitle='Scaled predictor training set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_model.predict(X_regression_validate).ravel()\n",
    "model_analysis(y_regression_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final predictions for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_naive = y_test_naive\n",
    "y_true = y_test\n",
    "y_predict_r = regression_model.predict(X_regression_test)\n",
    "y_predict_c = cnn_model.predict(X_cnn_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analysis(y_true, y_naive, y_predict_c, n_countries, title='Ridge', suptitle='Unscaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analysis(y_true, y_naive, y_predict_r, n_countries, title='CNN', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
