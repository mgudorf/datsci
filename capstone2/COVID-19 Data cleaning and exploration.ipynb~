{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import scipy\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "\n",
    "**Data available at:**\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "\n",
    "**Data available via github**\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "\n",
    "**Data available at:**\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "\n",
    "\n",
    "**If API used to pull data (I elect not to because the datasets are different)**\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "<!-- **Website which lead me to dataset**\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/) -->\n",
    "\n",
    "**Data available at:**\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool|\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "def clean_DataFrame(df):\n",
    "    \"\"\" Remove all NaN or single value columns. \n",
    "    \n",
    "    \"\"\"\n",
    "    # if 0 then column is all NaN, if 1 then could be mix of NaN and a\n",
    "    # single value at most. \n",
    "    df = df.loc[:, df.columns[(df.nunique() > 0)]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    if len(df.index.names) == 1:\n",
    "        placeholder = df.index.name\n",
    "        df = df.reset_index()\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index(placeholder)#.sum()\n",
    "    else:\n",
    "        placeholder = df.index.names[0]\n",
    "        df = df.reset_index(level=0)\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index([placeholder, df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    try:\n",
    "        df_table.columns = df_table.columns.droplevel()\n",
    "        df_table.columns.names = ['']\n",
    "    except:\n",
    "        pass\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_time_indices(data_table, index_column='cases'):\n",
    "    indexer = data_table.loc[:, ['location', index_column]].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "    country_groupby_indices = country_groupby(data_table)\n",
    "    country_groupby_indices_dropped_nan = country_groupby(indexer)\n",
    "    days_since = []\n",
    "    for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "        nonzero_list = list(range(len(c)))\n",
    "        zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "        days_since += list(zero_list)+nonzero_list\n",
    "\n",
    "    data_table.loc[:, 'days_since_first_case'] = days_since\n",
    "    data_table.loc[:, 'date_proxy'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))\n",
    "    return data_table\n",
    "\n",
    "\n",
    "def regularize_time_series(df_list):\n",
    "    country_intersection = df_list[0].index.levels[0].unique()\n",
    "    dates_union =  df_list[0].index.levels[1].unique()\n",
    "    dates_intersection =  df_list[0].index.levels[1].unique()\n",
    "\n",
    "    for i in range(len(df_list)-1):\n",
    "        country_intersection = country_intersection.intersection(df_list[i+1].index.levels[0].unique())\n",
    "        dates_union = dates_union.union(df_list[i+1].index.levels[1].unique())\n",
    "        # not really intersection, this is the minimum date that at least one country has data for, in each dataset.\n",
    "        dates_intersection = dates_intersection.intersection(df_list[i+1].index.levels[1].unique())\n",
    "\n",
    "    df_list_intersected = [intersect_country_index(df, country_intersection) for df in df_list]\n",
    "\n",
    "    #This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "    time_normalized_global_data = [resample_dates(df, dates_intersection.normalize()) for df in df_list_intersected]\n",
    "    # To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "    return time_normalized_global_data, dates_intersection, country_intersection\n",
    "\n",
    "\n",
    "def rolling_means(df, features, roll_widths):\n",
    "    new_feature_df_list = []\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values \n",
    "        rollmean = pd.DataFrame(df.groupby(by='location').rolling(window).mean().fillna(value=0.))\n",
    "#         rollstd = pd.DataFrame(df.groupby(by='location').rolling(window).std().fillna(value=0.))    \n",
    "#         new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        new_features = rollmean\n",
    "        new_cols = features +'_rolling_mean_' + str(window)\n",
    "#         rsind = features +'_rolling_std_' + str(window)\n",
    "#         new_cols = rmind.append(rsind)\n",
    "        new_features.columns = new_cols\n",
    "        new_feature_df_list.append(new_features)\n",
    "    return new_feature_df_list\n",
    "\n",
    "def tsplot(pd_series, roll_width, **kw):\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    rollmean = pd_series.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "    rollstd  = pd_series.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "    cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "    ax.fill_between(range(len(pd_series)), cis[0], cis[1], alpha=0.5)\n",
    "    ax.plot(range(len(pd_series)), rollmean, color='k')\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n",
    "\n",
    "def drop_all_but_least_missing(df, feature):\n",
    "    matching_columns = column_search(df, feature, return_style='iloc', threshold='match') \n",
    "    feature_index =  matching_columns[df.iloc[:, matching_columns].isna().sum().argmin()]\n",
    "\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def fix_incorrect_decrease(df, feature, search_threshold='match'):\n",
    "    matching_columns = column_search(df, feature, return_style='iloc', threshold=search_threshold) \n",
    "    feature_index =  matching_columns[df.iloc[:, matching_columns].isna().sum().argmin()]\n",
    "    location_index =  column_search(df, 'location', return_style='iloc', threshold=search_threshold)[0] \n",
    "    print('Fixing the {} column feature'.format(df.iloc[:, feature_index].name))\n",
    "    # premptive filling, not related to decreasing but rather missing values.\n",
    "    decreasing_incorrectly = []\n",
    "    flag = True\n",
    "    # flag is switched off immediately, but will be switched on if any values are changed.\n",
    "    df.iloc[:, feature_index] = df.iloc[:, [location_index, feature_index]].groupby('location').fillna(method='ffill')\n",
    "    df.iloc[:, feature_index] = df.iloc[:, [location_index, feature_index]].groupby('location').fillna(0)\n",
    "    while flag:\n",
    "        flag = False\n",
    "        decreasing_indices = np.where(df.iloc[:, [location_index, feature_index]].groupby('location').diff(1)< 0)[0].ravel()\n",
    "        if decreasing_indices.size > 0:\n",
    "            countries_with_missing = df.iloc[decreasing_indices, location_index].unique().tolist()\n",
    "            flag = True\n",
    "            df.iloc[decreasing_indices, feature_index] = np.nan\n",
    "            df.iloc[:, feature_index] = df.iloc[:, [location_index, feature_index]].groupby('location').fillna(method='ffill')\n",
    "            df.iloc[:, feature_index] = df.iloc[:, [location_index, feature_index]].groupby('location').fillna(0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only used in data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "#the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "#     df = column_or_index_string_reformat(pd.read_csv(x),columns=True,index=False)\n",
    "    df_list.append(tmp_df)\n",
    "\n",
    "daily_reports_df = pd.concat(df_list, axis=0)\n",
    "daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_df.loc[:, 'combined_key'] = (daily_reports_df.province_state.astype('str').replace(to_replace='nan', value='')+' '+ daily_reports_df.location.astype('str')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_df = daily_reports_df.drop(columns=['province_state', 'last_update', 'fips', 'admin2']).set_index(['location','date'])\n",
    "#daily_reports_df = daily_reports_df.groupby(['location','date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_df_list = []\n",
    "\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :4]\n",
    "    datecols = tmp_df.iloc[:, 4:]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  reformat_values(catcols.loc[:, 'country_region'], category='location').values\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    global_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))\n",
    "\n",
    "\n",
    "\n",
    "csse_global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "csse_global_time_series_df.index.names = ['location','date']\n",
    "csse_global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "csse_global_time_series_df.columns = ['cases', 'deaths', 'recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df_list = []\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :np.where(tmp_df.columns == '1/22/20')[0][0]]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  catcols.loc[:, 'province_state'].values\n",
    "    \n",
    "    datecols = tmp_df.iloc[:,np.where(tmp_df.columns == '1/22/20')[0][0]:]\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    usa_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))\n",
    "    \n",
    "usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "usa_time_series_df.index.names = ['location','date']\n",
    "usa_time_series_df.columns.names = ['csse_us_timeseries']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./ga_covid_data/demographics.csv').sort_values(by='race').groupby('race').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_width = 7\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "pds = usa_time_series_df.reset_index()[(usa_time_series_df.reset_index().location=='Georgia')]\n",
    "pds.loc[:, 'date'] = pds.loc[:, 'date'].dt.date\n",
    "pds = pds.set_index('date').deaths.diff(1)\n",
    "rollmean = pds.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "rollstd  = pds.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "\n",
    "ax.fill_between(range(len(pds)), cis[0], cis[1], alpha=0.5, label='+- 7-day STD Georgia')\n",
    "ax.plot(range(len(pds)), rollmean, color='k', label='7-day MA Georgia')\n",
    "\n",
    "pds = usa_time_series_df.reset_index()[(usa_time_series_df.reset_index().location=='Michigan')]\n",
    "pds.loc[:, 'date'] = pds.loc[:, 'date'].dt.date\n",
    "pds = pds.set_index('date').deaths.diff(1)\n",
    "rollmean = pds.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "rollstd  = pds.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "\n",
    "ax.fill_between(range(len(pds)), cis[0], cis[1], alpha=0.5, label='+- 7-day STD Michigan')\n",
    "ax.plot(range(len(pds)), rollmean, color='k',linestyle='--', label='7-day MA Michigan')\n",
    "\n",
    "plt.legend(loc=(0.01,0.7))\n",
    "plt.title('Daily deaths comparison, GA vs. MI')\n",
    "_ = ax.set_ylabel('Daily Deaths')\n",
    "_ = ax.set_xlabel('Date')\n",
    "_ = ax.set_xticks([0, 50,100])\n",
    "_ = ax.set_xticklabels(pds.index[::50])\n",
    "plt.savefig('GAvsMIdeaths.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "pds = usa_time_series_df.reset_index()[(usa_time_series_df.reset_index().location=='Georgia')]\n",
    "pds.loc[:, 'date'] = pds.loc[:, 'date'].dt.date\n",
    "pds = pds.set_index('date').confirmed.diff(1)\n",
    "rollmean = pds.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "rollstd  = pds.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "\n",
    "ax.fill_between(range(len(pds)), cis[0], cis[1], alpha=0.5, label='+- 7-day STD Georgia')\n",
    "ax.plot(range(len(pds)), rollmean, color='k', label='7-day MA Georgia')\n",
    "\n",
    "pds = usa_time_series_df.reset_index()[(usa_time_series_df.reset_index().location=='Michigan')]\n",
    "pds.loc[:, 'date'] = pds.loc[:, 'date'].dt.date\n",
    "pds = pds.set_index('date').confirmed.diff(1)\n",
    "rollmean = pds.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "rollstd  = pds.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "\n",
    "ax.fill_between(range(len(pds)), cis[0], cis[1], alpha=0.5, label='+- 7-day STD Michigan')\n",
    "ax.plot(range(len(pds)), rollmean, color='k',linestyle='--', label='7-day MA Michigan')\n",
    "\n",
    "plt.legend(loc=(0.01,0.7))\n",
    "plt.title('Daily new cases comparison, GA vs. MI')\n",
    "_ = ax.set_ylabel('Daily Deaths')\n",
    "_ = ax.set_xlabel('Date')\n",
    "_ = ax.set_xticks([0, 50,100])\n",
    "_ = ax.set_xticklabels(pds.index[::50])\n",
    "plt.savefig('GAvsMIcases.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.ewm(span=21).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df =pd.read_csv('./covid-19-data/public/data/owid-covid-data.csv')\n",
    "owid_df= owid_df[owid_df.new_cases_per_million > 0]#.new_cases_per_million.\n",
    "owid_df = regularize_names(owid_df, datekey='date', locationkey='location').set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "oxcgrt_df.loc[:, 'date'] = pd.to_datetime(oxcgrt_df.loc[:, 'date'], format='%Y%m%d')\n",
    "oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_cases = regularize_names(pd.read_csv('test_tracker_cases.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(\n",
    "                            ['location', 'date']).drop(\n",
    "                                    columns=['population','country','alpha3']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_cases.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_tests = regularize_names(pd.read_csv('test_tracker_tests.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(['location', 'date']).drop(\n",
    "                                    columns=['population','country','source','alpha3']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_tests.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used to model country-wide case numbers exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = [csse_global_time_series_df,\n",
    "                 testtracker_cases, testtracker_tests, oxcgrt_df, owid_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = ['jhucsse', 'ttc', 'ttt',  'oxcgrt', 'owid']\n",
    "export_list = []\n",
    "\n",
    "export_list_tmp, dates_intersection, country_intersection = regularize_time_series(global_data)\n",
    "print('The range of all dates is from {} to {}'.format(dates_intersection.min(), dates_intersection.max()))\n",
    "print('The final number of countries included is {}'.format(len(country_intersection)))\n",
    "\n",
    "for i, x in enumerate(export_list_tmp):\n",
    "    gd_export_copy = x.copy()\n",
    "    gd_export_copy.columns += '_' + names[i]\n",
    "    export_list.append(gd_export_copy)\n",
    "\n",
    "eda_data = multiindex_to_table(pd.concat(export_list, axis=1))\n",
    "eda_data = add_time_indices(eda_data, index_column='cases_jhucsse')\n",
    "eda_data.to_csv('eda_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_list, modeling_dates, modeling_countries = regularize_time_series(global_data)\n",
    "print('The range of all dates is from {} to {}'.format(modeling_dates.min(), modeling_dates.max()))\n",
    "print('The number of countries included is {}'.format(len(modeling_countries)))\n",
    "# Convert the multiindex DataFrame to a simple table, and then add numerical features which track the\n",
    "# date and # of days since first case. \n",
    "df = add_time_indices(multiindex_to_table(pd.concat(dataframe_list, axis=1)), index_column='cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: missing values, feature production, etc.\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to normalize by population, so first I need to fill in the missing values introduced by time series regularization / extrapolation; an additional manipulation is required for population density, namely, the population density feature is missing information on a single country: Afghanistan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.population_density.nunique(), df.location.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to figure out the units of the population densities, for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.location=='United States'].population_density.dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aligns with the population density being in units of people / km^2, per Google search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality related exploration\n",
    "\n",
    "    Differences in reporting \n",
    "    differences in time series.\n",
    "\n",
    "\n",
    "## COVID related exploration\n",
    "\n",
    "    Death rate estimation. \n",
    "    testing vs cases vs deaths\n",
    "    log-log plot for current growth trends\n",
    "    bollinger bands\n",
    "    Differences in government responses (early vs late)\n",
    "    histogram of trending upwards, flat, downwards\n",
    "    tools, different plots, correlation plots scatter matrix plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I am aggregating a number of different data sets, there are multiple features describing the same quantities (number of cases, deaths, etc.). The quality of the different sources varies, which informs the decision as to which of these columns to keep. This will be explored in more detail later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original idea was to normalize using the population of each country but I'm unsure as to whether this is the best\n",
    "idea; at least without another consideration. Using the population blindly results in micro-states and small countries\n",
    "being weighted heavily. Now, either I can drop these countries or I can weight the features differently; with the population *density*. This accounts for population as well as how large the state is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature production <a id='newfeatures'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant features; some features contain the same substring but are not redundant, hence the use of .difference()\n",
    "redundant_death_columns = column_search(df, 'death').difference(['cvd_death_rate'])\n",
    "redundant_test_columns = column_search(df, 'test').difference(['tests_units','h2_testing_policy'])\n",
    "redundant_case_columns= column_search(df, 'cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any feature production or investigation, fix the incorrect decreases in these four important columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = df.loc[:,  ['location', 'population','population_density']].groupby('location').max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent the SettingCopyWithSlice warning use loc, which needs index values, hence\n",
    "# the application of this workaround function. \n",
    "df.loc[country_search(df, 'Afghanistan'), 'population_density'] = 154\n",
    "df.loc[country_search(df, 'Kosovo'), 'population_density'] = 159\n",
    "df.loc[country_search(df, 'South Sudan'), 'population_density'] = 20.3\n",
    "df.loc[country_search(df, 'Guinea Bissau'), 'population'] = \\\n",
    "df.loc[country_search(df, 'Guinea Bissau'), 'population'].max()\n",
    "\n",
    "df.loc[country_search(df, 'Guinea Bissau'), 'population_density'] = \\\n",
    "df.loc[country_search(df, 'Guinea Bissau'), 'population_density'].max()\n",
    "\n",
    "\n",
    "for country_indices in country_groupby(df):\n",
    "    df.loc[country_indices, ['population','population_density']] = \\\n",
    "    df.loc[country_indices,  ['population','population_density']].fillna(method='ffill').fillna(method='bfill').values \n",
    "    \n",
    "per_million_population = df.population / 1000000.\n",
    "population_density = df.population_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_pop = df[df.date_proxy==df.date_proxy.max()].population.sum()\n",
    "percent_pop = df.population / world_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series for these four countries, before any manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df.location == 'Benin'].total_cases.values)\n",
    "ax.plot(df[df.location == 'Lithuania'].total_cases.values)\n",
    "ax.plot(df[df.location == 'Uganda'].total_cases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_incorrect_decrease(df, 'total_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df.location == 'Benin'].total_cases.values)\n",
    "ax.plot(df[df.location == 'Lithuania'].total_cases.values)\n",
    "ax.plot(df[df.location == 'Uganda'].total_cases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df.location == 'Benin'].tests_cumulative.values)\n",
    "ax.plot(df[df.location == 'Lithuania'].tests_cumulative.values)\n",
    "ax.plot(df[df.location == 'Uganda'].tests_cumulative.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_incorrect_decrease(df, 'tests', search_threshold='match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df[df.location == 'Benin'].tests_cumulative.values)\n",
    "ax.plot(df[df.location == 'Lithuania'].tests_cumulative.values)\n",
    "ax.plot(df[df.location == 'Uganda'].tests_cumulative.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of custom function to fill in missing values, as well as check the validity of the time series. Mainly,\n",
    "this means making sure that non-decreasing quantities in fact behave this way. It checks the difference between values $n$ and\n",
    "$n+1$. If negative, the value at $n+1$ is replaced with the previous day's value. If this creates a new discrepancy, (i.e. $n+2$ is smaller then $n+1$ post-update, then it will be handled upon the next scan). Tried to make it as simple as possible to avoid any strange interactions; technically, the approximate slopes are calculated all at once and the values are updated all at once; instead of starting from the beginning and sweeping all the way through the time series. This approximation always propagates forwards, and by virtue of its calculation, always takes the larger value to be the truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series post-corrections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fill and fix new features. They are not currently weighted but they will be in the future, hence\n",
    "the naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a number of tests feature\n",
    "# df.loc[:, 'n_tests_weighted'] = df.tests_cumulative.values\n",
    "\n",
    "# # Create a number of cases feature, there are multiple with the same name, hence the reason 'ind' is calculated\n",
    "# ind = df.loc[:, column_search(df, 'cases')].isna().sum().argmin()\n",
    "# df.loc[:, 'n_cases_weighted'] = df.loc[:, column_search(df, 'cases')].iloc[:, ind].values\n",
    "\n",
    "# # Create a number of deaths feature, there are multiple with the same name, hence the reason 'ind' is calculated\n",
    "# ind = df.loc[:, column_search(df, 'deaths')].isna().sum().argmin()\n",
    "# df.loc[:, 'n_deaths_weighted']  = df.loc[:, column_search(df, 'deaths')].iloc[:, ind].values\n",
    "\n",
    "# # Create a number of recovered feature\n",
    "# df.loc[:, 'n_recovered_weighted'] = df.recovered.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a number of tests feature\n",
    "df.loc[:, 'new_tests_weighted'] = (df.iloc[:,10].fillna(0) / (df.population/1000000.)).values\n",
    "\n",
    "# Create a number of cases feature, there are multiple with the same name, hence the reason 'ind' is calculated\n",
    "# ind = df.loc[:, column_search(df, 'cases')].isna().sum().argmin()\n",
    "df.loc[:, 'new_cases_weighted'] =  (df.iloc[:,6].fillna(0)/ (df.population/1000000.)).values\n",
    "\n",
    "# Create a number of deaths feature, there are multiple with the same name, hence the reason 'ind' is calculated\n",
    "# ind = df.loc[:, column_search(df, 'deaths')].isna().sum().argmin()\n",
    "df.loc[:, 'new_deaths_weighted']  =  (df.iloc[:,7].groupby(level=0).diff(1).fillna(0) / (df.population/1000000.)).values\n",
    "\n",
    "# Create a number of recovered feature\n",
    "df.loc[:, 'new_recovered_weighted'] = (df.recovered / (df.population/1000000.)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten_countries_with_most_missing_new_cases_data = \\\n",
    "# df[df.loc[:, column_search(df, 'new_cases')].iloc[:,0].isna()].groupby('location').count().sort_values(by='date')[-1:].index\n",
    "\n",
    "# series_sums = np.abs(((tmp.set_index(['date_proxy', 'location']).loc[:, column_search(df, 'new_cases')].iloc[:, 0].fillna(0) / (tmp.set_index(['date_proxy', 'location']).population/1000000.))-(tmp.set_index(['date_proxy', 'location']).loc[:, 'my_new_cases'])).groupby(level=1).sum())\n",
    "# differing_series = np.where(series_sums > 0.0000001)\n",
    "# differing_countries = df[df.location.isin(series_sums.index[differing_series[0][:1]])]\n",
    "# # args_countries_with_differing_series = np.where(>0.00001)\n",
    "\n",
    "# tmp = differing_countries#[df.location.isin(ten_countries_with_most_missing_new_cases_data)]\n",
    "# ax=(tmp.set_index(['date_proxy', 'location']).loc[:, column_search(df, 'new_cases')].iloc[:, 0].fillna(0) / (tmp.set_index(['date_proxy', 'location']).population/1000000.)).unstack().plot(figsize=(5,5))\n",
    "# # ax = tmp.set_index(['date_proxy', 'location']).loc[:, column_search(df, 'new_cases')].iloc[:, 0].fillna(0)\n",
    "# (tmp.set_index(['date_proxy', 'location']).loc[:, 'my_new_cases']).unstack().plot(ax=ax, figsize=(5,5))\n",
    "# plt.legend(loc=(1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better data source seems to be test_tracker for tests and cases. Take recovered from JHUCSSE, tests,cases, and deaths from TestTracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing_incorrectly = np.where(df.n_cases_weighted.diff(1)<0)[0]\n",
    "# decreasing_incorrectly = df.loc[df.index[decreasing_incorrectly],:]\n",
    "# decreasing_incorrectly[decreasing_incorrectly.date_proxy != 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_incorrect_decrease(df, 'n_recovered_weighted')\n",
    "# fix_incorrect_decrease(df, 'n_tests_weighted')\n",
    "# fix_incorrect_decrease(df, 'n_cases_weighted')\n",
    "# fix_incorrect_decrease(df, 'n_deaths_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing_incorrectly = np.where(df.n_cases_weighted.diff(1)<0)[0]\n",
    "# decreasing_incorrectly = df.loc[df.index[decreasing_incorrectly],:]\n",
    "# decreasing_incorrectly[decreasing_incorrectly.date_proxy != 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now just look at tests and cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigation_df = df.copy()\n",
    "\n",
    "investigation_df.loc[:, 'new_tests_per_million'] = df.loc[:, 'new_tests_weighted'].values   / per_million_population.values\n",
    "investigation_df.loc[:, 'new_cases_per_million'] =  df.loc[:, 'new_cases_weighted'].values  / per_million_population.values\n",
    "\n",
    "# investigation_df.loc[:, 'n_tests_per_density'] =  df.loc[:, 'n_tests_weighted'].values   / population_density.values\n",
    "# investigation_df.loc[:, 'n_cases_per_density'] = df.loc[:, 'n_cases_weighted'].values  / population_density.values\n",
    "\n",
    "# investigation_df.loc[:, 'n_tests_percent_pop'] = df.loc[:, 'n_tests_weighted'].values  * percent_pop.values\n",
    "# investigation_df.loc[:, 'n_cases_percent_pop'] =  df.loc[:, 'n_cases_weighted'].values  * percent_pop.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_day_df = investigation_df[investigation_df.date_proxy == investigation_df.date_proxy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10poptests = investigation_df.loc[present_day_df.new_tests_per_million.sort_values()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10poptests)]\n",
    "plttmp.set_index(['date_proxy', 'location']).new_tests_per_million.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n",
    "top10popcases= investigation_df.loc[present_day_df.new_cases_per_million.sort_values()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10popcases)]\n",
    "plttmp.set_index(['date_proxy', 'location']).new_cases_per_million.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that it is unwarranted to weight these countries with small populations so heavily as they comprise a tiny\n",
    "fraction of the world population. However, it also seems unfair to divide by area (i.e. to get density) for countries\n",
    "which have localized pockets of population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10casesdensity = investigation_df.loc[present_day_df.n_tests_per_density.sort_values().dropna()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10casesdensity)]\n",
    "plttmp.set_index(['date_proxy', 'location']).n_tests_per_density.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n",
    "\n",
    "top10testsdensity = investigation_df.loc[present_day_df.n_cases_per_density.sort_values().dropna()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10testsdensity)]\n",
    "plttmp.set_index(['date_proxy', 'location']).n_cases_per_density.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next weighting \"knows\" about the global population and so it couples each country to all other countries.\n",
    "I don't think this will cause any issues for the CNN in the modeling section but it might. It weights countries by\n",
    "the percent of the world population which they constitute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top10test_percent_pop = investigation_df.loc[present_day_df.n_tests_percent_pop.sort_values()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10test_percent_pop)]\n",
    "plttmp.set_index([plttmp.date_proxy, plttmp.location]).n_tests_percent_pop.unstack().plot(figsize=(5,5))\n",
    "plt.show()\n",
    "\n",
    "top10case_percent_pop = investigation_df.loc[present_day_df.n_cases_percent_pop.sort_values()[-10:].index,:].location\n",
    "plttmp = investigation_df[investigation_df.location.isin(top10test_percent_pop)]\n",
    "plttmp.set_index([plttmp.date_proxy, plttmp.location]).n_cases_percent_pop.unstack().plot(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate my logic: I want or perhaps need to normalize the data. If I normalize simply on a \"per million people\" basis,\n",
    "small micro-states such as San Marino will take on disproportionate values. I could drop these countries or look\n",
    "for other weighting techniques; I opt for the latter method. The other two options I explore are weighting by population density, and weighting by the percentage of the world's population that each country comprises. The goal is to find a normalization which captures how hard individual countries have been hit but also the possible future downside, which\n",
    "is of course is dependent upon how many people live in a country.\n",
    "\n",
    "Also, the time-series of countries which have caught more cases overall will likely have better reporting practices and\n",
    "be closer to the actual behavior of the virus transmission. Visual inspection of certain time series leads me to believe\n",
    "they will simply act as noise, obfuscating the average or \"actual\" behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current per capita weighting gives undue influence to microstate nations such as San Marino, as their population is\n",
    "only a small fraction of 1 million. Therefore, remove the countries in the bottom 10 percent (arbitrary) of populations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.population > df.population.quantile(0.25)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[:, 'n_tests_weighted'] =  df.loc[:, 'n_tests_weighted'].values / per_million_population.loc[df.index].values\n",
    "# df.loc[:, 'n_cases_weighted'] = df.loc[:, 'n_cases_weighted'].values  / per_million_population.loc[df.index].values\n",
    "# df.loc[:, 'n_deaths_weighted'] = df.loc[:, 'n_deaths_weighted'].values  / per_million_population.loc[df.index].values\n",
    "# df.loc[:, 'n_recovered_weighted'] =  df.loc[:, 'n_recovered_weighted'].values  / per_million_population.loc[df.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_day_df = df[df.date_proxy == df.date_proxy.max()]\n",
    "top10poptests = df.loc[present_day_df.new_tests_weighted.sort_values()[-10:].index,:].location\n",
    "plttmp = df[df.location.isin(top10poptests)]\n",
    "plttmp.set_index(['date_proxy', 'location']).new_tests_weighted.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n",
    "top10popcases= df.loc[present_day_df.new_cases_weighted.sort_values()[-10:].index,:].location\n",
    "plttmp = df[df.location.isin(top10popcases)]\n",
    "plttmp.set_index(['date_proxy', 'location']).new_cases_weighted.unstack().plot(figsize=(5,5))\n",
    "plt.legend(loc=(1,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the new features with which to model are in place, remove the redundant and useless infomation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pruned = df.drop(columns=(redundant_death_columns.tolist() \n",
    "                         + redundant_test_columns.tolist() \n",
    "                         + redundant_case_columns.tolist()+['recovered']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_quality_stringency_index = df_pruned.loc[:, 'stringency_index'].isna().sum().argmin()\n",
    "stringency = df_pruned.loc[:, 'stringency_index'].iloc[:, better_quality_stringency_index]\n",
    "\n",
    "df_pruned = df_pruned.drop(columns=['country_name', 'country_code',\n",
    "                                                    'm1_wildcard','stringency_index_for_display',\n",
    "                                                   'stringency_legacy_index', 'stringency_legacy_index_for_display',\n",
    "                                                    'government_response_index_for_display',\n",
    "                                                    'containment_health_index_for_display',\n",
    "                                                    'economic_support_index_for_display',\n",
    "                                                    'iso_code','stringency_index'])\n",
    "df_pruned.loc[:, 'stringency_index'] = stringency.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal preference of reordered the DataFrame; also helps with different missing value strategies later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = ['location','date','date_proxy','days_since_first_case']\n",
    "n_weighted_quantity_features = ['new_cases_weighted',\n",
    "            'new_tests_weighted', \n",
    "            'new_recovered_weighted', \n",
    "            'new_deaths_weighted']\n",
    "\n",
    "flag_features = column_search(df_pruned,'flag')\n",
    "time_independent_features =df_pruned.loc[:, 'population':'hospital_beds_per_100k'].columns.tolist()\n",
    "time_dependent_features = df_pruned.loc[:, 'c1_school_closing':'economic_support_index'].columns.difference(flag_features).tolist()+['stringency_index']\n",
    "flag_features = flag_features.tolist()+['tests_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = indexers + n_weighted_quantity_features + flag_features + time_dependent_features + time_independent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reorder = df_pruned.loc[:, indexers\n",
    "                              +n_weighted_quantity_features\n",
    "                              +time_dependent_features\n",
    "                              +time_independent_features\n",
    "                              +flag_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag columns take values 1.0, 0.0, np.nan. \n",
    "Convert these to categorical, and for the sake of column names map floats to str.\n",
    "Afterwards, drop the column that corresponds to missing values to remove collinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_df = df_reorder.loc[:, flag_features].fillna('Missing').astype('category')\n",
    "for col in flag_df.columns:\n",
    "    flag_df.loc[:, col] = flag_df.loc[:, col].cat.rename_categories({1.0 : '1', 0. : '0'})\n",
    "    \n",
    "flag_dummies = pd.get_dummies(flag_df)\n",
    "flag_dummies_without_collinearity = flag_dummies[flag_dummies.columns[~flag_dummies.columns.str.contains('Missing')]]\n",
    "df_reorder = pd.concat((df_reorder.drop(columns=flag_features),flag_dummies_without_collinearity),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I'm going to impute values for the time independent features, it may or may not be important to track this by flagging\n",
    "the missing values via one-hot encoding. Note that I have just manipulated the flagging columns so that *their* missing value flags are not included; but again this is because of linear dependency reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_flags = data_table_reorder.iloc[:,np.where(data_table_reorder.isna().sum() > 0)[0]].isna().astype(int)\n",
    "# missing_flags.columns = missing_flags.columns + '_missing_flag'\n",
    "# data_table_reorder = pd.concat((data_table_reorder, missing_flags), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time independent features, do not have to be careful with time; because there is no two-sided fillna method that\n",
    "I am aware of, handle this by filling forwards and backwards.\n",
    "\n",
    "Time dependent features can only be filled forward so that information from the future is not visible from the past. Additionally, any other missing values are filled with 0's; only other options would be, for instance, the mean up to that date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_indices in country_groupby(df_reorder):\n",
    "    fill_tmp = df_reorder.loc[country_indices, time_independent_features].fillna(method='ffill').fillna(method='bfill')\n",
    "    df_reorder.loc[country_indices, time_independent_features] = fill_tmp.values\n",
    "    df_reorder.loc[country_indices, time_dependent_features] = df_reorder.loc[country_indices, time_dependent_features].fillna(method='ffill').fillna(value=0)\n",
    "df_reorder.loc[:, time_independent_features] = df_reorder.loc[:, time_independent_features].fillna(df_reorder.loc[:, time_independent_features].median())\n",
    "df_reorder = regularize_names(df_reorder)\n",
    "df = df_reorder.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_means(df, features, roll_widths):\n",
    "    new_feature_df_list = []\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values\n",
    "        rollmean = None\n",
    "        for c in country_groupby(df):\n",
    "            if rollmean is None:\n",
    "                rollmean = df.loc[c, features].rolling(window).mean().fillna(value=0.)\n",
    "            else:\n",
    "                rollmean = pd.concat((rollmean,  df.loc[c,features].rolling(window).mean().fillna(value=0.)),axis=0)\n",
    "#         rollmean = pd.DataFrame(df..groupby(by='location').rolling(window).mean().fillna(value=0.))\n",
    "#         rollstd = pd.DataFrame(df.groupby(by='location').rolling(window).std().fillna(value=0.))    \n",
    "#         new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        new_features = rollmean\n",
    "        new_cols = pd.Index(features) +'_rolling_mean_' + str(window)\n",
    "#         rsind = features +'_rolling_std_' + str(window)\n",
    "#         new_cols = rmind.append(rsind)\n",
    "        new_features.columns = new_cols\n",
    "        new_feature_df_list.append(new_features)\n",
    "    return new_feature_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_df = df.loc[:, indexers + n_weighted_quantity_features + time_dependent_features].copy()\n",
    "new_features_list = rolling_means(model_df, n_weighted_quantity_features + time_dependent_features, roll_widths=[2,4,8,16,32])\n",
    "new_features = pd.concat(new_features_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=(10,10))\n",
    "for i, c in enumerate(df.loc[:,column_search(df,'index')].columns[1:]):\n",
    "    df.loc[:, [c,'location','date']].set_index(['location','date']).groupby('date').mean().plot(ax=axes.flatten()[i], legend=False)\n",
    "    axes.flatten()[i].set_title(c)\n",
    "    axes.flatten()[i].set_ylim([0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_features = indexers + n_weighted_quantity_features + df.loc[:,column_search(df,'index')].columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = model_df.loc[:, cnn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df.to_csv('cnn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_modeling = pd.concat((model_df, new_features),axis=1)\n",
    "export_to_modeling.to_csv('cnn_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook uses a variety of different COVID-19 related datasets to explore the behavior\n",
    "of the multiple time series'. This notebook also creates new features that attempt to encapsulate the\n",
    "time dependent (and time delayed) nature of the problem; these will be used during the model creation\n",
    "project which makes time dependent forecasting models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis<a id='EDA'></a>\n",
    "Ideas for the inclusion or creation of new columns.\n",
    "\n",
    "Moving averages\n",
    "fourier\n",
    "signal\n",
    "flags for lots of different things\n",
    "\n",
    "hardest hit countries\n",
    "\n",
    "days since\n",
    "\n",
    "which dataset it came from\n",
    "\n",
    "distribution of initial tests, responses, cases. \n",
    "\n",
    "humans view, interpret and forecast things in a way which are not available to robots. \n",
    "data driven, time dependent manner of modeling. Really trying to encapsulate the time dependence. \n",
    "\n",
    "I'm electing to fill the time independent features which are ***still*** missing with the median of ***all*** current values; otherwise\n",
    "I would have to drop them because of the missing values. For the time dependent features, I'll fill with 0's.\n",
    "\n",
    "Because the features I will be predicting are all measured per million people, it seems unwise to include\n",
    "very small countries and microstates such as San Marino; whose small population makes them has the maximum \n",
    "value in all quantities measured \"per million\". In other words, I want to normalize by population but the\n",
    "dynamics of these small states seems to be fundamentally different due to size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates = []\n",
    "past3days = df[df.date_proxy >= df.date_proxy.max()-2]\n",
    "for c in country_groupby(df):\n",
    "    growth_rates.append(past3days.loc[c.intersection(past3days.index),:].n_cases_weighted.diff(1).diff(1).dropna().values[0])\n",
    "growth_rates = np.array(growth_rates)\n",
    "growth_rates[np.abs(growth_rates)<10**-4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_growth = 100 * (growth_rates> 0).sum() / len(growth_rates)\n",
    "negative_growth =100 * (growth_rates < 0).sum() / len(growth_rates)\n",
    "stationary_growth = 100 * (growth_rates==0).sum() / len(growth_rates)\n",
    "print('Today, %.2f percent of countries have a positive growth rate' % positive_growth)\n",
    "print('Today, %.2f percent of countries have a negative growth rate' % negative_growth)\n",
    "print('Today, %.2f percent of countries have a stationary growth rate' % stationary_growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(df.loc[:, column_search(df, 'n_')[:4]]):\n",
    "    plt.scatter(len(df)*[i], (df.loc[:, c] - df.loc[:, c].mean()) / df.loc[:, c].std(), s=2,label=c)\n",
    "plt.legend(loc=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.location.isin(df.set_index([df.date_proxy, df.location]).index.get_level_values(1).unique()[-20:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index([df.date_proxy, df.location]).n_recovered_weighted.unstack().plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Time series variables with drift as baseline model : any columns with 'new' (cases, deaths, tests)\n",
    "Time series variables with naive as baseline model : The complement to the drift baseline variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The \"Our World in Data\" testing data isn't as reliable as the \"Test tracker\" testing data. \n",
    "This shows why it is important to be careful when selecting which representative from the\n",
    "set of redundant features to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "(100 * eda_data.set_index(['location', 'date']).isna().groupby(level=1).sum().loc[:, ['tests_cumulative_ttt', 'total_tests_owid']]\n",
    " / eda_data.location.nunique()).plot(ax=ax)\n",
    "plt.title('Percentage of missing daily test reports')\n",
    "plt.legend(loc=(0.5,0.8))\n",
    "every_nth = 2\n",
    "for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "    if n % every_nth != 0:\n",
    "        label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Death rate estimation, assuming 50% of people are asymptomatic and do not actually get tested or confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_data = df[df.date_proxy == df.date_proxy.max()]\n",
    "death_rate = []\n",
    "death_rate_estimation = []\n",
    "for c_i in country_groupby(df):\n",
    "    death_rate.append(100*df.loc[c_i,'n_deaths_weighted'].sum() / df.loc[c_i,'n_cases_weighted'].sum())\n",
    "    death_rate_estimation.append(100*df.loc[c_i,'n_deaths_weighted'].sum() / (2*(df.loc[c_i,'n_cases_weighted'].sum())))\n",
    "\n",
    "death_rate_estimation = np.array(death_rate_estimation)\n",
    "death_rate = np.array(death_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the number of missing values for cases is also the worst for the OWID data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eda_data.loc[:,['new_cases_ttc', 'new_cases_owid','cases_jhucsse']].isna().astype(int).groupby(eda_data.date).sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics of the \"age\" of the pandemic in each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The statistics of the age of the pandemic in each country, in units of days.\\n')\n",
    "print(df[df.date_proxy==df.date_proxy.max()].days_since_first_case.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government_responses = pd.concat((eda_data.loc[:, ['location','date_proxy','days_since_first_case']],\n",
    "                                  eda_data.loc[:, column_search(eda_data, 'oxcgrt')].drop(\n",
    "                                      columns=column_search(eda_data, 'flag')).iloc[:,2:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_with_all_responses = None\n",
    "for i, country_indices in enumerate(country_groupby(government_responses)):\n",
    "    government_responses.loc[country_indices,:] = government_responses.loc[country_indices,:].fillna(method='ffill')\n",
    "    has_all_flags = (government_responses.loc[country_indices,:].max() == 0).sum()\n",
    "    if has_all_flags != 0:\n",
    "        pass\n",
    "    else:\n",
    "        if countries_with_all_responses is None:\n",
    "            countries_with_all_responses = government_responses.loc[country_indices,:]\n",
    "        else:\n",
    "            countries_with_all_responses = pd.concat((countries_with_all_responses, government_responses.loc[country_indices,:]),axis=0)\n",
    "\n",
    "see_if_country_has_all_dates =countries_with_all_responses.groupby('location').count().sort_values(by='date_proxy') \n",
    "drop_these_countries = np.unique(np.where(~(see_if_country_has_all_dates == 148))[0])\n",
    "countries_with_all_responses = countries_with_all_responses[~countries_with_all_responses.location.isin(drop_these_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ranges = []\n",
    "responses = pd.Index(['pandemic_date_range']).append(countries_with_all_responses.columns[3:])\n",
    "for country_indices in country_groupby(countries_with_all_responses):\n",
    "    tmp = countries_with_all_responses.loc[country_indices,:].replace(to_replace=[0,0.], value=np.nan)\n",
    "    for c in tmp.columns[2:]:\n",
    "        active_range = tmp.set_index('date_proxy').loc[:,c].dropna()\n",
    "        response_ranges.append(pd.IndexSlice[int(active_range.index.min()):int(active_range.index.max()+1)])\n",
    "        \n",
    "response_slices_df = pd.DataFrame(np.array(response_ranges).reshape(countries_with_all_responses.location.nunique(), -1),\n",
    "                                 index=countries_with_all_responses.location.unique(), columns=responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = response_slices_df.applymap(lambda x : x.start)\n",
    "stop_dates = response_slices_df.applymap(lambda x : x.stop)\n",
    "start_dates.columns =[x[0] for x in response_slices_df.columns.str.split('_oxcgrt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = pd.plotting.scatter_matrix(start_dates, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_response_date = response_slices_df.applymap(lambda x : x.start).iloc[:, 1:].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_times = average_response_date.astype(int) - response_slices_df.applymap(lambda x : x.start).pandemic_date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(reaction_times, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_rates = pd.Series(death_rate, index=df.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_with_all_reactions = reaction_times.index.intersection(death_rates.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_reactions = (reaction_times.values - reaction_times.values.mean()) / reaction_times.values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_death_rates = death_rates.loc[countries_with_all_reactions]\n",
    "normalized_death_rates = (normalized_death_rates - normalized_death_rates.mean()) / normalized_death_rates.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(reaction_times, death_rates.loc[countries_with_all_reactions])\n",
    "\n",
    "plt.scatter(reaction_times, \n",
    "            death_rates.loc[countries_with_all_reactions])\n",
    "plt.plot(np.arange(-60,60), slope*np.arange(-60,60) + intercept, color='r') \n",
    "plt.title('Death rate vs. reaction time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Late responders, first government action AFTER first case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early responders, first government action BEFORE first case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day_responses_sum = countries_with_all_responses[countries_with_all_responses.days_since_first_case == 1].iloc[:, 3:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_of_first_responses = countries_with_all_responses[countries_with_all_responses.days_since_first_case == 1].date_proxy#.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(reaction_times, death_rates.loc[countries_with_all_reactions])\n",
    "\n",
    "plt.scatter(dates_of_first_responses, \n",
    "            death_rates.loc[countries_with_all_reactions])\n",
    "plt.plot(np.arange(-60,100), slope*np.arange(-60,100) + intercept, color='r') \n",
    "plt.title('Death rate vs. reaction time')\n",
    "print(slope, intercept, r_value, p_value, std_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_responders_args = first_day_responses_sum[first_day_responses_sum > 0].index\n",
    "early_responders = countries_with_all_responses.loc[early_responders_args,:].location.values\n",
    "print('\\n The early responders:\\n')\n",
    "print(early_responders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_responders_args = first_day_responses_sum[first_day_responses_sum == 0].index\n",
    "late_responders = countries_with_all_responses.loc[late_responders_args,:].location.values\n",
    "print('The late responders:\\n')\n",
    "print(late_responders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_df = df[df.location.isin(late_responders)]\n",
    "early_df = df[df.location.isin(early_responders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_df_present_day = early_df[early_df.date_proxy == df.date_proxy.max()]\n",
    "late_df_present_day = late_df[late_df.date_proxy == df.date_proxy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rate = pd.Series(death_rate, index=df.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_d_rates = d_rate.loc[early_df_present_day.location.unique()]\n",
    "late_d_rates = d_rate.loc[late_df_present_day.location.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(early_d_rates, early_df_present_day.n_cases_weighted.values, label='Early responders')\n",
    "_ = plt.scatter(late_d_rates, late_df_present_day.n_cases_weighted.values, label='Late responders')\n",
    "_ = plt.xlabel('Estimated death rate, (% of total cases)')\n",
    "_ = plt.ylabel('Number of cases per million people')\n",
    "_ = plt.legend(loc=(0.6,0.8))\n",
    "# plt.plot(np.sort(death_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not seem to be any demarcation between early and late responders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp = pd.concat((df, (df.n_deaths_weighted / df.n_cases_weighted).to_frame(name='death_rates')),axis=1)\n",
    "xcr = dftmp.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcr.loc['death_rates',:].sort_values()[:-1].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
