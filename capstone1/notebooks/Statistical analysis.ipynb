{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from IPython.core.display import HTML\n",
    "from scipy.stats import norm, poisson,expon,lognorm,skewnorm,exponnorm,skew,kstest\n",
    "from math import pi\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cleaned loan data\n",
    "loan_data = pd.read_csv('loan_data.csv', index_col=False)\n",
    "categorical_data = pd.read_csv('categorical_data.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes all of the sample values of a feature X (DataFrame.column or NumPy array)\n",
    "# and returns the equivalent number of samples(variates) from certain SciPy distributions\n",
    "# specified by keyword argument, \"distribution\"\n",
    "def fit_to_data(X, distribution='norm',**kwargs):\n",
    "    if distribution=='skewnorm':\n",
    "        params = skewnorm.fit(X)\n",
    "        variates = skewnorm.rvs(a=params[0], loc=params[1], scale=params[2],  size=len(X))\n",
    "        \n",
    "    elif distribution=='lognorm':\n",
    "        params = lognorm.fit(X)\n",
    "        variates = lognorm.rvs(s=params[0],loc=params[1],scale=params[2], size=len(X))\n",
    "\n",
    "    elif distribution=='expon':\n",
    "        params = expon.fit(X)\n",
    "        variates = expon.rvs(loc=params[0],scale=params[1], size=len(X))\n",
    "\n",
    "    elif distribution=='norm':\n",
    "        params = norm.fit(X)\n",
    "        variates = norm.rvs(loc=params[0], scale=params[0],size=len(X))\n",
    "\n",
    "    elif distribution=='exponnorm':\n",
    "        params = exponnorm.fit(X)\n",
    "        variates = exponnorm.rvs(params[0],loc=params[1],scale=params[2], size=len(X))\n",
    "\n",
    "    else:\n",
    "        dlist = ['skewnorm','norm','expon','lognorm','exponnorm']\n",
    "        print('Please specify one of the following distributions {},{},{},{},{}'.format(dlist))\n",
    "        return None\n",
    "    \n",
    "    return variates\n",
    "\n",
    "\n",
    "# Function that is used to contrast kernel density estimate distribution to histogram of actual\n",
    "# values. \n",
    "def hist_density_plot(x, variates,xlabel=None, title=None):\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    ax1.hist(x, alpha=0.5,bins=50)\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "\n",
    "    axtwin = ax1.twinx()\n",
    "    sns.kdeplot(variates,ax=axtwin,color='r')\n",
    "    axtwin.set_title(title)\n",
    "    axtwin.set_ylabel('Density')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "# Some of these were previously used but at now unused; they're hanging around incase I decide to use them again.\n",
    "def imbalance_thresholding(df, threshold):\n",
    "    mode_percentages = [len(df[df[x] == df[x].mode().values[0]])/len(df) for x in df.columns]\n",
    "    count_df = pd.DataFrame(mode_percentages, index=df.columns, columns=['percentages'])\n",
    "    balanced_df = df.loc[:, df.columns.isin(count_df[count_df.percentages < threshold].index.tolist())]\n",
    "    return balanced_df\n",
    "\n",
    "def max_pairwise_correlations(df):\n",
    "    # Produce all correlations to the relations between features\n",
    "    correlations_ = df.corr()\n",
    "    # Maximum correlations (excluding auto-correlation)\n",
    "    correlations_df = correlations_.unstack().to_frame(name='data')\n",
    "    # Remove the auto-correlations which are trivial / not useful values.\n",
    "    correlations_no_auto = correlations_df[correlations_df['data']!=1]\n",
    "    # To pick out the maximum pairwise correlations, \n",
    "    maxcvalues = correlations_no_auto[correlations_no_auto['data'] == correlations_no_auto.groupby(level=[0])['data'].transform(max)]\n",
    "    return maxcvalues\n",
    "\n",
    "def correlation_thresholding(df, threshold=0.999):\n",
    "    # Remove features ( technically remove one component of a pair) whose maximum pairwise pearson correlation is\n",
    "    # greater than threshold, default value 0.999, if no such columns exist then return original DataFrame.\n",
    "    maxcorr = max_pairwise_correlations(df)\n",
    "    maxcorr = maxcorr[maxcorr.data > threshold]\n",
    "    columns_to_drop = maxcorr.sort_values(by='data',ascending=False).reset_index()[::2].level_1.values\n",
    "    if len(columns_to_drop) > 0:\n",
    "        return df.drop(columns=columns_to_drop)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def clean(df,threshcorr=0.99, threshcount=0.99):\n",
    "    clean_df = df.copy()\n",
    "    print('Original shape: ', df.shape)\n",
    "    clean_df = clean_df.dropna()\n",
    "    clean_df = imbalance_thresholding(clean_df,threshcount)\n",
    "    print('pruned shape (filtering out imbalanced features):', clean_df.shape)\n",
    "    clean_df = correlation_thresholding(clean_df, threshcorr)\n",
    "    print('pruned shape (filtering out correlated features):', clean_df.shape)\n",
    "    return clean_df\n",
    "\n",
    "As a reminder, there are continuous and discrete numerical variables as well as categorical variables (converted to\n",
    "discrete numerical variables by transformation by integer codes). With over two million samples, my first thought was\n",
    "that most of these continuous variables would be normally distributed. This was a result of being new to financial data\n",
    "and I suppose naivety. As I'll show, there is a variety of distributions which the continuous variables take; some of which follow no recognizable or nice form.\n",
    "\n",
    "The goal of this particular portion of the statistical analysis is to get a feel for how the variables are distributed because \n",
    "there are hyperparameters such as class weighting that can be very beneficial depending on the results of this investigation.\n",
    "\n",
    "First, To demonstrate how imbalanced some of the data features are, let's look at the percentage of data values that the mode of each feature represents.\n",
    "\n",
    "loan_data.mode() / len(loan_data)\n",
    "\n",
    "Highly imbalanced data and highly correlated data require special care and considerations as they both seem to invite improper manipulation. An example would be to  modify the data by pruning features with pearson correlation above a certain threshold. For example, there are a number of pairs of features with pearson correlation scores greater than 0.999 for a specific and relatively obvious reason. Specifically, some features are essentially identical; an example being: the funded amount of a loan and funded amount of a loan from investors. If investors represent the overwhelming majority of loan funding then these features are nearly identical which seems to be the case upon inspection.The best course of action seems to be to leave the features untouched and to simply include considerations for these properties such as regularization and balanced class weighting. \n",
    "\n",
    "Along the same vein let's look at the maximal pairwise correlations between features using the function written above.\n",
    "\n",
    "max_pairwise_correlations(loan_data).sort_values(by='data',ascending=False)[::2]\n",
    "\n",
    "# Sampling distribution of summary statistic (func) replicates produced by permutation replicates\n",
    "# (sampling without replacement)\n",
    "def permutation_replicates(data1, data2, func, size=10000):\n",
    "    # Initialize array of replicates: perm_replicates\n",
    "    perm_replicates = np.empty(size)\n",
    "\n",
    "    for i in range(size):\n",
    "        # Generate permutation sample\n",
    "        perm_sample_1, perm_sample_2 = permutation_sample(data1, data2)\n",
    "\n",
    "        # Compute the test statistic\n",
    "        perm_replicates[i] = func(perm_sample_1,perm_sample_2)\n",
    "\n",
    "    return perm_replicates\n",
    "\n",
    "def permutation_sample(data1, data2):\n",
    "    # Concatenate the data sets: data\n",
    "    data = np.concatenate((data1,data2))\n",
    "\n",
    "    # Permute the concatenated array: permuted_data\n",
    "    permuted_data = np.random.permutation(data)\n",
    "\n",
    "    # Split the permuted array into two: perm_sample_1, perm_sample_2\n",
    "    perm_sample_1 = permuted_data[:len(data1)]\n",
    "    perm_sample_2 = permuted_data[len(data1):]\n",
    "\n",
    "    return perm_sample_1, perm_sample_2\n",
    "\n",
    "def diff_of_mean(data1, data2):\n",
    "    return np.mean(data1)-np.mean(data2)\n",
    "\n",
    "# Sampling distribution of summary statistic (func) replicates produced by bootstrap sampling\n",
    "# (sampling with replacement)\n",
    "def bootstrap_replicates(data, func, size=10000):\n",
    "    bs_replicates = np.empty(size)\n",
    "\n",
    "    # Generate replicates\n",
    "    for i in range(size):\n",
    "        bs_replicates[i] = func(np.random.choice(data, size=len(data)))\n",
    "\n",
    "    return bs_replicates\n",
    "\n",
    "def difference_of_mean_p_value(data1, data2, size=1000):\n",
    "    mean_diff = diff_of_mean(data1, data2) \n",
    "    print('The empirical difference of means is {:0.2f}'.format(mean_diff))\n",
    "    # Compute samples of difference of means: bs_diff_replicates\n",
    "    mean_diff_perm_replicates = permutation_replicates(data1, data2, diff_of_mean, size=size)\n",
    "\n",
    "    p = np.sum(mean_diff_perm_replicates >= mean_diff) / len(mean_diff_perm_replicates)\n",
    "    # Print the results\n",
    "    print('difference of means = ${:0.2f}'.format(mean_diff))\n",
    "    print('p-value =', p)\n",
    "    return p, mean_diff_perm_replicates\n",
    "\n",
    "# Import the cleaned loan data\n",
    "loan_data = pd.read_csv('loan_data.csv', index_col=False)\n",
    "categorical_data = pd.read_csv('categorical_data.csv', index_col=False)\n",
    "\n",
    "Need to account for time series data by either eliminating it from the dataset to avoid snooping or account for it. \n",
    "There aren't variables which has day-to-day interactions so perhaps there is enough time for the data to be uncorrelated. \n",
    "\n",
    "Splitting into numerical data for linear regression and object data for classification. The reason for this is to 1. reduce the number of time dependent variables in either set (is this sample bias?) \n",
    "What about the variables that aren't explicitly time variables but are implicitly dependent on time? For instance, in order to have a recovery amount, the loan must have been charged off, which means it is likely past the maturity date of the loan, which means it contains time dependent data. \n",
    "\n",
    "\n",
    "\n",
    "[len(df[df[x] == df[x].mode().values[0]])/len(df) for x in df.columns]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
