{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Datasets currently not used\n",
    "\n",
    "\n",
    "\n",
    "## [The COVID tracking project testing data.](#source1)\n",
    "[https://covidtracking.com/api](https://covidtracking.com/api)\n",
    "\n",
    "## [Delphi-epidata ](#delphi) which contains \n",
    "       Facebook surveys, google surveys, doctor visits, google health trends, quidel test data\n",
    "[https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)\n",
    "\n",
    "I have not dove into this dataset too thoroughly but it contains information from facebook and google\n",
    "surveys regarding COVID as well as doctor visits; the doctor visit data attempts to make distinctions between\n",
    "those sick with the annual influenza and those with COVID.\n",
    "\n",
    "\n",
    "## [IHME hospital data](#ihme)\n",
    "\n",
    "**Data available at:**\n",
    "[http://www.healthdata.org/covid/data-downloads](http://www.healthdata.org/covid/data-downloads)\n",
    "\n",
    "The IHME hospital data is one of the more unique datasets I've discovered with \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 0.5)\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "#     if normalization_method=='minmax':\n",
    "        # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "        # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "        # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "        # be repeated for each validation, test frame for each country for each timestep.\n",
    "    X_min = X_train.min(axis=(2))\n",
    "    X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "    X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "                    X_max[:, :, np.newaxis, :])\n",
    "    X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "                           X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "    if train_test_only:\n",
    "    # Normalize the training data by each frame's specific mean and std deviation. \n",
    "        splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    else:\n",
    "        X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "                                   X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "        splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "#     else:\n",
    "        \n",
    "#         X_mean = X_train.mean(axis=(1,2))\n",
    "#         X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "\n",
    "#         X_train_scaled = normal(X_train, \n",
    "#                                 X_mean[:, np.newaxis, np.newaxis, :],\n",
    "#                                 X_std[:, np.newaxis, np.newaxis, :])\n",
    "#         X_test_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "#         if train_test_only:\n",
    "#         # Normalize the training data by each frame's specific mean and std deviation. \n",
    "#             splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "#         else:\n",
    "#             X_validate_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "#             splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME hospital data\n",
    "<a id='ihme'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "[JHU CSSE](#csse) \n",
    "<font color='red'>\n",
    "### Has all USA states but only 32 countries which overlap with other data; stash this dataset for now. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_df = column_or_index_string_reformat(pd.read_csv(\n",
    "    './IHME_hospital_data/2020_04_12.02/Hospitalization_all_locs.csv').rename(columns={'location_name':'location'}))\n",
    "ihme_df.loc[:, 'date'] = pd.to_datetime(ihme_df.loc[:,'date']).dt.normalize()\n",
    "ihme_df = ihme_df.set_index(['location', 'date']).sort_index()\n",
    "ihme_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delphi-epidata\n",
    "<a id='delphi'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "data_source\tname of upstream data source \n",
    "(e.g., fb-survey, google-survey, ght, quidel, doctor-visits)\tstring\n",
    "\n",
    "signal\tname of signal derived from upstream data (see notes below)\tstring\n",
    "\n",
    "time_type\ttemporal resolution of the signal (e.g., day, week)\tstring\n",
    "\n",
    "geo_type\tspatial resolution of the signal (e.g., county, hrr, msa, dma, state)\tstring\n",
    "\n",
    "time_values\ttime unit (e.g., date) over which underlying events happened\tlist of time values (e.g., 20200401)\n",
    "\n",
    "geo_value\tunique code for each location, depending on geo_type (county -> FIPS 6-4 code, HRR -> HRR number, MSA -> CBSA code,\n",
    "DMA -> DMA code, state -> two-letter state code), or * for all\tstring\n",
    "\n",
    "As of this writing, data sources have the following signals:\n",
    "\n",
    "fb-survey signal values include raw_cli, raw_ili, raw_wcli, raw_wili, and also four additional named with raw_* replaced by smoothed_* (e.g. smoothed_cli, etc).\n",
    "google-survey signal values include raw_cli and smoothed_cli.\n",
    "ght signal values include raw_search and smoothed_search.\n",
    "quidel signal values include smoothed_pct_negative and smoothed_tests_per_device.\n",
    "doctor-visits signal values include smoothed_cli.\n",
    "\n",
    "Delphi API data :\n",
    "doctor visits : 20200201-20200429 (as of 20200503)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Currently Unused ----------------------#\n",
    "\n",
    "def pull_delphi_data(data_source=['fb-survey', 'google-survey', 'ght', 'quidel', 'quidelneg', 'doctor-visits'], \n",
    "                     daterange=pd.date_range(start=\"20200101\",\n",
    "                                             end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d'),\n",
    "                     **kwargs):\n",
    "    \"\"\" Pull data from https://cmu-delphi.github.io/delphi-epidata/api/\n",
    "        https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for data in data_source:\n",
    "        signal_dict = {'fb-survey':'smoothed_cli',\n",
    "                       'google-survey':'smoothed_cli',\n",
    "                       'ght':'smoothed_search',\n",
    "                       'quidel':'smoothed_tests_per_device',\n",
    "                       'quidelneg':'smoothed_pct_negative',\n",
    "                       'doctor-visits':'smoothed_cli'}\n",
    "        \n",
    "        signal = signal_dict[data]\n",
    "        if data=='quidelneg':\n",
    "            #change the proxy for the quidel signal\n",
    "            data = 'quidel'\n",
    "        for days in daterange:\n",
    "            resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "            day_data = resp.json().get('epidata', None)\n",
    "            if day_data is None:\n",
    "                pass\n",
    "            else:\n",
    "                var_number += pd.json_normalize(day_data).size\n",
    "                print(pd.json_normalize(day_data).shape)    \n",
    "                \n",
    "                \n",
    "# date_range_2020 = pd.date_range(start=\"20200101\",end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d')\n",
    "# var_number = 0 \n",
    "# for days in date_range_2020:\n",
    "# #     days='20200302'\n",
    "#     resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "#     day_data = resp.json().get('epidata', None)\n",
    "#     if day_data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         var_number += pd.json_normalize(day_data).size\n",
    "#         print(pd.json_normalize(day_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Unused as of now</font>\n",
    "\n",
    "\n",
    "## Repeat of the above calculations for United States only data.\n",
    "\n",
    "\n",
    "\n",
    "The United States' data merits separate investigation 1. because of the case number 2. because the IHME dataset is only really\n",
    "properly defined for the statewide description of the U.S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_us_timeseries_df.groupby(level=1).sum().to_csv('csse_united_states.csv')\n",
    "\n",
    "usa_data = [\n",
    "    csse_us_timeseries_df,\n",
    "    ihme_df,\n",
    "    owid_df,\n",
    "    oxcgrt_df,\n",
    "    testtrack_df]\n",
    "\n",
    "us_table = multiindex_to_table(csse_us_timeseries_df)\n",
    "today = us_table[us_table.date==us_table.date.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "#Pull the data using their API (for whatever reason this data set is different from the manual download).\n",
    "# url_to_present_date = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-01-02/' \\\n",
    "#                         + str(datetime.now().date())\n",
    "# response = requests.get(url_to_present_date)\n",
    "# response_json = response.json()\n",
    "# response_json_nested_dict = response_json['data']\n",
    "\n",
    "# response_api_df = pd.DataFrame.from_dict({(i,j): response_json_nested_dict[i][j] \n",
    "#                            for i in response_json_nested_dict.keys() \n",
    "#                            for j in response_json_nested_dict[i].keys()},\n",
    "#                        orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.25)\n",
    "sigmoid = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "A = np.vstack([x, np.ones(len(x))]).T\n",
    "m, c = np.linalg.lstsq(A, sigmoid, rcond=None)[0]\n",
    "\n",
    "over = np.where(((m * x + c - sigmoid) > 0)  & (x<=0))\n",
    "under = np.where(((m * x + c - sigmoid) < 0) & (x>=0))\n",
    "xover_width = x[over].max() - x[over].min() \n",
    "xunder_width = x[under].max() - x[under].min()\n",
    "\n",
    "sigmoidmin, sigmoidmax = [-0.05, 1.05]\n",
    "\n",
    "# fig, (ax,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(x, sigmoid, color='k')\n",
    "_ = ax.plot(x, m * x + c, '--',color='k')\n",
    "_ = ax.set_ylim([-0.05, 1.05])\n",
    "_ = ax.vlines(x[over].min(), ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.vlines(0, ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.vlines(x[under].max(), ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.hlines(0, xmin=-10, xmax=10)\n",
    "_ = ax.add_patch(Rectangle((x[over].min(), 0), xover_width, sigmoidmax, \n",
    "                           angle=0.0, alpha=0.5, color='k', label='Overestimation'))\n",
    "_ = ax.add_patch(Rectangle((0, 0), x[under].max(), sigmoidmax, \n",
    "                           angle=0.0, alpha=0.25, color='gray', label='Underestimation'))\n",
    "\n",
    "_ = ax.set_title('Possible explanation for regression prediction behavior')\n",
    "_ = ax.legend(loc=(0,1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "#the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "    df_list.append(tmp_df)\n",
    "\n",
    "daily_reports_df = pd.concat(df_list, axis=0)\n",
    "daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values\n",
    "\n",
    "daily_reports_df.loc[:, 'combined_key'] = (daily_reports_df.province_state.astype('str').replace(to_replace='nan', value='')+' '+ daily_reports_df.location.astype('str')).values\n",
    "\n",
    "daily_reports_df = daily_reports_df.drop(columns=['province_state', 'last_update', 'fips', 'admin2']).set_index(['location','date'])\n",
    "\n",
    "daily_reports_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
