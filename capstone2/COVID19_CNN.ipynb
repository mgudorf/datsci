{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN COVID-19 case number forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've still been tinkering with the CNN post-report and so the reported results are different than the results contained here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################  Helper functions for debugging, mainly #################\n",
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "\n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict, n_countries, title='',suptitle='', \n",
    "                   figname=None, scale=None, s=None, model_params=None):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    # compute scores \n",
    "    mse_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_naive, r2_predict))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    \n",
    "    ax1.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_naive,3)),fontsize=14)\n",
    "    ax1.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_naive,3)),fontsize=14)\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "             \n",
    "    ax2.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_predict,3)),fontsize=14)\n",
    "    ax2.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_predict,3)),fontsize=14)\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "\n",
    "    \n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    ax3.set_title('')\n",
    "    ax4.set_title('')\n",
    "    ax3.set_ylabel('Residual')\n",
    "    ax4.set_ylabel('Residual')\n",
    "    ax3.grid(True)\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    if figname is not None:\n",
    "        plt.savefig(figname, bbox_inches='tight')\n",
    "        \n",
    "    if model_params is not None:\n",
    "        param_labels = ['epochs = ', 'batch_size = ', 'filter_1 = ', 'filter_2 = ', \n",
    "                        'kernel_size_1 = ', 'kernel_size_2 = ', 'first_dense_layer_output_dimension = ']\n",
    "        label_value_pairs = [param_labels[i] + str(model_params[i]) for i in range(len(model_params))]\n",
    "        plt.text(0.0, 1.15, ', '.join(label_value_pairs), transform=ax1.transAxes)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Normalize with respect to some arbitrary absolute max, just choose 2*absolute max of training set. \n",
    "    The function is so long because the different shapes of the different ps\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The main format (shape) of the data and how it is being sliced and then reshaped is described in the notebook ```COVID19_model_prototypes.ipynb``` and so I will avoid a redundant discussion here. \n",
    "\n",
    "In regards to the feature data being considered, I am using the time dependent variables pertaining directly to COVID-19, such as new cases, new deaths, etc. as well as time dependent variables which quantify different governments' reactions to the pandemic; most notably is the usage of the oxford's \"stringency index\" which scores each goverments reaction with a number from 0 to 100.\n",
    "\n",
    "I am in fact not using the data on the number of tests, however, because the feature lacks a consistent set of units. In the regression this is counteracted upon by using one-hot encoding to categorize the units but instead of stratifying the new tests variable by units, I elect to simply drop the testing data, which may or may not be an unwise choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data on 132 countries, and for 161 different days. There is not case information for all countries for all dates, this may be a source of error and one potential correction is to remove all \"frames\" which contain 0 information on the pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 different countries included in the data\n",
      "The time series span 183 different days\n"
     ]
    }
   ],
   "source": [
    "print('There are {} different countries included in the data'.format(data.location.nunique()))\n",
    "print('The time series span {} different days'.format(data.time_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full intersection of countries in fact included more information, but the countries with populations less than 1 million people were pruned from the dataset, in order to remove any chance of undue influence from very small states such as the Vatican or San Marino, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data.csv', index_col=0)\n",
    "tmp =pd.read_csv('regression_data_full.csv', index_col=0)\n",
    "data = pd.concat((data,tmp.loc[:, column_search(tmp,'government_response')]),axis=1)\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(columns=column_search(data,'test'))\n",
    "data = data.drop(columns=column_search(data,'deaths'))\n",
    "data = data.drop(columns=column_search(data,'recovered'))\n",
    "# data = data.drop(columns=column_search(data,'log'))\n",
    "data = data.drop(columns=column_search(data,'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this notebook is to tune the hyperparameters and architecture of a simple one dimensional convolutional neural network such that it is able to make accurate predictions of the number of new cases. In pursuit of this goal, I elect to utilize the keras API for the deep learning implementation. The details or philosophy on how the model is constructed is also detailed in the model prototypes notebook, this notebook is simply devoted to tuning the model parameters; the architecture could also be played with (and has been) but I will leave any dramatically different architectures to different notebooks. The current architecture in this notebook represents the best (using the performance from the prototyping stage as a metric). Other inclusions which were previously tested were: more activation layers, pooling layers, dropout layers, non negative kernel constraints (the predicted value should be non-negative, there is a constraint for this). The architecture that is fixed in this notebook consists of two convolutional layers followed by two fully connected layers. In the first convolutional layer, the number of filters is chosen to be much larger than the filters in the second convolutional layer (but is also tuned). Likewise, the output of the first fully connected layer is much larger than the second (which has only 1 output in this setup). To ensure that all future tests are comparable, I always start with the same model coefficients by initializing the kernels of each layer of the network. This introduces even more parameters, using a normal distribution to initialize, such as the mean and standard deviation. This is not an issue however as in the final model these kernel initializers will be unincluded. The data is split such that the final holdout set contains a single day's information, the validation set contains 1 week or 7 days of information and the remainder is in the training set.\n",
    "\n",
    "As a reminder, the data format as well as the features being included are both described in the model prototyping notebook. The most obvious parameters with which to tune the model are the number of training epochs, the batch size of the training, the number of filters in each convolutional layer, the size of the convolutional kernel (i.e. convolutional window) and finally the number of outputs in the first dense layer. The number of convolutional filters are chosen such that the ratio is always the same between the first and second layer. Something to keep in mind as well is that the data I am training with contains only 20000 samples, and so the number of parameters should reflect this fact, by not being too numerous. \n",
    "\n",
    "The following values will be used for tuning purposes. \n",
    "\n",
    "    epochs = [50, 100, 200]\n",
    "    batch_size = [64, 256, 1024]\n",
    "    filter1 = [16,32,64]\n",
    "    filter2 = [2,4,8]\n",
    "    kernel1_size = [4,6,8]\n",
    "    kernel2_size = [4,6,8]\n",
    "    \n",
    "I elect to constrain the output of the first dense layer to be the same as the number of inputs to the first dense layer, which is in turn the number of outputs of the second convolutional layer. This dimension depends on the kernel size as well as filter number in said convolutional layers.\n",
    "\n",
    "Create a 5-d tensor for convolution and FC layer parameters using itertools.\n",
    "The layers of the CNN will remain the same throughout testing for now. That is, \n",
    "\n",
    "    1. Conv1D(f1, k1)\n",
    "    2. Conv1D(f2, k2)\n",
    "    3. Flatten\n",
    "    4. Dense(d)\n",
    "\n",
    "\n",
    "I don't know enough yet, but my intuition tells me that the overall trend (macroscopic picture)\n",
    "is much more important to capture than microscopic. \n",
    "   \n",
    "    f1 : number of filters,  first convolutional layer\n",
    "    k1 : kernel size, first convolutional layer\n",
    "    f1 : number of filters,  second convolutional layer\n",
    "    k1 : kernel size, second convolutional layer\n",
    "    d : number of nodes in hidden fully connected layer\n",
    "    \n",
    "\n",
    "from https://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf\n",
    "To achieve error $\\epsilon$ with filter number $m$ the number of samples needed is $\\mathcal{O}(m/\\epsilon^2)$\n",
    "Let $\\epsilon = 0.1$, then $100m$ = samples or $m = samples/100$. The number of samples is\n",
    "It seems that included information from the too distant past actually makes the model worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main differences from prototyping stage.\n",
    "\n",
    "The data being used, and the parameters being tuned; it was the architecture that was mainly tested previously in the\n",
    "prototyping notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.time_index>=40]\n",
    "# model_data = data.copy().iloc[:, 3:]\n",
    "modeling_features = ['new_cases_per_million', 'government_response_index', 'log_new_cases_per_million']\n",
    "model_data = data.copy().loc[:, modeling_features]\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 28\n",
    "start_date = frame_size + data.time_index.min()\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create, split, and scale the data into tensors which will abide by keras conventions (after collapsing the first axis, at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 154 181\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_cnn_train, y_cnn_train, X_cnn_validate, y_cnn_validate, X_cnn_test, y_cnn_test) = scaled_splits\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the split into training, validation, testing sets using a single time series. The data passed to the CNN utilizes frames of time, so in the visualization I am identifying each frame with its leading edge. I.e. the testing (green) set contains a single frame of 28 days of values, not a single day's worth of values as one might be lead to believe by the single green point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAEvCAYAAAByngQ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xU9Znn8c8DFDQXRYZLwlVQMVykG9q2C9aQl5jpKLqC2Rglia44WZnNxDY7JlPRV3aMZuOO6WSIE1czYxLXRIjGJaOSRUYxdoKbwYamuag0BFSgW7wgIveGBn77R1W3RXdV9+nuc6pOVX3frxcv6nKq6lenT1c9/fye8/zMOYeIiIiIBKNXtgcgIiIiks8UbImIiIgESMGWiIiISIAUbImIiIgESMGWiIiISIAUbImIiIgEqE+2B9DWsGHD3Pjx47M9DBEREZFOrV+//gPn3PCOtgldsDV+/Hhqa2uzPQwRERGRTpnZrs620TSiiIiISIAUbImIiIgESMGWiIiISIBCV7OVSnNzM42NjTQ1NWV7KDmhqKiIMWPGEIlEsj0UERGRgpcTwVZjYyNnnXUW48ePx8yyPZxQc86xb98+GhsbmTBhQraHIyIiUvByYhqxqamJoUOHKtDywMwYOnSosoAiIiIh4SnYMrMrzWybme0wsztT3P8ZM6szs5Nmdl2b+242s+2Jfzd3d6AKtLzTvhIREQmPToMtM+sNPATMBaYAXzKzKW022w0sBH7d5rF/AXwXiALlwHfNbEjPh515H330EQ8//HCXH3fVVVfx0UcfBTAiERERyQVeMlvlwA7n3JvOuRPAk8D85A2cczudc5uB020eewWwyjn3oXNuP7AKuNKHcWdcumDr1KlTHT7uueee45xzzglqWCIiIhJyXgrkRwMNSdcbiWeqvEj12NEeHxsqd955J2+88QbTp08nEokwaNAgRo4cycaNG9myZQvXXnstDQ0NNDU18Y1vfINFixYBH3fEP3z4MHPnzuXTn/40//7v/87o0aN59tln6d+/f5bfmYiIiATJS2YrVQGQ8/j8nh5rZovMrNbMavfu3evxqTu2ast73P3sa6za8p4vz3f//fdz/vnns3HjRn74wx+ydu1a7rvvPrZs2QLAo48+yvr166mtreUnP/kJ+/bta/cc27dv5+tf/zqvv/4655xzDr/97W99GZuIiIiEl5dgqxEYm3R9DLDH4/N7eqxz7hHnXJlzrmz48A7XcvRk1Zb3uP2JDfxqzS5uf2KDbwFXsvLy8jNaK/zkJz+hpKSEmTNn0tDQwPbt29s9ZsKECUyfPh2Aiy++mJ07d/o+LhEREQkXL8HWOmCimU0ws77AAmC5x+d/HvicmQ1JFMZ/LnFboF7evpdjzfFaqmPNp3h5uz/ZsmQDBw5svfyHP/yBF198kTVr1rBp0yZmzJiRsvVCv379Wi/37t2bkydP+j4uERERCZdOgy3n3EngNuJBUj3wlHPudTP7npnNAzCzS8ysEfgi8C9m9nrisR8C/4N4wLYO+F7itkDNnjic/pHeAPSP9Gb2xJ5ny8466ywOHTqU8r4DBw4wZMgQBgwYwNatW3nllVd6/HoiIiKSHzx1kHfOPQc81+a2u5MuryM+RZjqsY8Cj/ZgjF1WMeUT/ORLM3h5+15mTxxOxZRP9Pg5hw4dyqWXXspFF11E//79+cQnPn7OK6+8kn/+53+muLiYT33qU8ycObPHryciIiL5wZzzWuueGWVlZa62tvaM2+rr65k8eXKWRpSbtM9ERESCZ2brnXNlHW2TE8v1iIiIiOQqBVsiIiIiAVKwJSIiIhIgBVsiIiIiAVKwJSIiIhIgBVsiIiIiAVKwFZBBgwYBsGfPHq677rqU21x22WW0bXPR1gMPPMDRo0d9H5+IiIhkhoKtgI0aNYply5Z1+/EKtkRERHKbgi2Pvv3tb/Pwww+3Xr/nnnu49957+exnP0tpaSnTpk3j2Wefbfe4nTt3ctFFFwFw7NgxFixYQHFxMTfccAPHjh1r3e5rX/saZWVlTJ06le9+97tAfHHrPXv2MGfOHObMmQPACy+8wKxZsygtLeWLX/wihw8fDvJti4iISA/lb7C19TlY8a34/z5YsGABv/nNb1qvP/XUU9xyyy08/fTT1NXVUV1dzTe/+U066sj/05/+lAEDBrB582a+853vsH79+tb77rvvPmpra9m8eTN//OMf2bx5M7fffjujRo2iurqa6upqPvjgA77//e/z4osvUldXR1lZGYsXL/bl/YmIiEgwPK2NmHO2Pge//StoPgYbl8AXHoVJV/XoKWfMmMH777/Pnj172Lt3L0OGDGHkyJH87d/+LatXr6ZXr168/fbbvPfee3zyk59M+RyrV6/m9ttvB6C4uJji4uLW+5566ikeeeQRTp48yTvvvMOWLVvOuB/glVdeYcuWLVx66aUAnDhxglmzZvXofYmIiEiw8jPYeuOleKAF8f/feKnHwRbAddddx7Jly3j33XdZsGABS5cuZe/evaxfv55IJML48eNpamrq8DnMrN1tb731Fj/60Y9Yt24dQ4YMYeHChSmfxzlHRUUFTzzxRI/fi4iIiGRGfk4jnn85RPrHL0f6x6/7YMGCBTz55JMsW7aM6667jgMHDjBixAgikQjV1dXs2rWrw8d/5jOfYenSpQC89tprbN68GYCDBw8ycOBABg8ezHvvvcfKlStbH3PWWWdx6NAhAGbOnMmf/vQnduzYAcDRo0f585//7Mt7ExERkWDkZ2Zr0lXxqcM3XooHWj5ktQCmTp3KoUOHGD16NCNHjuQrX/kK11xzDWVlZUyfPp1JkyZ1+Pivfe1r3HLLLRQXFzN9+nTKy8sBKCkpYcaMGUydOpXzzjuvdZoQYNGiRcydO5eRI0dSXV3NY489xpe+9CWOHz8OwPe//30uvPBCX96fiIiI+M86KujOhrKyMte291R9fT2TJ0/O0ohyk/aZSG5oaGigqqqKmpoaotEosViMsWPHZntYIuKRma13zpV1tE1+TiOKiOSAhoYGSkpKeGxVHTsGl/LYqjpKSkpoaGjI9tBExEcKtkREsqSqqormEZMZcvUdnH3xNQy5+g6aR0ymqqoq20MTER8p2BIRyZKamhoiY6fRK1IEQK9IEZGx01i7dm2WRyYiflKwJSKSJdFolOaGVzndHG/1crq5ieaGV1tPnhGR/JCfZyOKiOSAWCzG0qUl7F+xmMjYaTQ3vErk/XpisSezPTQR8ZEyWyIiWTJ27Fg2bdrEwopSJh7cwMKKUjZt2qSzEUXyjDJbHn300Uf8+te/5m/+5m+6/NgHHniARYsWMWDAgABGJiK5bOzYsTz44IPZHoaIBEiZLY8++ugjHn744W499oEHHuDo0aM+j0hERERygTJbHt1555288cYbTJ8+nYqKCkaMGMFTTz3F8ePH+fznP8+9997LkSNHuP7662lsbOTUqVP8/d//Pe+99x579uxhzpw5DBs2jOrq6my/FREREcmgvA22qndXs2bPGmaNmsWccXN6/Hz3338/r732Ghs3buSFF15g2bJlrF27Fucc8+bNY/Xq1ezdu5dRo0axYsUKAA4cOMDgwYNZvHgx1dXVDBs2rMfjEBERkdySl9OI1buria2O8cS2J4itjlG9299s0gsvvMALL7zAjBkzKC0tZevWrWzfvp1p06bx4osv8u1vf5uXX36ZwYMH+/q6IiIiknvyMrO1Zs8amk7F+9Y0nWpizZ41vmS3WjjnuOuuu/jrv/7rdvetX7+e5557jrvuuovPfe5z3H333b69roiIiOSevMxszRo1i6Le8Y7MRb2LmDVqVo+f86yzzuLQoUMAXHHFFTz66KMcPnwYgLfffpv333+fPXv2MGDAAG688Ua+9a1vUVdX1+6xIiIi0nUNDQ1UVlZSXl5OZWVlTq0hmpeZrTnj5lD1mSpfa7aGDh3KpZdeykUXXcTcuXP58pe/zKxZ8SBu0KBBLFmyhB07dvB3f/d39OrVi0gkwk9/+lMAFi1axNy5cxk5cqQK5EVERLqoZdH2y0Ye5aZxUP3SZkqWLs2ZvnTmnMv2GM5QVlbmamtrz7itvr6eyZMnZ2lEuUn7TERE8kVlZSVvv/QzHp/fl4F9jSMnHDc9e4LRl9+a9T51ZrbeOVfW0TZ5OY0oIiIi+aOmpoY542BgXwPi/88ZR84s2q5gS0REREItGo1SvRuOnIjPxh054ajeTc4s2p6XNVsiIiKSP2KxGCVLl3LTs0eZMw6qd8Mf3hnAplgs20PzJGcyW2GrLQsz7SsREcknLYu2j778VpZ8WMLoy2/NmeJ4yJHMVlFREfv27WPo0KGYWbaHE2rOOfbt20dRUVG2hyIiIuKbXF603VOwZWZXAv8E9AZ+7py7v839/YBfARcD+4AbnHM7zSwC/BwoTbzWr5xz/9DVQY4ZM4bGxkb27t3b1YcWpKKiIsaMGZPtYYiIiAgegi0z6w08BFQAjcA6M1vunNuStNlXgf3OuQvMbAHwA+AG4ItAP+fcNDMbAGwxsyecczu7MshIJMKECRO68hARERGRUPBSs1UO7HDOvemcOwE8Ccxvs8184JeJy8uAz1p8vs8BA82sD9AfOAEc9GXkIiIiIjnAS7A1Gkjuid+YuC3lNs65k8ABYCjxwOsI8A6wG/iRc+7DHo5ZREREJGd4CbZSVaS3Pd0t3TblwClgFDAB+KaZndfuBcwWmVmtmdWqLktEREQ6kmvrJHoJthqB5HMrxwB70m2TmDIcDHwIfBn4N+dcs3PufeBPQLuW9s65R5xzZc65suHDh3f9XYiIiEhBaFkn8e2XfsZNQzfz9ks/o6SkJNQBl5dgax0w0cwmmFlfYAGwvM02y4GbE5evA15y8WZPu4HLLW4gMBPY6s/QRUREpNBUVVVx2cijPD6/L5XRfjw+vy+XjTxKVVVVtoeWVqfBVqIG6zbgeaAeeMo597qZfc/M5iU2+wUw1Mx2AHcAdyZufwgYBLxGPGj73865zT6/BxERESkQubhOoqc+W86554Dn2tx2d9LlJuJtHto+7nCq20VERES6IxqNUv3SZv5qhmNgX/t4ncTLy2loaKCqqoqamhqi0SixWCwUXeYtbEu7lJWVudra2mwPQ0REREKopWbrspFnrpO4cuVK5s6dizvf0X9Sf45tPYa9YYEv62Nm651z7erRk+XM2ogiIiIi6dZJXLJkCe58x6hbRzH0L4cy6tZRuPNdKGq5cmJtRBERESksHU0Jplonsaamhv6T+tOrXzyP1KtfL/pP6h+KWi5ltkRERCRUutPeIRqNcmzrMU4fPw3A6eOnObb1GOXl5Zkadlqq2RIREZFQqays5O2Xfsbj8/u2FsHf9OwJRl9+a7uMVouWAE01WyIiIiKd6E57h5Zarhtn3sjYP4/lxpk3Bh5oeaWaLREREQmVjto7dCRVLVcYaBpRREREQiVde4ewZKqSaRpRREREck669g5hC7S80jSiiIiIhE5YpwS7Q5ktERERkQAp2BIREZGc1dDQQGVlJeXl5VRWVnbYiytbFGyJiIhITupO89NsULAlIiIiOamqqorLRh7l8fl9qYz24/H5fbls5NFQrIeYTMGWiIiI5KTuND/NBgVbIiIikpOi0SjVu+HIiXjP0NbmpyFYDzGZmpqKiIhITgpD81M1NRURyaBcOCtKJJ/kSvNTZbZERHzQ8hd284jJRMZOo7nhVSLv14fyg19E/KPMlohIhlRVVdE8YjJDrr6Dsy++hiFX30HziMmhOytKRDJPwZaIiA9qamqIjJ1Gr0gRAL0iRUTGTgvdWVEiknkKtkREfBCNRmlueJXTzU0AnG5uornh1dCdFSUimaeaLRERH6hmS6QwqWZLRCRDWs6KWlhRysSDG1hYUapAS0QA6JPtAYiIdFdDQwNVVVXU1NQQjUaJxWJZDW7Gjh3Lgw8+mLXXF5FwUmZLRHJSy7TdY6vq2DG4lMdW1YVyAVoREQVbIpKT1GpBRHKFgi0RyUlqtSCSHwph5QUFWyKSk9RqQST3tZQDvP3Sz7hp6GbefulneVkOoGBLRDLGz79gY7EYkffr2b9iMQfX/479KxYTeb+eWCzm44hFJEhVVVVcNvIoj8/vS2W0H4/P78vsEYcoKyvLqyyXgi0RyQi/C9rVakEk3Lz8cVVTU8OccTCwrwHx/2+dEeG/lx7IqyyXgi0RyQg/CtrbfngDPPjgg9TU1PDggw8WfKBVCLUvkhu8/nEVjUap3g1HTsQbrDc1O/7y/D6tWa7LRh7Ni5NeFGyJSEb0tKBdrR46pv0jYeL1j6tYLMYf3hnATc+e4MGa47z41kmK+nyc5Zozjrw46UXBlohkRHcL2luyNWVlZRwf9im1ekhDrTAkTLz+cdVSDjD68lv5ft1gfr7hZGuW68gJR/Vu8uKkF3WQF5GMiMViLF1aEi9kT1o7MBZ7Mu1jzlhvsPjzDBz8CbV6SCP+5Vaq/SOhEI1GqV9Vx+niCnpFij7+46qifeDUsvJCLBajpKSEm549ypxxUL0b/vDOADblwUkvymyJSEZ0p6C9bbamaPx03MkTQGZbPeRCLZRaYUiYdOds4eQs15IPSxh9+a15c9KLOec638jsSuCfgN7Az51z97e5vx/wK+BiYB9wg3NuZ+K+YuBfgLOB08AlzrmmdK9VVlbmamtru/VmRCS/lJeXs2NwKWdffE3rbUd3rOXkgfdaM2NBfxifkV1LysiF7UsgV8YphaNl7dK1a9dSXl6e9bVLg2Jm651zZR1t02lmy8x6Aw8Bc4EpwJfMbEqbzb4K7HfOXQD8GPhB4rF9gCXAf3XOTQUuA5q7+D5EJIf1JCuUKltzZPPzRDY/nbFWD7lSC6VWGBI2LdODOlvYQ2bLzGYB9zjnrkhcvwvAOfcPSds8n9hmTSLAehcYTjxA+7Jz7kavA1JmSyR/9DTbEoZsTars2sH1v2PiwQ3U1NRkZAwiEl6+ZLaA0UDyn6KNidtSbuOcOwkcAIYCFwLOzJ43szozy/0qNxHxrKdZoTBka1QLJSI95eVsREtxW9t0WLpt+gCfBi4BjgK/T0SAvz/jwWaLgEUA48aN8zAkEckFfpwh1zIVEbSW+pKamhqi0WhrfUl3zqIUEUnmJbPVCCT/GTkG2JNum8Q04mDgw8Ttf3TOfeCcOwo8B5S2fQHn3CPOuTLnXNnw4cO7/i5EJJRyJSvUUUPQMGTXRPJJLpzd6zcvNVt9gD8DnwXeBtYRr8N6PWmbrwPTnHP/1cwWAP/JOXe9mQ0Bfk88u3UC+Dfgx865FeleTzVbIvkjDDVXXlRWVvLYqjqGXH1Ha0+g/SsWs7CiNCNZNZFC0fKZcNnINr20QvaZ0BW+1GwlarBuA54H6oGnnHOvm9n3zGxeYrNfAEPNbAdwB3Bn4rH7gcXEA7SNQF1HgZaI5JdcyQr1dCkhEfGmqqqKy0Ye5fH5ffNu/cOOeOog75x7jvgUYPJtdyddbgK+mOaxS4i3fxCRApSpmqueSNXt+viujew8sJPKysq87Q8kkmk1NTXcNC6+7iF8vP7hkjz/w0Yd5EUk69rWcNTU1GS0pqNtt+sPllfhnKO5+PNa0FnER9FolOrd5OX6hx3x1EE+k1SzJVJY2td1bebIn19h4IUziYwtDrTOK/kMxClT4r2aV65cyeHB5zH0mm+pfkvEZ6rZEhHJgva9uL7J2Z/+Svz/ADu2tz0D8bevbGf58uWMHDmSfueW5Ez9ViGe2SXh1dnxmM/rH3bEU82WiEhQUvXi6n9BNPBgJznI6xUp4nRxBftXLOb06UPxdhVJ9VvNDa9SXhG+aY4zs4Kl1K+qY+nSkoL48pLw8Xo85kIdp9+U2RKRrErVi+vYjprAe3OlOwOxV69eZ9Rv7V+xONHENHwLYOTKuo1SGHQ8pqdgS0Syqm1x+v4V/8jB/7c0/n+AwU66hquzZ8/OiXYV0HnLCk0xSiaphUp6CrZEJKva9+K6mDVr1rCw4uJAg532Qd7HQV3LNEdNTQ0PPvhgKAMt6LhDf0dd8UWCkCsrRmSDzkYUEV+lW2MwjFrGunbtWsrLy0M91lQ66tBfVVWlrviSUbmyYoTfvJyNqGBLRHxTqB+22ZQuYCwvL2fH4FLOvvia1m0Prv8dEw9uoKamJosjlnyW63/AdIeXYEtnI4qIb9Kd4VdVVRV4NiWXMmp+SndmV6qu+F09q7JQ96l0XyGeaeiFMlsi4ptsZVOylVELczDS032iLKWIN2pqKiIZla0C2Wycch72AvSeLgKu0/jFC53x6o0yWyLim2xlQ9Jl1E6sWcItt9wSSMapsrIyrwvQVfMlnVH2M06ZLRHJqJ5mU7orVUbt9PEj9J11Y2AZp2z2FMpENkGn8UtnlP30TpktEemxbNcutf0L+/TxI5x9ybWBZpyyldnKVDYh6NfJ9jEjPafsZ5wyWyISuDDULiVn1E6sWUKvfgMD76reUVPU7vIytkxlE9pmKb8wcyLz5s3jC1/4Qo+zaWE4ZqTnlP30TpktEemRsNUudTSeWCzma7bGa08hL1kcr5mkbGQT/M5yhe2Yke5RzVacMlsiEriwrYfWUcbJ76yQl2V9vGZxvI4tG9kEv/db2I4Z6Z5s1WjmIgVbItIjYZtK6OgLIBtf8l4DFa9jC2L6sjN+77ewHTPSfbmyjmi2KdgSkR7Jxpd/Z9J9AWTjS95roOJ1bNnIJvi938J4zIgESTVbItJjubIeWjZqTLzWJ4W5/qX92DbTvLOOSZMmMXv27G79vHPlmMlH2TgTNJ/PPvVSs4VzLlT/Lr74Yici+W/37t3utttuc5dccom77bbb3O7duzP6uuXl5Rl53d27d7shQ4a4QZ/6D27IX/51/P8hQ1K+bqbH1hUtYyspKXH9+vVzgz41q9P309Xn9utYyNSxla1juCfaH4+zXL9+/VxJSYnn99DV992V34FcBNS6TmIbZbZEJOPCnMUJQj5lcfw+k9DvY0F9yDqW6ud3cN0z9Oo30NN76M77zvezT3U2oogEpif9qjLZeToMa7eFoYjYr/3gd7G838dCpo6tIF8nyD5kqX5+g6PXeX4P3XnfOvtUwZaIdENPvwwy9eGr5plxfu4Hv4vl/T4WMnVsBfk6QQZybX9+7tRJrHcfwNt76M771tmnCrZEpBt6+mWQqQ/fIL60wpAp6yo/94PfZxKmOhaO79rIzp07u7V/M3VsBfk6QQZybX9+B2qWdek9dOd96+xTnY0oIt3Q0y7mmaqr8bvberZqzXpav9OV/eC1271fNWht92nTzg0AFI2f0a39G8SZk95ex79jIegap5af38svv8zWrVuJjC8lMrY4sJqt5NfMh7rFtlSzJSKB6Olf9ZnqFeV39iGTtWYt/JgC9LofvL6WnzVoycdCZPPTmPVi2LxYt/dv8vON2/sKzTvriIwvZffwmb5OIwd5DAedCWr5+W3cuJHt27ezsOJiz++hu+87DHWL2aTMloh0Wa6cTej3OLOxLqEfWY6O9gPQmslqamrirRODGHL1N7Ny1pjf+zeXz4ILWyYon/tk9ZQyWyISiFxZE83vcYa5A31H0u0H4IxM1o5j/YmMLc7aWWNhL77PpDBlgnSiSc/1yfYARCQ3tXwZhJ2f44zFYixdWhKf1knKEMViT/ry/KlEo1HqV9VxuriiNTvT3PAq5RVdC0BS7YfKysrWadFekSIGFVdwcN0znG5u6tFrdVd392+6rItf+86LfM78JE+f94oUcbq4gv0rFlNVVZUTnwFhoGlEEZEuyPT0TpBTtumm7Zp2buh2gXpPdXX/djZFmg8NTrMtG9PnuUTTiCIiPsv09E6QU7app+02c0H/Y1mbHu7q/u3opIVMTXdn48SJruhpuxL1yeo5ZbZEREIoyGmpludevXo127Zt69Kp/2EThqxLGMaQjh9Zt3zP3PWUMlsiIjkoyILk5OduGDGLyPhSmnfWMW7vK6E90aEjYci6BD2G5MzUwoULWbhwoecslR9Zt1w5ISbMlNkSEQmZIFsWZLIdQiaKxsOQdQlyDD1t+hrmrFu+8C2zZWZXmtk2M9thZnemuL+fmf0mcX+NmY1vc/84MztsZt/qyhsQESlEQbYsyOV1KVPVHmU665LpMbTNTA2bfyfD59/pOUsVhsyfeGj9YGa9gYeACqARWGdmy51zW5I2+yqw3zl3gZktAH4A3JB0/4+Blf4NW0QyLZ9PbQ+bIFsWZKodgt/tAs7M8JRSv6qOpUtLWoOaTLQgyMYY4sFx6cfBcZ++rfd5CZSz0a5E2vOS2SoHdjjn3nTOnQCeBOa32WY+8MvE5WXAZ83MAMzsWuBN4HV/hiwimaamhv7xcmZYkMu1ZGpRYL8zaH7UHvX0rLxsnHXYLjN18gTu5In4ZQ9ZKtVbhYOXYGs0kHxENiZuS7mNc+4kcAAYamYDgW8D9/Z8qCKSLWE/tT1XdGXtwaC+IHN1XcqeBm9+/MHQ3TF4DfJSbdc2OP7g2fvZ++z9XQqUw9SNvlB5CbYsxW1tq+rTbXMv8GPn3OEOX8BskZnVmlnt3r17PQxJRDIpl5c9CZOuBK1BfkFm4ss3VQat97uvc+jQoW5llnoavPnxB0N3xuA1yEu3HXBGcHz9pZO5/tLJylLlmE7PRjSzWcA9zrkrEtfvAnDO/UPSNs8ntlljZn2Ad4HhwGqg5Sg4BzgN3O2c+1/pXk9nI4qETy4v6BsmhXZmWHI3+MmTJ/PMM89w6pNTu3XGXk/P+Eu37yObn+bcc8/1VIfYnTGk+t3Z97sfMujAW1x//fWtr+n375hqLDPHr7MR1wETzWyCmfUFFgDL22yzHLg5cfk64CUXN9s5N945Nx54APifHQVaIhJOmarzyXeFdmZYcgbtrLPO4tQnp3Y7s9TT6c9U+75p5wYODz7P87Rid8aQKis8sPgKmos/f8Zr+pk9Vo1l+Hjqs2VmVxEPlnoDjzrn7jOz7wG1zrnlZlYEPA7MAD4EFjjn3mzzHPcAh51zP+rotZTZEgmnTK8JmI/C0BMqCF6yKNnO6rXd98d3bcQ5x7B5sUCzte0yVidPYID16XvGawK+ZbaUiVqZ7dEAABO2SURBVM4s3/psOeeec85d6Jw73zl3X+K2u51zyxOXm5xzX3TOXeCcK28baCW2uaezQEtEsi9dMa+KbHsuH88M85pF8Tur19UzC9vu+0EH3qJo/IxA6hCTx3bo0CF6v/t6a1a4aedGLNG+Ifk1/cwe+5Ul6+nZm/IxdZAXkVb5mnmR4HjNovh5bPnxXEFlf1KNrfe7r3PttdeycuVKDg8+j6HXfCtl/daNN97IkiVLepw99uO96bPAOy+ZLQVbItJK0w/SVV2ZHvRrKjrMwURHY4vFYj1aeser7r635OngpqYm3joxiCFXf1OfBZ3QQtQi0iVq8SBd1ZXpQb+mov04ToOa0u1obMmvGdn8NGa9GDYv5nvvuu68t7bTwTuO9ScytlifBT7pdLkeESkcmVrKRfJH0MvBpCq+9+s4TV5iJ/l1pkyZAsCWLVu63Dahs7ElB5w7BpcEFsx0dfmgtssrDSqu4OC6Zzjd3KTPAh9oGlFEWr9oVq9ezbZt24iMLyUytlh1GuJJUGeqppsOW7lyJXPnzvVtCrDt65w5vbeZ5p11TJo0idmzZ/vWi8uvKXu/gsR008FNOzf4Ps2Zb1SzJSKdav/l8PGXy/Tp04HufXiL9FRn9U9+BXidtWc4uO4ZevUb2OXap47G5kfNWMdBYteeL/W+/kcm9D1M//791e6lAwq2RKRT6b7QvjBzIsuXL9fZSNIlfnYuz1RvrlSvk8ydOon17uN7kXhPM4Jee3h5GavOPuw+FciLSKfSFfSuXLlSi09Ll/jduTxTHffbvc7JE7iTJ4CPAy0Irq6quycMtPvd7dM3ZQ8vr2PJtx5wYaICeZECl66gNwI6M1G6pG2R9eniCvavWExVVVW3MkFBF9+ne53k6bjTx49w9iXXhrJIvN3vbpvMVlfH2tWievFO04giBS7d9MG8efP47Svb1XOrmwpxIeAgpv0ytUxU20WzATZu3MjWrVtDe8KInzVb0n2q2RIRT1J9oQGq4eimQq1/ycemuGFfEzRVkFhfXx/KseYrBVsi0iNh/6IJq3wMOrwo1CBTCpuXYEs1WyIFyOsUl2o4uideuFxacPVuLUXWrQF6RTmx2JMKtKTgKdgSKTBnZh9KqV9Vx9KlJco++KiQO/ErQBdpT9OIIgWmUKe4MknTaSKFQ322RKQdLTYdPPUsEpFkmkYUKTCFPMWVSZpOE5EWmkYUKTCa4hIR8Y+mEUWkHU1xiYhklqYRRQqQprhERDJHmS0RERGRACnYEhEREQmQgi0RERGRACnYEhEREQmQgi2RAtHQ0EBlZSXl5eVUVlbS0NCQ7SGJiBQEBVsiBaClt9Zjq+rYMbiUx1bVUVJSooBLRCQDFGyJFICqqiqaR0xmyNV3cPbF1zDk6jtoHjGZqqqqbA9NRCTvKdgSKQBaD1FEJHsUbIkUgGg0SnPDq5xubgL4eD3Ecq2HKCISNK2NKFIAtB6iiEgwtDaiiABaD1FEJJu0NqJIgdB6iCIi2aHMloiIiEiAFGyJiIiIBEjBloiIiEiAFGyJ5BktyyMiEi6egi0zu9LMtpnZDjO7M8X9/czsN4n7a8xsfOL2CjNbb2avJv6/3N/hi0gyLcsjIhI+nQZbZtYbeAiYC0wBvmRmU9ps9lVgv3PuAuDHwA8St38AXOOcmwbcDDzu18BFpD0tyyMiEj5eMlvlwA7n3JvOuRPAk8D8NtvMB36ZuLwM+KyZmXNug3NuT+L214EiM+vnx8BFpD0tyyMiEj5egq3RQPIcRGPitpTbOOdOAgeAoW22+QKwwTl3vHtDFZHOaFkeEZHw8dLU1FLc1naNnw63MbOpxKcWP5fyBcwWAYsAxo0b52FIIpJKLBZj6dIS9q9YfMayPLHYk9kemohIwfKS2WoEktf0GAPsSbeNmfUBBgMfJq6PAZ4G/rNz7o1UL+Cce8Q5V+acKxs+fHjX3oGItNKyPCIi4eMls7UOmGhmE4C3gQXAl9tss5x4Afwa4DrgJeecM7NzgBXAXc65P/k3bBFJR8vyiIiES6eZrUQN1m3A80A98JRz7nUz+56ZzUts9gtgqJntAO4AWtpD3AZcAPy9mW1M/Bvh+7sQERERCSlzrm35VXaVlZW52trabA9DREREpFNmtt45V9bRNuogL5LH1E1eRCT7FGyJ5Cl1kxcRCQcFWyI5Ll32St3kRUTCwcvZiCISUi3Zq+YRk4mMLaV+VR1Ll5awadOmRDf5UnWTFxHJMmW2RHJYR9krdZMXEQkHZbZEclhH2atly5apm7yISAgosyWSwzrKXqmbvIhIOKjPlkgOamhooKqqitWrV7Nt2zYi40uJjC1uzV4pqBIRyQwvfbY0jSiSY84sip9F5PhAmnfWcX7RUWZXzCYWe1KBlohIiCjYEskxyUXxvSJFnC6uYP+KxcyeXao1EUVEQkg1WyI5Jl4UP00tHUREcoSCLZEco5YOIiK5RQXyIjnmzJqtaSqKFxHJIi1ELZKH1NJBRCS3qEBeJEe0tHuoqakhGo0Si8UUYImI5ABltkRyQMvU4WOr6tgxuJTHVtVRUlLSuui0iIiEl4ItkRzQ0RqIIiISbppGFAmp5GnDXbt2ESn+vNo9iIjkIAVbIiF05hmHpRwf3Au3cwODiivijUxb2j1UqN2DiEjYKdgSCaFUXeI/WF7Fvt/9iH7nlrS2e4jFnsz2UEVEpBOq2RIJoVRd4ovGz2DQgTfV7kFEJMcosyUSQtFolPpVdZxuM234leuv1/qHIiI5Rh3kRUJIXeJFRHKDOsiL5Ch1iRcRyR+aRhQJEXWJFxHJP8psiYSEusSLiOQnBVsiIaEu8SIi+UnBlkhIpGr3oC7xIiK5T8GWSEhEo1GaG17ldHMTwMdd4svVJV5EJJepQF4ky1qK4levXk3zzm3sX/GPRMYWq0u8iEieULAlkkVn9tOaReT4QJp31nF+0VFmV8wmFntSZyOKiOQ4BVsiWZRqDcT9KxYze3apOsWLiOQJ1WyJZJGK4kVE8p+CLZEMaGhooLKykvLyciorK6mpqaGyspJdu3ZxfNcmFcWLiOQxrY0oEpDkwvdt27YRGV+aKHzfzJE/v8LAC2cSGVtM084NABSNn6E1EEVEcoyXtRFVsyXSQ8lL7EyZMgWADRs2JAVYsygaXMzZl1zbWpfVe8QzrdcHFVew73c/JLL5ab5y/fUqihcRyTOegi0zuxL4J6A38HPn3P1t7u8H/Aq4GNgH3OCc25m47y7gq8Ap4Hbn3PO+jV7Eo1QB0ZYtW9Jejkaj3HjjjSxZsqTTxzzzzDOc+uRUImNLee1PLVmqMwMsd+ok1jv+69YrUkT/C6Jn1Gn1O3c64w86FcWLiOShToMtM+sNPARUAI3AOjNb7pzbkrTZV4H9zrkLzGwB8APgBjObAiwApgKjgBfN7ELn3Cm/30hXtF3s1+uXasvlIB+j8fg/nvQBUfrL9avW89BDDyWm+tJv9/q/b8QN/xTDEmcTDiq5AgOsT98zAizr3af1+unmJo7tqCEyZGQ809VSp1WhOi0RkXzUac2Wmc0C7nHOXZG4fheAc+4fkrZ5PrHNGjPrA7wLDAfuTN42ebt0rxd0zdaZfY2mdVg/k+5ykI/RePwfz/FdG3HOMWxeLB7cnDzRGhClvdzcxMF1H0/1dbRd067NDLggdaCUHGAdXPcMvfoNbPceVKclIpK7vNRseQm2rgOudM79l8T1m4Coc+62pG1eS2zTmLj+BhAF7gFecc4tSdz+C2Clc25ZutcLOtiqrKzksVV1H/c18vql2p0v4m5+eWs8/o+no4AonePvv0W/ERM63e7ojrUUnVvcfjxtAqzmnXVMmjSJ2bNnt2bn1q5dS3l5ObFYTIGWiEgO8qtA3lLc1jZCS7eNl8diZouARQDjxo3zMKTui/c1Kj2jXuaM+pk+fVu3TXs5yMdoPIE8N8TbKnQlQDtjqq+D7Q5v+jeObH6BfueWtMvCtQZYFbOJxX5zRkAVjUYREZH85yXYagSS/+QeA+xJs01jYhpxMPChx8finHsEeATimS2vg++OaDRK/ao6ThdXtGa2PH2pdueLuJtf3hqP/+NJFxB1Nl156v03OpnWfJW+e7dx7bXXUl+/gcmXTgagvn4D5RXl7QIsEREpPF6CrXXARDObALxNvOD9y222WQ7cDKwBrgNecs45M1sO/NrMFhMvkJ8IZLU1diwWY+nSEvavWHxGzVZnX6rd+SLu3pe3xuP/eNIHROkul1eUc+Mvf9w61dfRdmrVICIiHfHU1NTMrgIeIN764VHn3H1m9j2g1jm33MyKgMeBGcQzWgucc28mHvsd4K+Ak8B/c86t7Oi1MtHUtOVsxJZ6meT6mcmTW75I69NeDvIxGk8w41FNlIiIBMGXAvlMUwd5ERERyRVegi2tjSgiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgFSsCUiIiISIAVbIiIiIgEy51y2x3AGM9sL7MrQyw0DPsjQa4WZ9oP2QQvtB+2DFtoP2gcttB863gfnOueGd/Tg0AVbmWRmtc65smyPI9u0H7QPWmg/aB+00H7QPmih/dDzfaBpRBEREZEAKdgSERERCVChB1uPZHsAIaH9oH3QQvtB+6CF9oP2QQvthx7ug4Ku2RIREREJWqFntkREREQCVbDBlpldaWbbzGyHmd2Z7fFkgpmNNbNqM6s3s9fN7BuJ2//CzFaZ2fbE/0OyPdagmVlvM9tgZv83cX2CmdUk9sFvzKxvtscYNDM7x8yWmdnWxDExq0CPhb9N/D68ZmZPmFlRvh8PZvaomb1vZq8l3ZbyZ29xP0l8Vm42s9LsjdxfafbDDxO/E5vN7GkzOyfpvrsS+2GbmV2RnVH7K9U+SLrvW2bmzGxY4npBHQuJ2ysTP+/Xzawq6fYuHQsFGWyZWW/gIWAuMAX4kplNye6oMuIk8E3n3GRgJvD1xPu+E/i9c24i8PvE9Xz3DaA+6foPgB8n9sF+4KtZGVVm/RPwb865SUAJ8f1RUMeCmY0GbgfKnHMXAb2BBeT/8fAYcGWb29L97OcCExP/FgE/zdAYM+Ex2u+HVcBFzrli4M/AXQCJz8oFwNTEYx5OfJfkusdovw8ws7FABbA76eaCOhbMbA4wHyh2zk0FfpS4vcvHQkEGW0A5sMM596Zz7gTwJPEdmtecc+845+oSlw8R/3IdTfy9/zKx2S+Ba7MzwswwszHA1cDPE9cNuBxYltikEPbB2cBngF8AOOdOOOc+osCOhYQ+QH8z6wMMAN4hz48H59xq4MM2N6f72c8HfuXiXgHOMbORmRlpsFLtB+fcC865k4mrrwBjEpfnA0865447594CdhD/LslpaY4FgB8DMSC5sLugjgXga8D9zrnjiW3eT9ze5WOhUIOt0UBD0vXGxG0Fw8zGAzOAGuATzrl3IB6QASOyN7KMeID4h8jpxPWhwEdJH7CFcDycB+wF/ndiOvXnZjaQAjsWnHNvE/9rdTfxIOsAsJ7COx4g/c++kD8v/wpYmbhcMPvBzOYBbzvnNrW5q2D2QcKFwOxEScEfzeySxO1d3g+FGmxZitsK5rRMMxsE/Bb4b865g9keTyaZ2X8E3nfOrU++OcWm+X489AFKgZ8652YAR8jzKcNUEnVJ84EJwChgIPGpkrby/XjoSCH+fmBm3yFeerG05aYUm+XdfjCzAcB3gLtT3Z3itrzbB0n6AEOIl938HfBUYiaky/uhUIOtRmBs0vUxwJ4sjSWjzCxCPNBa6pz718TN77WkghP/v5/u8XngUmCeme0kPn18OfFM1zmJaSQojOOhEWh0ztUkri8jHnwV0rEA8JfAW865vc65ZuBfgf9A4R0PkP5nX3Cfl2Z2M/Afga+4j/sjFcp+OJ/4Hx+bEp+TY4A6M/skhbMPWjQC/5qYNl1LfDZkGN3YD4UabK0DJibOOOpLvNBteZbHFLhERP4LoN45tzjpruXAzYnLNwPPZnpsmeKcu8s5N8Y5N574z/0l59xXgGrgusRmeb0PAJxz7wINZvapxE2fBbZQQMdCwm5gppkNSPx+tOyHgjoeEtL97JcD/zlxJtpM4EDLdGM+MrMrgW8D85xzR5PuWg4sMLN+ZjaBeJH42myMMUjOuVedcyOcc+MTn5ONQGniM6OgjgXgGeJ/kGNmFwJ9iS9G3fVjwTlXkP+Aq4ifafIG8J1sjydD7/nTxFOdm4GNiX9XEa9Z+j2wPfH/X2R7rBnaH5cB/zdx+bzEL8sO4P8A/bI9vgy8/+lAbeJ4eIZ4urzgjgXgXmAr8BrwONAv348H4AniNWrNxL9Mv5ruZ098yuShxGflq8TP3Mz6ewhwP+wgXo/T8hn5z0nbfyexH7YBc7M9/qD2QZv7dwLDCvRY6AssSXw21AGXd/dYUAd5ERERkQAV6jSiiIiISEYo2BIREREJkIItERERkQAp2BIREREJkIItERERkQAp2BIREREJkIItERERkQAp2BIREREJ0P8HewEy9NemJ6EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_scaled_tmp = np.concatenate((X_cnn_train,X_cnn_validate,X_cnn_test), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.scatter(range(len(X_scaled_tmp)), X_scaled_tmp[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=30,color='k')\n",
    "ax.scatter(range(len(X_cnn_train)), X_cnn_train[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='train')\n",
    "ax.scatter(range(len(X_cnn_train), len(X_cnn_train)+len(X_cnn_validate)), X_cnn_validate[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='validate')\n",
    "ax.scatter(range(len(X_cnn_train)+len(X_cnn_validate), len(X)), X_cnn_test[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='test')\n",
    "plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is split with respect to time, the axis corresponding to time is collapsed such that the \n",
    "\"batch\" (keras notation) axis now contains all frames for all countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_3d = concatenate_4d_into_3d(scaled_splits)\n",
    "(X_cnn_train_model, y_cnn_train_model, X_cnn_validate_model,\n",
    " y_cnn_validate_model, X_cnn_test_model, y_cnn_test_model) = splits_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the naive baseline values for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations of hyperparameters with which to tune the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [500]\n",
    "# use a larger batch size for cross validation to speed it up\n",
    "batch_size_list = [1024]\n",
    "filter1_list = [16]\n",
    "filter2_list = [4]\n",
    "kernel1_size_list = [8]\n",
    "kernel2_size_list = [8]\n",
    "# need to keep the number of parameters down, used to be way too many parameters. \n",
    "# With these kernel sizes, the dimension of the input to first dense layer is 56\n",
    "first_dense_layer_output_dimension_list = [2, 4, 8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the parameter lists into combinations using itertools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations = list(itertools.product(epochs_list, batch_size_list, filter1_list,\n",
    "                                                filter2_list, kernel1_size_list, kernel2_size_list,\n",
    "                                                first_dense_layer_output_dimension_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to both save the results as well as ensure that the run of this notebook can be broken up into sessions, create a DataFrame which logs the scores and saves them to a .csv file. During the exploration of hyperparameters, this csv will be overwritten with the inclusion of any new scores calculated. This will allow for comparison and analysis of the hyper parameter space, as well as ensure that we do not have to fit the model 729 times per run of this notebook. The scores that are saved are the ones generated by the validation set predictions. Also included for comparison purposes are the scores resulting from the naive baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename = 'cnn_score_logging_all_but_dense_dimension_frozen.csv'\n",
    "if os.path.isfile(save_filename):\n",
    "    score_logging_df = pd.read_csv(save_filename, index_col=0)\n",
    "else:\n",
    "    score_logging_df = pd.DataFrame(np.array(parameter_combinations), \n",
    "                                    columns=['epochs','batch_size','filter_1','filter_2','kernel_1','kernel_2','first_dense_layer_output_dimension'])\n",
    "    score_logging_df.loc[:, 'mean_squared_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_squared_error'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network model \n",
    "<a id='model'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use static seeds to ensure that the cross-validation process is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_seeds = [0,1,2]\n",
    "one_seeds = [3,4,5]\n",
    "two_seeds = [6,7,8]\n",
    "three_seeds = [9,10,11]\n",
    "zero_seeds = [0]\n",
    "one_seeds = [3]\n",
    "two_seeds = [6]\n",
    "three_seeds = [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The convolution will be with respect to time, specifically the time steps within frames of time of predetermined length. The architecture of the CNN itself is two convolutional layers followed by two dense layers, ending with a ReLU activation layer. The key pieces of information to keep in mind when creating the neural network models are that I need to keep the parameter number small to account for the relatively small number of samples, to include only most important time dependent features and to make sure time ordering is respected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  elif not isinstance(value, collections.Sized):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#"
     ]
    }
   ],
   "source": [
    "for i, hyper_parameters in enumerate(parameter_combinations):\n",
    "    \n",
    "    mse_list = []\n",
    "    naive_mse_list = []\n",
    "    explained_variance_list = []\n",
    "    naive_explained_variance_list = []\n",
    "    mae_list = []\n",
    "    naive_mae_list = []\n",
    "    \n",
    "    (epochs, batch_size, filter1, \n",
    "     filter2, kernel1_size, kernel2_size,first_dense_layer_output_dimension) = hyper_parameters\n",
    "    \n",
    "    if score_logging_df.isna().loc[i,'mean_squared_error']:\n",
    "        for j in range(len(zero_seeds)):\n",
    "            cnn = Sequential()\n",
    "            cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                             padding='valid',\n",
    "                             input_shape=X_cnn_train.shape[2:],\n",
    "                             use_bias=False,\n",
    "                            kernel_initializer=RandomNormal(seed=zero_seeds[j])\n",
    "                            )\n",
    "                     )\n",
    "            cnn.add(Conv1D(filters=int(filter2), \n",
    "                             kernel_size=int(kernel2_size), \n",
    "                             padding='valid',\n",
    "                             use_bias=False,\n",
    "                            kernel_initializer=RandomNormal(seed=one_seeds[j])\n",
    "                            )\n",
    "                     )\n",
    "            cnn.add(Flatten())\n",
    "            cnn.add(Dense(first_dense_layer_output_dimension, \n",
    "                          kernel_initializer=RandomNormal(seed=two_seeds[j])\n",
    "                           )\n",
    "                     )\n",
    "            cnn.add(Dense(1, \n",
    "                            activation='relu',\n",
    "                          kernel_initializer=RandomNormal(seed=three_seeds[j])\n",
    "                               ))\n",
    "            cnn.compile(loss='mse', optimizer=Adam())\n",
    "            history = cnn.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "                      batch_size=batch_size, verbose=0)\n",
    "\n",
    "            y_true = y_cnn_validate_model.ravel()\n",
    "            y_predict = cnn.predict(X_cnn_validate_model).ravel()\n",
    "            y_naive = y_validate_naive.ravel()\n",
    "\n",
    "            # naive mse, r2, mae\n",
    "            naive_mse_list.append(mean_squared_error(y_true.ravel(), y_naive.ravel()))\n",
    "            naive_explained_variance_list.append(explained_variance_score(y_true.ravel(), y_naive.ravel()))\n",
    "            naive_mae_list.append(mean_absolute_error(y_true.ravel(), y_naive.ravel()))\n",
    "\n",
    "            # mse, r2, mae\n",
    "            mse_list.append(mean_squared_error(y_true.ravel(), y_predict))\n",
    "            explained_variance_list.append(explained_variance_score(y_true.ravel(), y_predict))\n",
    "            mae_list.append(mean_absolute_error(y_true.ravel(), y_predict))\n",
    "\n",
    "        score_logging_df.loc[i,'naive_mean_squared_error'] =  np.mean(naive_mse_list)\n",
    "        score_logging_df.loc[i,'mean_squared_error']  = np.mean(mse_list)\n",
    "        score_logging_df.loc[i,'naive_explained_variance']  = np.mean(naive_explained_variance_list)\n",
    "        score_logging_df.loc[i,'explained_variance']  = np.mean(explained_variance_list)\n",
    "        score_logging_df.loc[i,'naive_mean_absolute_error']  =np.mean(naive_mae_list)\n",
    "        score_logging_df.loc[i,'mean_absolute_error']  = np.mean(mae_list)\n",
    "        # every time a new score is calculated, overwrite the original file, to save space but also save progress scoring.\n",
    "        score_logging_df.to_csv(save_filename)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('#',end='')\n",
    "    if (i % 50 == 0) & (i>0):\n",
    "          print(' {} runs completed'.format(str(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the different results (mean squared error of predictions on validation set)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_logging_df.mean_squared_error.apply(lambda x : np.log10(x+1)).plot(linestyle='none', marker='.', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these \"best\" model parameters with random kernels instead of the seeded ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_parameters = score_logging_df.loc[score_logging_df.mean_squared_error.idxmin(),:].iloc[:7]\n",
    "best_model_parameters.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static seeds for the ultimate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel5 = RandomNormal(seed=5)\n",
    "kernel6 = RandomNormal(seed=6)\n",
    "kernel7 = RandomNormal(seed=7)\n",
    "kernel8 = RandomNormal(seed=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that parameters have been tuned, can use both the validation and training sets for training, and then predict on the final hold-out, testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_train = np.concatenate((X_cnn_train_model, X_cnn_validate_model),axis=0)\n",
    "y_final_train = np.concatenate((y_cnn_train_model, y_cnn_validate_model),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and preview the final model to ensure the parameter number isn't too large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_parameters= best_model_parameters.astype(int).values\n",
    "(epochs, batch_size, filter1, \n",
    " filter2, kernel1_size, kernel2_size,first_layer_output_dimension) = best_model_parameters\n",
    "best_cnn = Sequential()\n",
    "best_cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                 padding='valid',\n",
    "                 input_shape=X_cnn_train.shape[2:],\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel5,\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Conv1D(filters=int(filter2), \n",
    "                 kernel_size=int(kernel2_size), \n",
    "                 padding='valid',\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel6,\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Flatten())\n",
    "best_cnn.add(Dense(first_layer_output_dimension,\n",
    "                kernel_initializer=kernel7,\n",
    "               )\n",
    "         )\n",
    "best_cnn.add(Dense(1, \n",
    "                activation='relu',  \n",
    "                kernel_initializer=kernel8,\n",
    "                   ))\n",
    "best_cnn.compile(loss='mse', optimizer=Adam())\n",
    "best_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the final set of weights to see if anything unusual occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5),sharex=True)\n",
    "(ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "_ = ax1.hist(best_cnn.get_weights()[0].ravel())\n",
    "_ = ax2.hist(best_cnn.get_weights()[1].ravel())\n",
    "_ = ax3.hist(best_cnn.get_weights()[2].ravel())\n",
    "_ = ax4.hist(best_cnn.get_weights()[3].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_cnn.fit(X_final_train, y_final_train, epochs=epochs, validation_data=(X_cnn_test_model, y_cnn_test_model), \n",
    "          batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and conclusion\n",
    "<a id='conclusion'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we are not overtraining the final CNN model, let's look at the plots of validation loss and training loss as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'][-25:], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'][-25:], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overtraining would be signalled by the validation loss increasing while the training loss decreases; there does not seem to be substantial evidence for this so we can move onto the prediction results. As a final sanity check, predict the outcomes of the training data, to see whether or not the predictions perform better than the naive baseline; if not, then either something is wrong or the model is not worth pursuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_final_train.ravel()\n",
    "y_predict = best_cnn.predict(X_final_train).ravel()\n",
    "y_naive = np.concatenate((y_train_naive, y_validate_naive),axis=0)\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Naive baseline vs. predictions of training set',\n",
    "              figname='cnn_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the efficacy of the model by predicting the validation data. Note that this is different from the \"testing\" set, or hold-out set, which is used for the final check of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_test_model.ravel()\n",
    "y_predict = best_cnn.predict(X_cnn_test_model).ravel()\n",
    "y_naive = y_test_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Naive baseline vs. predictions of testing set',\n",
    "              figname='cnn_test_performance.jpg', model_params=best_model_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "I would like to test whether or not removing some of the less \"helpful\" data would train the CNN more accurately. I.e. take the subset of the data where all countries have at least a single presenting case; this can be sliced by the conditional statement \"data.days_since_first_case >= 1\". Alternatively, I coulud simply truncate all time series but a certain cutoff date. This would be more uniform and hopefully have the same effect. Because the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Ridge()\n",
    "_ = r.fit(score_logging_df.iloc[:,:7],score_logging_df.iloc[:,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of the mean squared error is larger at 50 epochs but so is the absolute minimum. This means that 50 epochs is more unreliable due to undertraining. Therefore recommend 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_logging_df.plot.scatter(x='epochs',y='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cnn_score_logging.csv', 'cnn_score_logging_25_epochs.csv','cnn_score_logging_dense_output_dimension.csv',\n",
    "'cnn_score_logging_dense_output_dimension_more_training.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('cnn_score_logging.csv',index_col=0).groupby('epochs').mean_squared_error.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('cnn_score_logging_dense_output_dimension_more_training.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltdat = pd.read_csv('cnn_score_logging_dense_output_dimension_more_training.csv',index_col=0)#.groupby('first_dense_layer_output_dimension').mean_squared_error.plot(legend=True)\n",
    "# sns.lineplot(x='epochs', y='mean_squared_error', hue='first_dense_layer_output_dimension', data=pltdat)\n",
    "sns.lineplot(x='epochs', y='mean_squared_error', hue='first_dense_layer_output_dimension', data=pltdat, legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though originally I thought that including too many parameters would negatively affect performance, the opposite appears to be true. Because of the large batch size it's not obvious how many epochs to train on. The higher the dimension is, the longer it would take to overtrain (at least that is my intuition). We can see when overtraining starts to occur by seeing when the validation loss (mean squared error) no longer uniformly decreases with respect to parameter dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "score_df = None\n",
    "for x in glob.glob('cnn_score_logging*'):\n",
    "    tmp = pd.read_csv(x, index_col=0)\n",
    "    if score_df is None:\n",
    "        score_df = tmp\n",
    "    else:\n",
    "        score_df = pd.concat((score_df,tmp),axis=0)\n",
    "score_df = score_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncolors = score_df.first_dense_layer_output_dimension.nunique()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cax = score_df.plot.scatter(x='epochs', y='mean_squared_error', \n",
    "                      c='first_dense_layer_output_dimension',\n",
    "                      figsize=(10,10), \n",
    "                      ax=ax,\n",
    "                      cmap=plt.cm.get_cmap('viridis', ncolors))\n",
    "loc,lab = plt.xticks()\n",
    "_ = ax.set_xticks([int(l) for l in loc[1:]])\n",
    "_ = ax.set_xticklabels([int(l) for l in loc[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores seem to be the best for 500 epochs, let's see the order of the first layer output dimension and take the one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
