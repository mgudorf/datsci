{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. This process is kept track of by encoding the dates which have interpolated values.\n",
    "There are two measures to do so. Essentially its one hot encoding for the categories ['extrapolated', 'interpolated', 'actual']. The other measure is to track the \"days since infection\" where 0 represents the first day with a recorded\n",
    "case of COVID within that country. I leave the more complex feature creation to the exploratory data analysis portion\n",
    "of this project.\n",
    "\n",
    "Some of the data is currently not used but may be incorporated later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "<!-- ## [The COVID tracking project testing data.](#source1)\n",
    "[https://covidtracking.com/api](https://covidtracking.com/api)\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "## [IHME hospital data](#ihme)\n",
    "[http://www.healthdata.org/covid/data-downloads](http://www.healthdata.org/covid/data-downloads)\n",
    "\n",
    "The IHME hospital data is one of the more unique datasets I've discovered with \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/)\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "## [Delphi-epidata (currently not used)**](#delphi) which contains \n",
    "       Facebook surveys, google surveys, doctor visits, google health trends, quidel test data\n",
    "[https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)\n",
    "\n",
    "I have not dove into this dataset too thoroughly but it contains information from facebook and google\n",
    "surveys regarding COVID as well as doctor visits; the doctor visit data attempts to make distinctions between\n",
    "those sick with the annual influenza and those with COVID.\n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "\n",
    "def column_or_index_string_reformat(df, columns=True, index=False, dt_formats=('%m/%d/%y', '%Y-%m-%d')):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    if columns:\n",
    "        reformatted_column_names = []\n",
    "        for c in df.columns:\n",
    "            # handle labels which can be cast to datetime objects\n",
    "            try:\n",
    "                reformatted_column_names.append(datetime.strftime(\n",
    "                    datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "            except ValueError:\n",
    "                reformatted_column_names.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c)\n",
    "                                                                .lower()).split()))\n",
    "        df.columns = reformatted_column_names        \n",
    "        \n",
    "    if index:\n",
    "        # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "        \n",
    "        \n",
    "        reformatted_country_names = []\n",
    "        for c in df.index.get_level_values(0):\n",
    "            reformatted_country_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "                                                        .split()).title())\n",
    "        \n",
    "        reformatted_dates = pd.to_datetime(df.index.get_level_values(1)).normalize()\n",
    "        restored_columns = df.index.names\n",
    "        df = df.reset_index()\n",
    "        df.loc[:, restored_columns[0]] = reformatted_country_names\n",
    "        df.loc[:, restored_columns[1]] = reformatted_dates\n",
    "        df = df.set_index(restored_columns).sort_index()\n",
    "        \n",
    "#     if index:\n",
    "#         # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "#         reformatted_index_names = []\n",
    "#         for c in df.index.get_level_values(0):\n",
    "#             # handle labels which can be cast to datetime objects\n",
    "#             try:\n",
    "#                 reformatted_index_names.append(datetime.strftime(\n",
    "#                     datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "#             except ValueError:\n",
    "#                 reformatted_index_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "#                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "#                                                         .split()).title())\n",
    "#         restored_column = df.index.names[0]\n",
    "#         df = df.reset_index(level=0)\n",
    "#         df.loc[:, restored_column] = reformatted_index_names\n",
    "#         df = df.set_index([restored_column, df.index]).sort_index()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def csse_daily_reports_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE daily report data from local machine. \n",
    "    \"\"\"\n",
    "    csv_different_formats_list = []\n",
    "    \n",
    "    # the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_daily_reports/*'):\n",
    "        if os.path.isdir(x):\n",
    "            df_list = []\n",
    "            for days in glob.glob(x+'/*'):\n",
    "                df = pd.read_csv(days)\n",
    "                df_list.append(df)\n",
    "            csv_different_formats_list.append(column_or_index_string_reformat(pd.concat(df_list, axis=0).reset_index(drop=True)))\n",
    "    \n",
    "    # concatenate the data\n",
    "    daily_reports_df = pd.concat(csv_different_formats_list).reset_index(drop=True)\n",
    "    # convert the date-like variable to datetime\n",
    "    daily_reports_df.loc[:, 'last_update'] = pd.to_datetime(daily_reports_df.last_update).dt.normalize()\n",
    "    # In the reporting there are duplicate values. Also, I'm aggregating by country because the other datasets\n",
    "    # are not nearly as detailed. Probably should flag this somehow. \n",
    "    daily_reports_df = daily_reports_df.drop_duplicates().groupby(['country_region','last_update']).sum()\n",
    "    # Reformat the location names and datetime index. Look at documentation above for details. \n",
    "    daily_reports_df = column_or_index_string_reformat(daily_reports_df, index=True, columns=True)\n",
    "    # name the indices and columns for later concatenation\n",
    "    daily_reports_df.index.names = ['location','date']\n",
    "    daily_reports_df.columns.names = ['csse_global_daily_reports']\n",
    "    return daily_reports_df\n",
    "    \n",
    "def csse_timeseries_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE time series data from local machine. \n",
    "    \"\"\"\n",
    "    global_df_list = []\n",
    "\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "        global_tmp = column_or_index_string_reformat(pd.read_csv(x))\n",
    "        # only include the actual time series info; this removes latitude and \n",
    "        # longitude as well as other useless data.\n",
    "        global_specific_indice_list = [1] + list(range(4, global_tmp.shape[1]))\n",
    "        global_tmp = global_tmp.iloc[:,global_specific_indice_list].groupby(by='country_region').sum()\n",
    "        # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "        time_series_name = '_'.join(x.split('.')[0].split('_')[-2:][::-1])\n",
    "        global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    # concatenate the data and name it to abide by my convention. \n",
    "    global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "    global_time_series_df.index.names = ['location','date']\n",
    "    global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "    global_time_series_df = column_or_index_string_reformat(global_time_series_df, index=True, columns=False)\n",
    "\n",
    "    # Repeat the steps above but for United States statewide data. \n",
    "    usa_df_list = []\n",
    "    for y in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "        usa_tmp = column_or_index_string_reformat(pd.read_csv(y))\n",
    "        try:\n",
    "            usa_tmp = usa_tmp.drop(columns='population')\n",
    "        except: \n",
    "            pass\n",
    "        usa_specific_indice_list = [6] + list(range(10, usa_tmp.shape[1]))\n",
    "        usa_tmp = usa_tmp.iloc[:,usa_specific_indice_list].groupby(\n",
    "            by='province_state').sum()\n",
    "        time_series_name = '_'.join(y.split('.')[0].split('_')[-2:][::-1])\n",
    "        usa_tmp.index.name = 'state'\n",
    "        usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "    usa_time_series_df.index.names = ['location','date']\n",
    "    usa_time_series_df.columns.names = ['csse_us_timeseries']\n",
    "    usa_time_series_df = column_or_index_string_reformat(usa_time_series_df, index=True, columns=False)\n",
    "    \n",
    "    return global_time_series_df, usa_time_series_df\n",
    "\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    df = df.reset_index(level=0)\n",
    "    df.loc[:,'location'] = df.loc[:,'location'].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    df = df.set_index(['location', df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    df_table.columns = df_table.columns.droplevel()\n",
    "    df_table.columns.names = ['']\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "def flag_nan_differences(df, df_altered, suffix):\n",
    "    # Use bitwise XOR to flag the values which have been changed from NaN to something else.\n",
    "    # values which get mapped true -> false are those that are changed. \n",
    "    flag_df = df.isna() ^ df_altered.isna()\n",
    "    z1 = tuple(flag_df.columns.get_level_values(0).tolist())\n",
    "    z2 = tuple((flag_df.columns.get_level_values(1) + suffix).tolist())\n",
    "    flag_df.columns = pd.MultiIndex.from_tuples(list(zip(z1,z2)),names=['dataset', 'features'])\n",
    "    return flag_df\n",
    "\n",
    "\n",
    "#----------------- Currently Unused ----------------------#\n",
    "\n",
    "def pull_delphi_data(data_source=['fb-survey', 'google-survey', 'ght', 'quidel', 'quidelneg', 'doctor-visits'], \n",
    "                     daterange=pd.date_range(start=\"20200101\",\n",
    "                                             end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d'),\n",
    "                     **kwargs):\n",
    "    \"\"\" Pull data from https://cmu-delphi.github.io/delphi-epidata/api/\n",
    "        https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for data in data_source:\n",
    "        signal_dict = {'fb-survey':'smoothed_cli',\n",
    "                       'google-survey':'smoothed_cli',\n",
    "                       'ght':'smoothed_search',\n",
    "                       'quidel':'smoothed_tests_per_device',\n",
    "                       'quidelneg':'smoothed_pct_negative',\n",
    "                       'doctor-visits':'smoothed_cli'}\n",
    "        \n",
    "        signal = signal_dict[data]\n",
    "        if data=='quidelneg':\n",
    "            #change the proxy for the quidel signal\n",
    "            data = 'quidel'\n",
    "        for days in daterange:\n",
    "            resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "            day_data = resp.json().get('epidata', None)\n",
    "            if day_data is None:\n",
    "                pass\n",
    "            else:\n",
    "                var_number += pd.json_normalize(day_data).size\n",
    "                print(pd.json_normalize(day_data).shape)    \n",
    "                \n",
    "                \n",
    "# date_range_2020 = pd.date_range(start=\"20200101\",end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d')\n",
    "# var_number = 0 \n",
    "# for days in date_range_2020:\n",
    "# #     days='20200302'\n",
    "#     resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "#     day_data = resp.json().get('epidata', None)\n",
    "#     if day_data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         var_number += pd.json_normalize(day_data).size\n",
    "#         print(pd.json_normalize(day_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function declared for this purpose, import and reform JHU CSSE data. Likewise, for\n",
    "the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_global_daily_reports_df = csse_daily_reports_reformat().loc[:, ['confirmed','active','deaths','recovered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csse_global_timeseries_df, csse_us_timeseries_df = csse_timeseries_reformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csse_global_timeseries</th>\n",
       "      <th>global_confirmed</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tanzania</th>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Azerbaijan</th>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czechia</th>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belize</th>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iceland</th>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>1689</td>\n",
       "      <td>8</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "csse_global_timeseries  global_confirmed  global_deaths  global_recovered\n",
       "location   date                                                          \n",
       "Tanzania   2020-03-03                  0              0                 0\n",
       "Azerbaijan 2020-03-13                 15              1                 3\n",
       "Czechia    2020-01-26                  0              0                 0\n",
       "Belize     2020-03-05                  0              0                 0\n",
       "Iceland    2020-04-11               1689              8               841"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csse_global_timeseries_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME hospital data\n",
    "<a id='ihme'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "[JHU CSSE](#csse) \n",
    "<font color='red'>\n",
    "### Has all USA states but only 32 countries which overlap with other data; stash this dataset for now. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_df = column_or_index_string_reformat(pd.read_csv(\n",
    "    './IHME_hospital_data/2020_04_12.02/Hospitalization_all_locs.csv').rename(columns={'location_name':'location'}))\n",
    "ihme_df.loc[:, 'date'] = pd.to_datetime(ihme_df.loc[:,'date']).dt.normalize()\n",
    "ihme_df = ihme_df.set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>allbed_mean</th>\n",
       "      <th>allbed_lower</th>\n",
       "      <th>allbed_upper</th>\n",
       "      <th>icubed_mean</th>\n",
       "      <th>icubed_lower</th>\n",
       "      <th>icubed_upper</th>\n",
       "      <th>inv_ven_mean</th>\n",
       "      <th>inv_ven_lower</th>\n",
       "      <th>inv_ven_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>new_icu_upper</th>\n",
       "      <th>totdea_mean</th>\n",
       "      <th>totdea_lower</th>\n",
       "      <th>totdea_upper</th>\n",
       "      <th>bedover_mean</th>\n",
       "      <th>bedover_lower</th>\n",
       "      <th>bedover_upper</th>\n",
       "      <th>icuover_mean</th>\n",
       "      <th>icuover_lower</th>\n",
       "      <th>icuover_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vermont</th>\n",
       "      <th>2020-06-16</th>\n",
       "      <td>166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.730</td>\n",
       "      <td>27.000</td>\n",
       "      <td>61.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estonia</th>\n",
       "      <th>2020-04-28</th>\n",
       "      <td>117</td>\n",
       "      <td>453.862952</td>\n",
       "      <td>52.0600</td>\n",
       "      <td>1944.260000</td>\n",
       "      <td>118.942453</td>\n",
       "      <td>14.4625</td>\n",
       "      <td>519.877727</td>\n",
       "      <td>107.296235</td>\n",
       "      <td>12.9275</td>\n",
       "      <td>466.177045</td>\n",
       "      <td>...</td>\n",
       "      <td>76.592235</td>\n",
       "      <td>184.329</td>\n",
       "      <td>41.000</td>\n",
       "      <td>721.175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.26</td>\n",
       "      <td>77.942453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.877727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Andalucia</th>\n",
       "      <th>2020-07-21</th>\n",
       "      <td>201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1011.910</td>\n",
       "      <td>848.975</td>\n",
       "      <td>1423.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hungary</th>\n",
       "      <th>2020-04-14</th>\n",
       "      <td>103</td>\n",
       "      <td>415.871093</td>\n",
       "      <td>78.1475</td>\n",
       "      <td>1552.564706</td>\n",
       "      <td>103.809325</td>\n",
       "      <td>29.9975</td>\n",
       "      <td>353.250000</td>\n",
       "      <td>94.083411</td>\n",
       "      <td>23.3975</td>\n",
       "      <td>334.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>60.588235</td>\n",
       "      <td>120.195</td>\n",
       "      <td>101.000</td>\n",
       "      <td>187.050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austria</th>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>123</td>\n",
       "      <td>2.635000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>7.790789</td>\n",
       "      <td>0.580112</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.894868</td>\n",
       "      <td>0.377034</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.300395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>421.470</td>\n",
       "      <td>363.000</td>\n",
       "      <td>609.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       v1  allbed_mean  allbed_lower  allbed_upper  \\\n",
       "location  date                                                       \n",
       "Vermont   2020-06-16  166     0.000000        0.0000      0.000000   \n",
       "Estonia   2020-04-28  117   453.862952       52.0600   1944.260000   \n",
       "Andalucia 2020-07-21  201     0.000000        0.0000      0.000000   \n",
       "Hungary   2020-04-14  103   415.871093       78.1475   1552.564706   \n",
       "Austria   2020-05-04  123     2.635000        0.5000      7.790789   \n",
       "\n",
       "                      icubed_mean  icubed_lower  icubed_upper  inv_ven_mean  \\\n",
       "location  date                                                                \n",
       "Vermont   2020-06-16     0.000000        0.0000      0.000000      0.000000   \n",
       "Estonia   2020-04-28   118.942453       14.4625    519.877727    107.296235   \n",
       "Andalucia 2020-07-21     0.000000        0.0000      0.000000      0.000000   \n",
       "Hungary   2020-04-14   103.809325       29.9975    353.250000     94.083411   \n",
       "Austria   2020-05-04     0.580112        0.0000      1.894868      0.377034   \n",
       "\n",
       "                      inv_ven_lower  inv_ven_upper  ...  new_icu_upper  \\\n",
       "location  date                                      ...                  \n",
       "Vermont   2020-06-16         0.0000       0.000000  ...       0.000000   \n",
       "Estonia   2020-04-28        12.9275     466.177045  ...      76.592235   \n",
       "Andalucia 2020-07-21         0.0000       0.000000  ...       0.000000   \n",
       "Hungary   2020-04-14        23.3975     334.825000  ...      60.588235   \n",
       "Austria   2020-05-04         0.0000       1.300395  ...       0.000000   \n",
       "\n",
       "                      totdea_mean  totdea_lower  totdea_upper  bedover_mean  \\\n",
       "location  date                                                                \n",
       "Vermont   2020-06-16       35.730        27.000        61.025           0.0   \n",
       "Estonia   2020-04-28      184.329        41.000       721.175           0.0   \n",
       "Andalucia 2020-07-21     1011.910       848.975      1423.025           0.0   \n",
       "Hungary   2020-04-14      120.195       101.000       187.050           0.0   \n",
       "Austria   2020-05-04      421.470       363.000       609.025           0.0   \n",
       "\n",
       "                      bedover_lower  bedover_upper  icuover_mean  \\\n",
       "location  date                                                     \n",
       "Vermont   2020-06-16            0.0           0.00      0.000000   \n",
       "Estonia   2020-04-28            0.0         207.26     77.942453   \n",
       "Andalucia 2020-07-21            0.0           0.00      0.000000   \n",
       "Hungary   2020-04-14            0.0           0.00      0.000000   \n",
       "Austria   2020-05-04            0.0           0.00      0.000000   \n",
       "\n",
       "                      icuover_lower  icuover_upper  \n",
       "location  date                                      \n",
       "Vermont   2020-06-16            0.0       0.000000  \n",
       "Estonia   2020-04-28            0.0     478.877727  \n",
       "Andalucia 2020-07-21            0.0       0.000000  \n",
       "Hungary   2020-04-14            0.0       0.000000  \n",
       "Austria   2020-05-04            0.0       0.000000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihme_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df = column_or_index_string_reformat(pd.read_csv('./OWID_git_and_manual_case_and_test_data/owid-covid-data.csv'))\n",
    "owid_df.loc[:, 'date'] = pd.to_datetime(owid_df.loc[:, 'date']).dt.normalize()\n",
    "owid_df = owid_df.set_index(['location','date']).sort_index()\n",
    "owid_df = regularize_country_names(owid_df)\n",
    "owid_df.columns.names = ['owid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owid</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>tests_units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algeria</th>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>DZA</td>\n",
       "      <td>2629</td>\n",
       "      <td>94</td>\n",
       "      <td>375</td>\n",
       "      <td>8</td>\n",
       "      <td>59.953</td>\n",
       "      <td>2.144</td>\n",
       "      <td>8.552</td>\n",
       "      <td>0.182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Niger</th>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>NER</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>SGP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>IRL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>NZL</td>\n",
       "      <td>647</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>134.170</td>\n",
       "      <td>19.700</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23777.0</td>\n",
       "      <td>2093.0</td>\n",
       "      <td>4.918</td>\n",
       "      <td>0.433</td>\n",
       "      <td>units unclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "owid                   iso_code  total_cases  new_cases  total_deaths  \\\n",
       "location    date                                                        \n",
       "Algeria     2020-04-20      DZA         2629         94           375   \n",
       "Niger       2020-04-01      NER           20          0             3   \n",
       "Singapore   2020-01-08      SGP            0          0             0   \n",
       "Ireland     2020-02-21      IRL            0          0             0   \n",
       "New Zealand 2020-03-31      NZL          647         95             1   \n",
       "\n",
       "owid                    new_deaths  total_cases_per_million  \\\n",
       "location    date                                              \n",
       "Algeria     2020-04-20           8                   59.953   \n",
       "Niger       2020-04-01           0                    0.826   \n",
       "Singapore   2020-01-08           0                    0.000   \n",
       "Ireland     2020-02-21           0                    0.000   \n",
       "New Zealand 2020-03-31           0                  134.170   \n",
       "\n",
       "owid                    new_cases_per_million  total_deaths_per_million  \\\n",
       "location    date                                                          \n",
       "Algeria     2020-04-20                  2.144                     8.552   \n",
       "Niger       2020-04-01                  0.000                     0.124   \n",
       "Singapore   2020-01-08                  0.000                     0.000   \n",
       "Ireland     2020-02-21                  0.000                     0.000   \n",
       "New Zealand 2020-03-31                 19.700                     0.207   \n",
       "\n",
       "owid                    new_deaths_per_million  total_tests  new_tests  \\\n",
       "location    date                                                         \n",
       "Algeria     2020-04-20                   0.182          NaN        NaN   \n",
       "Niger       2020-04-01                   0.000          NaN        NaN   \n",
       "Singapore   2020-01-08                   0.000          NaN        NaN   \n",
       "Ireland     2020-02-21                   0.000          NaN        NaN   \n",
       "New Zealand 2020-03-31                   0.000      23777.0     2093.0   \n",
       "\n",
       "owid                    total_tests_per_thousand  new_tests_per_thousand  \\\n",
       "location    date                                                           \n",
       "Algeria     2020-04-20                       NaN                     NaN   \n",
       "Niger       2020-04-01                       NaN                     NaN   \n",
       "Singapore   2020-01-08                       NaN                     NaN   \n",
       "Ireland     2020-02-21                       NaN                     NaN   \n",
       "New Zealand 2020-03-31                     4.918                   0.433   \n",
       "\n",
       "owid                      tests_units  \n",
       "location    date                       \n",
       "Algeria     2020-04-20            NaN  \n",
       "Niger       2020-04-01            NaN  \n",
       "Singapore   2020-01-08            NaN  \n",
       "Ireland     2020-02-21            NaN  \n",
       "New Zealand 2020-03-31  units unclear  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = column_or_index_string_reformat(pd.read_csv('./OxCGRT_response_data/OxCGRT_20200504.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df.loc[:,'date'] = pd.to_datetime(oxcgrt_df.date,format='%Y%m%d').dt.normalize()\n",
    "oxcgrt_df = oxcgrt_df.set_index(['country_name', 'date']).sort_index()\n",
    "oxcgrt_df.index.names = ['location','date']\n",
    "oxcgrt_df.columns.names = ['oxcgrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oxcgrt</th>\n",
       "      <th>country_code</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>c1_flag</th>\n",
       "      <th>c2_workplace_closing</th>\n",
       "      <th>c2_flag</th>\n",
       "      <th>c3_cancel_public_events</th>\n",
       "      <th>c3_flag</th>\n",
       "      <th>c4_restrictions_on_gatherings</th>\n",
       "      <th>c4_flag</th>\n",
       "      <th>c5_close_public_transport</th>\n",
       "      <th>...</th>\n",
       "      <th>h3_contact_tracing</th>\n",
       "      <th>h4_emergency_investment_in_healthcare</th>\n",
       "      <th>h5_investment_in_vaccines</th>\n",
       "      <th>m1_wildcard</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>confirmed_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>stringency_index_for_display</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Greenland</th>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>GRL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.90</td>\n",
       "      <td>86.90</td>\n",
       "      <td>88.81</td>\n",
       "      <td>88.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jamaica</th>\n",
       "      <th>2020-04-13</th>\n",
       "      <td>JAM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.42</td>\n",
       "      <td>80.42</td>\n",
       "      <td>75.71</td>\n",
       "      <td>75.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sierra Leone</th>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>SLE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.48</td>\n",
       "      <td>81.48</td>\n",
       "      <td>80.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guatemala</th>\n",
       "      <th>2020-01-20</th>\n",
       "      <td>GTM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tunisia</th>\n",
       "      <th>2020-04-12</th>\n",
       "      <td>TUN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>685.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>63.89</td>\n",
       "      <td>63.89</td>\n",
       "      <td>80.00</td>\n",
       "      <td>80.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "oxcgrt                  country_code  c1_school_closing  c1_flag  \\\n",
       "location     date                                                  \n",
       "Greenland    2020-04-18          GRL                3.0      1.0   \n",
       "Jamaica      2020-04-13          JAM                3.0      1.0   \n",
       "Sierra Leone 2020-04-15          SLE                3.0      1.0   \n",
       "Guatemala    2020-01-20          GTM                0.0      NaN   \n",
       "Tunisia      2020-04-12          TUN                3.0      1.0   \n",
       "\n",
       "oxcgrt                   c2_workplace_closing  c2_flag  \\\n",
       "location     date                                        \n",
       "Greenland    2020-04-18                   2.0      1.0   \n",
       "Jamaica      2020-04-13                   2.0      1.0   \n",
       "Sierra Leone 2020-04-15                   3.0      1.0   \n",
       "Guatemala    2020-01-20                   0.0      NaN   \n",
       "Tunisia      2020-04-12                   3.0      1.0   \n",
       "\n",
       "oxcgrt                   c3_cancel_public_events  c3_flag  \\\n",
       "location     date                                           \n",
       "Greenland    2020-04-18                      2.0      1.0   \n",
       "Jamaica      2020-04-13                      2.0      1.0   \n",
       "Sierra Leone 2020-04-15                      2.0      1.0   \n",
       "Guatemala    2020-01-20                      0.0      NaN   \n",
       "Tunisia      2020-04-12                      2.0      1.0   \n",
       "\n",
       "oxcgrt                   c4_restrictions_on_gatherings  c4_flag  \\\n",
       "location     date                                                 \n",
       "Greenland    2020-04-18                            3.0      1.0   \n",
       "Jamaica      2020-04-13                            4.0      1.0   \n",
       "Sierra Leone 2020-04-15                            3.0      1.0   \n",
       "Guatemala    2020-01-20                            0.0      NaN   \n",
       "Tunisia      2020-04-12                            NaN      NaN   \n",
       "\n",
       "oxcgrt                   c5_close_public_transport  ...  h3_contact_tracing  \\\n",
       "location     date                                   ...                       \n",
       "Greenland    2020-04-18                        2.0  ...                 2.0   \n",
       "Jamaica      2020-04-13                        0.0  ...                 1.0   \n",
       "Sierra Leone 2020-04-15                        0.0  ...                 2.0   \n",
       "Guatemala    2020-01-20                        0.0  ...                 0.0   \n",
       "Tunisia      2020-04-12                        0.0  ...                 1.0   \n",
       "\n",
       "oxcgrt                   h4_emergency_investment_in_healthcare  \\\n",
       "location     date                                                \n",
       "Greenland    2020-04-18                                    NaN   \n",
       "Jamaica      2020-04-13                                    NaN   \n",
       "Sierra Leone 2020-04-15                                    0.0   \n",
       "Guatemala    2020-01-20                                    0.0   \n",
       "Tunisia      2020-04-12                                    0.0   \n",
       "\n",
       "oxcgrt                   h5_investment_in_vaccines  m1_wildcard  \\\n",
       "location     date                                                 \n",
       "Greenland    2020-04-18                        NaN          NaN   \n",
       "Jamaica      2020-04-13                        NaN          NaN   \n",
       "Sierra Leone 2020-04-15                        0.0          NaN   \n",
       "Guatemala    2020-01-20                        0.0          NaN   \n",
       "Tunisia      2020-04-12                        0.0          NaN   \n",
       "\n",
       "oxcgrt                   confirmed_cases  confirmed_deaths  stringency_index  \\\n",
       "location     date                                                              \n",
       "Greenland    2020-04-18             11.0               0.0             86.90   \n",
       "Jamaica      2020-04-13             72.0               4.0             80.42   \n",
       "Sierra Leone 2020-04-15             11.0               0.0             81.48   \n",
       "Guatemala    2020-01-20              NaN               NaN              0.00   \n",
       "Tunisia      2020-04-12            685.0              28.0             63.89   \n",
       "\n",
       "oxcgrt                   stringency_index_for_display  \\\n",
       "location     date                                       \n",
       "Greenland    2020-04-18                         86.90   \n",
       "Jamaica      2020-04-13                         80.42   \n",
       "Sierra Leone 2020-04-15                         81.48   \n",
       "Guatemala    2020-01-20                          0.00   \n",
       "Tunisia      2020-04-12                         63.89   \n",
       "\n",
       "oxcgrt                   legacy_stringency_index  \\\n",
       "location     date                                  \n",
       "Greenland    2020-04-18                    88.81   \n",
       "Jamaica      2020-04-13                    75.71   \n",
       "Sierra Leone 2020-04-15                    80.00   \n",
       "Guatemala    2020-01-20                     0.00   \n",
       "Tunisia      2020-04-12                    80.00   \n",
       "\n",
       "oxcgrt                   legacy_stringency_index_for_display  \n",
       "location     date                                             \n",
       "Greenland    2020-04-18                                88.81  \n",
       "Jamaica      2020-04-13                                75.71  \n",
       "Sierra Leone 2020-04-15                                80.00  \n",
       "Guatemala    2020-01-20                                 0.00  \n",
       "Tunisia      2020-04-12                                80.00  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "#Pull the data using their API (for whatever reason this data set is different from the manual download).\n",
    "# url_to_present_date = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-01-02/' \\\n",
    "#                         + str(datetime.now().date())\n",
    "# response = requests.get(url_to_present_date)\n",
    "# response_json = response.json()\n",
    "# response_json_nested_dict = response_json['data']\n",
    "\n",
    "# response_api_df = pd.DataFrame.from_dict({(i,j): response_json_nested_dict[i][j] \n",
    "#                            for i in response_json_nested_dict.keys() \n",
    "#                            for j in response_json_nested_dict[i].keys()},\n",
    "#                        orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_tracker</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>penalty</th>\n",
       "      <th>population</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Belize</th>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>398000</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occupied Palestinian Territory</th>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>368</td>\n",
       "      <td>4890</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5101000</td>\n",
       "      <td>95.9</td>\n",
       "      <td>95.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burkina Faso</th>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20903000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bhutan</th>\n",
       "      <th>2020-04-07</th>\n",
       "      <td>203</td>\n",
       "      <td>1166</td>\n",
       "      <td>1.3</td>\n",
       "      <td>772000</td>\n",
       "      <td>151.0</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mainland China</th>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>0</td>\n",
       "      <td>320000</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1439324000</td>\n",
       "      <td>22.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_tracker                               new_tests  tests_cumulative  \\\n",
       "location                       date                                      \n",
       "Belize                         2020-04-21          0                13   \n",
       "Occupied Palestinian Territory 2020-03-26        368              4890   \n",
       "Burkina Faso                   2020-03-31          0                99   \n",
       "Bhutan                         2020-04-07        203              1166   \n",
       "Mainland China                 2020-04-24          0            320000   \n",
       "\n",
       "test_tracker                               penalty  population  per100k  \\\n",
       "location                       date                                       \n",
       "Belize                         2020-04-21      1.3      398000      3.3   \n",
       "Occupied Palestinian Territory 2020-03-26      1.3     5101000     95.9   \n",
       "Burkina Faso                   2020-03-31      1.3    20903000      0.5   \n",
       "Bhutan                         2020-04-07      1.3      772000    151.0   \n",
       "Mainland China                 2020-04-24      1.3  1439324000     22.2   \n",
       "\n",
       "test_tracker                               testsPer100k  \n",
       "location                       date                      \n",
       "Belize                         2020-04-21           3.3  \n",
       "Occupied Palestinian Territory 2020-03-26          95.9  \n",
       "Burkina Faso                   2020-03-31           0.5  \n",
       "Bhutan                         2020-04-07         151.0  \n",
       "Mainland China                 2020-04-24          22.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtrack_df = pd.read_csv('./TestTracker_data/Tests_20200504.csv')\n",
    "testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date']).dt.normalize()\n",
    "# testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date'], format='%Y-%m-%d', errors='coerce')\n",
    "testtrack_df = testtrack_df.set_index(['country','date']).sort_index()\n",
    "testtrack_df.index.names = ['location','date']\n",
    "testtrack_df.columns.names = ['test_tracker']\n",
    "unused_columns = ['ind', 'jhu_ID.x', 'source', 'X.x', 'X.y', 'alpha2', 'alpha3',\n",
    "                  'numeric', 'latitude', 'longitude', 'jhu_ID.y', 'notes']\n",
    "\n",
    "testtrack_df = testtrack_df.drop(columns=unused_columns)\n",
    "testtrack_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delphi-epidata\n",
    "<a id='delphi'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "<font color='red'>\n",
    " ### currently unused\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_source\tname of upstream data source \n",
    "(e.g., fb-survey, google-survey, ght, quidel, doctor-visits)\tstring\n",
    "\n",
    "signal\tname of signal derived from upstream data (see notes below)\tstring\n",
    "\n",
    "time_type\ttemporal resolution of the signal (e.g., day, week)\tstring\n",
    "\n",
    "geo_type\tspatial resolution of the signal (e.g., county, hrr, msa, dma, state)\tstring\n",
    "\n",
    "time_values\ttime unit (e.g., date) over which underlying events happened\tlist of time values (e.g., 20200401)\n",
    "\n",
    "geo_value\tunique code for each location, depending on geo_type (county -> FIPS 6-4 code, HRR -> HRR number, MSA -> CBSA code,\n",
    "DMA -> DMA code, state -> two-letter state code), or * for all\tstring\n",
    "\n",
    "As of this writing, data sources have the following signals:\n",
    "\n",
    "fb-survey signal values include raw_cli, raw_ili, raw_wcli, raw_wili, and also four additional named with raw_* replaced by smoothed_* (e.g. smoothed_cli, etc).\n",
    "google-survey signal values include raw_cli and smoothed_cli.\n",
    "ght signal values include raw_search and smoothed_search.\n",
    "quidel signal values include smoothed_pct_negative and smoothed_tests_per_device.\n",
    "doctor-visits signal values include smoothed_cli.\n",
    "\n",
    "Delphi API data :\n",
    "doctor visits : 20200201-20200429 (as of 20200503)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    csse_us_timeseries_df\n",
    "    ihme_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "    csse_us_timeseries_df, ihme_df, owid_df, oxcgrt_df, testtrack_df]\n",
    "\n",
    "global_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "                owid_df, oxcgrt_df, testtrack_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(all_data):\n",
    "    all_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(global_data):\n",
    "    global_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_intersection = global_data[0].index.levels[0]\n",
    "dates_union =  global_data[0].index.levels[1].unique()\n",
    "for i in range(len(global_data)-1):\n",
    "    country_intersection = country_intersection.intersection(global_data[i+1].index.levels[0])\n",
    "    dates_union = dates_union.union(global_data[i+1].index.levels[1].unique())\n",
    "\n",
    "global_data_intersected = [intersect_country_index(df, country_intersection) for df in global_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2019-12-31 00:00:00 to 2020-05-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('The range of all dates is from {} to {}'.format(dates_union.min(), dates_union.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of countries included is 110\n"
     ]
    }
   ],
   "source": [
    "print('The final number of countries included is {}'.format(len(country_intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense, because of the intersections between data; to us the u.s. time series and ihme data together but not with\n",
    "the global data. The hospital data is very useful and so it may be important to look specifically at the small number of countries it contains. Regardless; by using only the global data we can keep 110 countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Want to have all time dependent data defined on the same time ranges for convenience;\n",
    "this involves two steps. 1. Initialize the new dates, 2. deal with the missing values. \n",
    "These missing values references the ones introduced by resampling or redefining the range of \n",
    "each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "normalized_global_data = [resample_dates(df, dates_union) for df in global_data_intersected]\n",
    "# To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "data = pd.concat([make_multilevel_columns(df) for df in normalized_global_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = multiindex_to_table(data)\n",
    "case_features = data_table.columns[data_table.columns.str.contains('confirmed') | data_table.columns.str.contains('cases')].tolist()\n",
    "case_features_to_drop = case_features[4:6]\n",
    "case_features_to_avg = case_features[:3] + [case_features[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These features are dropped because of how similar they are to the target ['total_cases_per_million', 'new_cases_per_million']\n",
      "These features are being averaged and constitute the target variable ['confirmed', 'global_confirmed', 'total_cases', 'confirmed_cases']\n"
     ]
    }
   ],
   "source": [
    "print('These features are dropped because of how similar they are to the target', case_features_to_drop)\n",
    "print('These features are being averaged and constitute the target variable', case_features_to_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groupby_indices = [data_table[data_table.location==country].index for country in data_table.location.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_averages = data_table.loc[country_groupby_indices[0], case_features_to_avg].mean(axis=1)\n",
    "for indices in country_groupby_indices[1:]:\n",
    "    case_averages = pd.concat((case_averages, data_table.loc[indices, case_features_to_avg].mean(axis=1)),axis=0)\n",
    "    \n",
    "data_table.loc[:, 'cases_average'] = case_averages\n",
    "data_table = data_table.drop(columns=case_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was planning on using a \"days since first case\" variable, which would equal zero until the date of the\n",
    "first case, but I believe this would correlate too strongly with the target variable. To test this assumption I'll compute it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_number_of_cases = data_table.cases_average.replace(to_replace=0., value=np.nan).dropna().index\n",
    "no_cases_dropped = data_table.loc[positive_number_of_cases,:]\n",
    "country_groupby_indices_dropped_nan = [no_cases_dropped[no_cases_dropped.location==country].index for country in no_cases_dropped.location.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I actually need this so that I can make predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32899905945257746\n"
     ]
    }
   ],
   "source": [
    "days_since = []\n",
    "for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "    nonzero_list = list(range(len(c)))\n",
    "    zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "    days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "data_table.loc[:, 'days_since_first_case'] = days_since\n",
    "print(data_table.days_since_first_case.corr(data_table.cases_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I misinterpreted the fact that days since first case is linear growth by defininition (really has the shape of a ReLU) and number of cases is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another peculiarity is the existence of two different features both called 'new_tests'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tests_tmp = data_table.loc[:, 'new_tests'].mean(1)\n",
    "data_table = data_table.drop(columns=['new_tests'])\n",
    "data_table.loc[:, 'new_tests'] = new_tests_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have aggregated and dropped the respective features, the missing values of the remaining data can be flagged and\n",
    "created into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which values are missing\n",
    "missing_flags = data_table.isna()\n",
    "# Add a suffix to label these flag variables\n",
    "missing_flags.columns = missing_flags.columns + '_missing_flag'\n",
    "# The first two features consist of location and date; they do not miss any values and so the flag columns would be all 0's. \n",
    "# therefore they are sliced out. \n",
    "missing_flags = missing_flags.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = data_table.columns.str.contains('flag')\n",
    "\n",
    "# 'tests_units' have string like values\n",
    "data_table.loc[:, 'tests_units'] = data_table.loc[:, 'tests_units'].fillna('Missing')\n",
    "# the 'flag' columns from oxcgrt data set already use 0 as a value, so fill them separately with -1\n",
    "data_table.loc[:, data_table.columns[flag_columns]] = data_table.loc[:, data_table.columns[flag_columns]].fillna(value=-1)\n",
    "# Population is a static number but some entries are missing; its ok to backfill this.\n",
    "data_table.loc[:, 'population'] = data_table.loc[:, ['location', 'population']].replace(\n",
    "                                    to_replace=0., value=np.nan).groupby('location').fillna(method='bfill')\n",
    "\n",
    "# The remainder of the columns are filled with interpolation, then ffill \n",
    "data_interp = data_table.loc[:, data_table.columns[~flag_columns]]\n",
    "\n",
    "# Cannot fill with interpolation, because it will \"look\" into the future. \n",
    "# interpolated = data_interp.groupby(by='location', as_index=False).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "forwardfill = data_interp.groupby(by='location', as_index=False).fillna(method='ffill')\n",
    "# forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "remaining_nan = forwardfill.fillna(value=0)\n",
    "# remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "data_table.loc[remaining_nan.index, remaining_nan.columns] = remaining_nan\n",
    "data_table = data_table.drop(columns=['country_code','iso_code','m1_wildcard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OxCGRT's \"flag\" columns (which indicate a target or general response) are numerical but I will cast them as categorical\n",
    "#so that they are not affected by the upcoming numerical feature manipulations. \n",
    "# flag_columns =  data.columns.levels[1][data.columns.levels[1].str.contains('flag')]\n",
    "# multiindex_for_flag_columns = pd.MultiIndex.from_product([['oxcgrt'], flag_columns], names=['dataset', 'features'])\n",
    "# data.loc[:, multiindex_for_flag_columns] = data.loc[:, multiindex_for_flag_columns].fillna(value=-1.).astype('category')\n",
    "# data_numerical = data.copy().select_dtypes(include='number')\n",
    "\n",
    "# # Flagging every step is probably overkill\n",
    "# interpolated = data_numerical.groupby(level=0).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# # interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "# forwardfill = interpolated.groupby(level=0).fillna(method='ffill')\n",
    "# # forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "# remaining_nan = forwardfill.fillna(value=0)\n",
    "# # remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "# backfill with interpolation, forward fill the remainder; NaNs may remain if there are only missing values\n",
    "# in their group. Therefore, still need to replace the remainder with something. Because so many of the features\n",
    "# utilize 0, I'm going to fill the remainder of missing values with -1 because nowhere do negative values appear. \n",
    "# data.loc[data_numerical.index, data_numerical.columns] = remaining_nan\n",
    "# data.loc[:, ('owid', 'tests_units')] = data.loc[:, ('owid', 'tests_units')].fillna('Missing')\n",
    "\n",
    "# still_missing_values = data.loc[:, pd.IndexSlice['test_tracker',:]].isna().sum()#.loc[pd.IndexSlice[:, #.index.levels[1]\n",
    "# throw_out_these = still_missing_values.index[still_missing_values > 0]\n",
    "\n",
    "# data = data.drop(columns=[('owid','iso_code'),\n",
    "#                          ('oxcgrt','m1_wildcard'), ('oxcgrt','country_code')]\n",
    "#                           + throw_out_these.tolist())\n",
    "# only remaining missing values are not numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "active                                   0.934062\n",
       "deaths                                   0.903164\n",
       "recovered                                0.718987\n",
       "global_deaths                            0.915767\n",
       "global_recovered                         0.726982\n",
       "total_deaths                             0.881795\n",
       "new_deaths                               0.830676\n",
       "total_deaths_per_million                 0.431965\n",
       "new_deaths_per_million                   0.371301\n",
       "total_tests                              0.874978\n",
       "total_tests_per_thousand                 0.151815\n",
       "new_tests_per_thousand                   0.119395\n",
       "c1_school_closing                        0.169630\n",
       "c1_flag                                  0.109961\n",
       "c2_workplace_closing                     0.203262\n",
       "c2_flag                                  0.145811\n",
       "c3_cancel_public_events                  0.167065\n",
       "c3_flag                                  0.098734\n",
       "c4_restrictions_on_gatherings            0.205405\n",
       "c4_flag                                  0.110628\n",
       "c5_close_public_transport                0.123029\n",
       "c5_flag                                  0.126615\n",
       "c6_stay_at_home_requirements             0.195979\n",
       "c6_flag                                  0.120386\n",
       "c7_restrictions_on_internal_movement     0.177180\n",
       "c7_flag                                  0.120239\n",
       "c8_international_travel_controls         0.109518\n",
       "e1_income_support                        0.272140\n",
       "e1_flag                                  0.167412\n",
       "e2_debt_contract_relief                  0.206847\n",
       "e3_fiscal_measures                       0.055412\n",
       "e4_international_support                -0.000586\n",
       "h1_public_information_campaigns          0.106266\n",
       "h1_flag                                  0.088017\n",
       "h2_testing_policy                        0.199444\n",
       "h3_contact_tracing                       0.075162\n",
       "h4_emergency_investment_in_healthcare    0.083166\n",
       "h5_investment_in_vaccines               -0.000225\n",
       "stringency_index                         0.161440\n",
       "stringency_index_for_display             0.161440\n",
       "legacy_stringency_index                  0.158147\n",
       "legacy_stringency_index_for_display      0.157082\n",
       "tests_cumulative                         0.887895\n",
       "penalty                                  0.137723\n",
       "population                               0.151872\n",
       "per100k                                  0.055755\n",
       "testsPer100k                             0.055755\n",
       "cases_average                            1.000000\n",
       "days_since_first_case                    0.329833\n",
       "new_tests                                0.497220\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.corrwith(data_table.cases_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>active</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>penalty</th>\n",
       "      <th>population</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "      <th>cases_average</th>\n",
       "      <th>days_since_first_case</th>\n",
       "      <th>new_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7794</th>\n",
       "      <td>Mauritania</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215</td>\n",
       "      <td>...</td>\n",
       "      <td>75.71</td>\n",
       "      <td>75.71</td>\n",
       "      <td>713.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4650000.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>3951.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.553</td>\n",
       "      <td>...</td>\n",
       "      <td>94.29</td>\n",
       "      <td>94.29</td>\n",
       "      <td>39476.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>164689000.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4353.666667</td>\n",
       "      <td>47</td>\n",
       "      <td>3386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>33.57</td>\n",
       "      <td>33.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273524000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>Kosovo</td>\n",
       "      <td>2020-03-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.52</td>\n",
       "      <td>9.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1800000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>97.14</td>\n",
       "      <td>97.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27691000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        location       date  active  deaths  recovered  global_deaths  \\\n",
       "7794  Mauritania 2020-04-17     4.0     1.0        2.0            1.0   \n",
       "1123  Bangladesh 2020-04-24  3951.0   127.0      108.0          131.0   \n",
       "5959   Indonesia 2020-02-06     0.0     0.0        0.0            0.0   \n",
       "6999      Kosovo 2020-03-09     0.0     0.0        0.0            0.0   \n",
       "7278  Madagascar 2020-04-05    70.0     0.0        2.0            0.0   \n",
       "\n",
       "      global_recovered  total_deaths  new_deaths  total_deaths_per_million  \\\n",
       "7794               2.0           1.0         0.0                     0.215   \n",
       "1123             112.0          91.0         7.0                     0.553   \n",
       "5959               0.0           0.0         0.0                     0.000   \n",
       "6999               0.0           0.0         0.0                     0.000   \n",
       "7278               2.0           0.0         0.0                     0.000   \n",
       "\n",
       "      ...  legacy_stringency_index  legacy_stringency_index_for_display  \\\n",
       "7794  ...                    75.71                                75.71   \n",
       "1123  ...                    94.29                                94.29   \n",
       "5959  ...                    33.57                                33.57   \n",
       "6999  ...                     9.52                                 9.52   \n",
       "7278  ...                    97.14                                97.14   \n",
       "\n",
       "      tests_cumulative  penalty   population  per100k  testsPer100k  \\\n",
       "7794             713.0      1.3    4650000.0     15.3          15.3   \n",
       "1123           39476.0      1.3  164689000.0     24.0          24.0   \n",
       "5959               0.0      0.0  273524000.0      0.0           0.0   \n",
       "6999               0.0      0.0    1800000.0      0.0           0.0   \n",
       "7278               0.0      0.0   27691000.0      0.0           0.0   \n",
       "\n",
       "      cases_average  days_since_first_case  new_tests  \n",
       "7794       7.000000                     34        0.0  \n",
       "1123    4353.666667                     47     3386.0  \n",
       "5959       0.000000                      0        0.0  \n",
       "6999       0.000000                      0        0.0  \n",
       "7278      71.500000                     16        0.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_names in data.columns.levels[0]:\n",
    "#     dataset_datatable = multiindex_to_table(data.loc[:, pd.IndexSlice[dataset_names, :]])\n",
    "#     dataset_datatable.to_csv(dataset_names+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_table = pd.concat((data_table, missing_flags), axis=1)\n",
    "data_table.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13860, 107)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat of the above calculations for United States only data.\n",
    "\n",
    "<font color='red'>\n",
    "unfinished as of now\n",
    "</font>\n",
    "\n",
    "The United States' data merits separate investigation 1. because of the case number 2. because the IHME dataset is only really\n",
    "properly defined for the statewide description of the U.S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = [\n",
    "    csse_us_timeseries_df,\n",
    "    ihme_df,\n",
    "    owid_df,\n",
    "    oxcgrt_df,\n",
    "    testtrack_df]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
