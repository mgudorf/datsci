{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN COVID-19 case number forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"files/architecture2.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n",
    "\n",
    "![title](conv_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    \n",
    "    # plot residuals in addition to the residual per country as\n",
    "    # line plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax1)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax2)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax1)\n",
    "        residual_plot(y_true,y_predict, ax=ax2)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def flatten_Xy_splits(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict, n_countries, title='',suptitle='',figname=None, scale=None, s=None):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    # compute scores \n",
    "    mse_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_naive, r2_predict))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    \n",
    "    ax1.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_naive,3)),fontsize=14)\n",
    "    ax1.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_naive,3)),fontsize=14)\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "             \n",
    "    ax2.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_predict,3)),fontsize=14)\n",
    "    ax2.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_predict,3)),fontsize=14)\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "\n",
    "    \n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    ax3.set_title('')\n",
    "    ax4.set_title('')\n",
    "    ax3.set_ylabel('Residual')\n",
    "    ax4.set_ylabel('Residual')\n",
    "    ax3.grid(True)\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    if figname is not None:\n",
    "        plt.savefig(figname, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    Normalize with respect to some absolute max, just choose 2*absolute max of training set. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The main format (shape) of the data and how it is being sliced and then reshaped is described in the notebook ```COVID19_model_prototypes.ipynb``` and so I will avoid a redundant discussion here. \n",
    "\n",
    "In regards to the feature data being considered, I am using the time dependent variables pertaining directly to COVID-19, such as new cases, new deaths, etc. as well as time dependent variables which quantify different governments' reactions to the pandemic; most notably is the usage of the oxford's \"stringency index\" which scores each goverments reaction with a number from 0 to 100.\n",
    "\n",
    "I am in fact not using the data on the number of tests, however, because the feature lacks a consistent set of units. In the regression this is counteracted upon by using one-hot encoding to categorize the units but instead of stratifying the new tests variable by units, I elect to simply drop the testing data, which may or may not be an unwise choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data on 132 countries, and for 161 different days. There is not case information for all countries for all dates, this may be a source of error and one potential correction is to remove all \"frames\" which contain 0 information on the pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 132 different countries included in the data\n",
      "The time series span 161 different days\n"
     ]
    }
   ],
   "source": [
    "print('There are {} different countries included in the data'.format(data.location.nunique()))\n",
    "print('The time series span {} different days'.format(data.time_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full intersection of countries in fact included more information, but the countries with populations less than 1 million people were pruned from the dataset, in order to remove any chance of undue influence from very small states such as the Vatican or San Marino, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import cleaned data produced by other notebook. \n",
    "# data = pd.read_csv('cnn_data.csv',index_col=0)\n",
    "# # data = data.drop(columns=['date'])\n",
    "\n",
    "# # pandemic_start_date_median = data[data.days_since_first_case == 1].time_index.median()\n",
    "# # data[data.days_since_first_case==int(data.time_index.max()-pandemic_start_date_median)].location.nunique()\n",
    "# # new_dates = data[data.time_index>=60]\n",
    "# # dates_with_days_since_first_case_equals_zero = new_dates.groupby('location').days_since_first_case.min()\n",
    "# # countries_to_drop = dates_with_days_since_first_case_equals_zero[dates_with_days_since_first_case_equals_zero==0].index\n",
    "# # data = new_dates[~new_dates.location.isin(countries_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data.csv', index_col=0)\n",
    "tmp =pd.read_csv('regression_data_full.csv', index_col=0)\n",
    "data = pd.concat((data,tmp.loc[:, column_search(tmp,'government_response')]),axis=1)\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(columns=column_search(data,'test'))\n",
    "data = data.drop(columns=column_search(data,'deaths'))\n",
    "data = data.drop(columns=column_search(data,'recovered'))\n",
    "# data = data.drop(columns=column_search(data,'log'))\n",
    "data = data.drop(columns=column_search(data,'std'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this notebook is to tune the hyperparameters and architecture of a simple one dimensional convolutional neural network such that it is able to make accurate predictions of the number of new cases. In pursuit of this goal, I elect to utilize the keras API for the deep learning implementation. The details or philosophy on how the model is constructed is also detailed in the model prototypes notebook, this notebook is simply devoted to tuning the model parameters; the architecture could also be played with (and has been) but I will leave any dramatically different architectures to different notebooks. The current architecture in this notebook represents the best (using the performance from the prototyping stage as a metric). Other inclusions which were previously tested were: more activation layers, pooling layers, dropout layers, non negative kernel constraints (the predicted value should be non-negative, there is a constraint for this). The architecture that is fixed in this notebook consists of two convolutional layers followed by two fully connected layers. In the first convolutional layer, the number of filters is chosen to be much larger than the filters in the second convolutional layer (but is also tuned). Likewise, the output of the first fully connected layer is much larger than the second (which has only 1 output in this setup). To ensure that all future tests are comparable, I always start with the same model coefficients by initializing the kernels of each layer of the network. This introduces even more parameters, using a normal distribution to initialize, such as the mean and standard deviation. This is not an issue however as in the final model these kernel initializers will be unincluded. The data is split such that the final holdout set contains a single day's information, the validation set contains 1 week or 7 days of information and the remainder is in the training set.\n",
    "\n",
    "As a reminder, the data format as well as the features being included are both described in the model prototyping notebook. The most obvious parameters with which to tune the model are the number of training epochs, the batch size of the training, the number of filters in each convolutional layer, the size of the convolutional kernel (i.e. convolutional window) and finally the number of outputs in the first dense layer. The number of convolutional filters are chosen such that the ratio is always the same between the first and second layer. Something to keep in mind as well is that the data I am training with contains only 20000 samples, and so the number of parameters should reflect this fact, by not being too numerous. \n",
    "\n",
    "The following values will be used for tuning purposes. \n",
    "\n",
    "    epochs = [50, 100, 200]\n",
    "    batch_size = [64, 256, 1024]\n",
    "    filter1 = [16,32,64]\n",
    "    filter2 = [2,4,8]\n",
    "    kernel1_size = [4,6,8]\n",
    "    kernel2_size = [4,6,8]\n",
    "    \n",
    "I elect to constrain the output of the first dense layer to be the same as the number of inputs to the first dense layer, which is in turn the number of outputs of the second convolutional layer. This dimension depends on the kernel size as well as filter number in said convolutional layers.\n",
    "\n",
    "Create a 5-d tensor for convolution and FC layer parameters using itertools.\n",
    "The layers of the CNN will remain the same throughout testing for now. That is, \n",
    "\n",
    "    1. Conv1D(f1, k1)\n",
    "    2. Conv1D(f2, k2)\n",
    "    3. Flatten\n",
    "    4. Dense(d)\n",
    "\n",
    "\n",
    "I don't know enough yet, but my intuition tells me that the overall trend (macroscopic picture)\n",
    "is much more important to capture than microscopic. \n",
    "   \n",
    "    f1 : number of filters,  first convolutional layer\n",
    "    k1 : kernel size, first convolutional layer\n",
    "    f1 : number of filters,  second convolutional layer\n",
    "    k1 : kernel size, second convolutional layer\n",
    "    d : number of nodes in hidden fully connected layer\n",
    "    \n",
    "\n",
    "from https://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf\n",
    "To achieve error $\\epsilon$ with filter number $m$ the number of samples needed is $\\mathcal{O}(m/\\epsilon^2)$\n",
    "Let $\\epsilon = 0.1$, then $100m$ = samples or $m = samples/100$. The number of samples is\n",
    "It seems that included information from the too distant past actually makes the model worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main differences from prototyping stage.\n",
    "\n",
    "The data being used, and the parameters being tuned; it was the architecture that was mainly tested previously in the\n",
    "prototyping notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.time_index>=40]\n",
    "# model_data = data.copy().iloc[:, 3:]\n",
    "modeling_features = ['new_cases_per_million', 'government_response_index', 'log_new_cases_per_million']\n",
    "model_data = data.copy().loc[:, modeling_features]\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 28\n",
    "start_date = frame_size + data.time_index.min()\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create, split, and scale the data into tensors which will abide by keras conventions (after collapsing the first axis, at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 132 159\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_cnn_train, y_cnn_train, X_cnn_validate, y_cnn_validate, X_cnn_test, y_cnn_test) = scaled_splits\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 132, 28, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_scaled_tmp = np.concatenate((X_cnn_train,X_cnn_validate,X_cnn_test), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.scatter(range(len(X_scaled_tmp)), X_scaled_tmp[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=30,color='k')\n",
    "ax.scatter(range(len(X_cnn_train)), X_cnn_train[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='train')\n",
    "ax.scatter(range(len(X_cnn_train), len(X_cnn_train)+len(X_cnn_validate)), X_cnn_validate[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='validate')\n",
    "ax.scatter(range(len(X_cnn_train)+len(X_cnn_validate), len(X)), X_cnn_test[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='test')\n",
    "plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_3d = concatenate_4d_into_3d(scaled_splits)\n",
    "(X_cnn_train_model, y_cnn_train_model, X_cnn_validate_model,\n",
    " y_cnn_validate_model, X_cnn_test_model, y_cnn_test_model) = splits_3d\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [50, 100, 150]\n",
    "batch_size_list = [64]\n",
    "filter1_list = [64]\n",
    "filter2_list = [4, 8]\n",
    "kernel1_size_list = [4, 6, 8]\n",
    "kernel2_size_list = [4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations = list(itertools.product(epochs_list, batch_size_list, filter1_list,\n",
    "                                                filter2_list, kernel1_size_list, kernel2_size_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to both save the results as well as ensure that the run of this notebook can be broken up into sessions, create a DataFrame which logs the scores and saves them to a .csv file. During the exploration of hyperparameters, this csv will be overwritten with the inclusion of any new scores calculated. This will allow for comparison and analysis of the hyper parameter space, as well as ensure that we do not have to fit the model 729 times per run of this notebook. The scores that are saved are the ones generated by the validation set predictions. Also included for comparison purposes are the scores resulting from the naive baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('cnn_score_logging.csv'):\n",
    "    score_logging_df = pd.read_csv('cnn_score_logging.csv', index_col=0)\n",
    "else:\n",
    "    score_logging_df = pd.DataFrame(np.array(parameter_combinations), \n",
    "                                    columns=['epochs','batch_size','filter_1','filter_2','kernel_1','kernel_2'])\n",
    "    score_logging_df.loc[:, 'mean_squared_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_squared_error'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "kernel0 = RandomNormal(seed=0)\n",
    "kernel1 = RandomNormal(seed=1)\n",
    "kernel2 = RandomNormal(seed=2)\n",
    "kernel3 = RandomNormal(seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>filter_1</th>\n",
       "      <th>filter_2</th>\n",
       "      <th>kernel_1</th>\n",
       "      <th>kernel_2</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>naive_mean_absolute_error</th>\n",
       "      <th>naive_explained_variance</th>\n",
       "      <th>naive_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  batch_size  filter_1  filter_2  kernel_1  kernel_2  \\\n",
       "35     150          64        64         8         8         6   \n",
       "10      50          64        64         8         8         4   \n",
       "22     100          64        64         8         8         4   \n",
       "5       50          64        64         4         8         6   \n",
       "3       50          64        64         4         6         6   \n",
       "\n",
       "    mean_squared_error  mean_absolute_error  explained_variance  \\\n",
       "35                 NaN                  NaN                 NaN   \n",
       "10                 NaN                  NaN                 NaN   \n",
       "22                 NaN                  NaN                 NaN   \n",
       "5                  NaN                  NaN                 NaN   \n",
       "3                  NaN                  NaN                 NaN   \n",
       "\n",
       "    naive_mean_absolute_error  naive_explained_variance  \\\n",
       "35                        NaN                       NaN   \n",
       "10                        NaN                       NaN   \n",
       "22                        NaN                       NaN   \n",
       "5                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "\n",
       "    naive_mean_squared_error  \n",
       "35                       NaN  \n",
       "10                       NaN  \n",
       "22                       NaN  \n",
       "5                        NaN  \n",
       "3                        NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_logging_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  elif not isinstance(value, collections.Sized):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################"
     ]
    }
   ],
   "source": [
    "for i, hyper_parameters in enumerate(parameter_combinations):\n",
    "    \n",
    "    (epochs, batch_size, filter1, \n",
    "     filter2, kernel1_size, kernel2_size) = hyper_parameters\n",
    "    \n",
    "    if score_logging_df.isna().loc[i,'mean_squared_error']:\n",
    "        cnn = Sequential()\n",
    "        cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                         padding='valid',\n",
    "                         input_shape=X_cnn_train.shape[2:],\n",
    "                         use_bias=False,\n",
    "                        kernel_initializer=kernel0\n",
    "                        )\n",
    "                 )\n",
    "        cnn.add(Conv1D(filters=int(filter2), \n",
    "                         kernel_size=int(kernel2_size), \n",
    "                         padding='valid',\n",
    "                         use_bias=False,\n",
    "                        kernel_initializer=kernel1\n",
    "                        )\n",
    "                 )\n",
    "        cnn.add(Flatten())\n",
    "        cnn.add(Dense(cnn.output.shape[1], \n",
    "                      kernel_initializer=kernel2\n",
    "                       )\n",
    "                 )\n",
    "        cnn.add(Dense(1, \n",
    "                        activation='relu',\n",
    "                      kernel_initializer=kernel3\n",
    "                           ))\n",
    "        cnn.compile(loss='mse', optimizer=Adam())\n",
    "        history = cnn.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "                  batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        y_true = y_cnn_validate_model.ravel()\n",
    "        y_predict = cnn.predict(X_cnn_validate_model).ravel()\n",
    "        y_naive = y_validate_naive.ravel()\n",
    "\n",
    "        score_logging_df.loc[i,'naive_mean_squared_error'] = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'mean_squared_error']  = mean_squared_error(y_true.ravel(), y_predict)\n",
    "        score_logging_df.loc[i,'naive_explained_variance']  = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'explained_variance']  = explained_variance_score(y_true.ravel(), y_predict)\n",
    "        score_logging_df.loc[i,'naive_mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_predict)\n",
    "        # every time a new score is calculated, overwrite the original file, to save space but also save progress scoring.\n",
    "        score_logging_df.to_csv('cnn_score_logging.csv')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('#',end='')\n",
    "    if (i % 50 == 0) & (i>0):\n",
    "          print(' {} runs completed'.format(str(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_logging_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_logging_df.mean_squared_error.apply(lambda x : np.log(x+1)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of having results for the meeting, cut the parameter space search short and proceed with analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_parameters = score_logging_df.loc[score_logging_df.mean_squared_error.idxmin(),:].iloc[:6]\n",
    "best_model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these \"best\" model parameters with random kernels instead of the seeded ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel5 = RandomNormal(seed=5)\n",
    "kernel6 = RandomNormal(seed=6)\n",
    "kernel7 = RandomNormal(seed=7)\n",
    "kernel8 = RandomNormal(seed=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_train = np.concatenate((X_cnn_train_model, X_cnn_validate_model),axis=0)\n",
    "y_final_train = np.concatenate((y_cnn_train_model, y_cnn_validate_model),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(epochs, batch_size, filter1, \n",
    " filter2, kernel1_size, kernel2_size) = best_model_parameters.astype(int).values\n",
    "\n",
    "best_cnn = Sequential()\n",
    "best_cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                 padding='valid',\n",
    "                 input_shape=X_cnn_train.shape[2:],\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel5,\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Conv1D(filters=int(filter2), \n",
    "                 kernel_size=int(kernel2_size), \n",
    "                 padding='valid',\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel6,\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Flatten())\n",
    "best_cnn.add(Dense(best_cnn.output.shape[1],\n",
    "                kernel_initializer=kernel7,\n",
    "               )\n",
    "         )\n",
    "best_cnn.add(Dense(1, \n",
    "                activation='relu',  \n",
    "                kernel_initializer=kernel8,\n",
    "                   ))\n",
    "best_cnn.compile(loss='mse', optimizer=Adam())\n",
    "history = best_cnn.fit(X_final_train, y_final_train, epochs=epochs, validation_data=(X_cnn_test_model, y_cnn_test_model), \n",
    "          batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we are not overtraining the CNN, let's look at the plots of validation loss and training loss as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'][-100:], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'][-100:], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overtraining would be signalled by the validation loss increasing while the training loss decreases; there does not seem to be substantial evidence for this so we can move onto the prediction results. As a \"sanity check\", predict the outcomes of the training data, to see whether or not the predictions perform better than the naive baseline; if not, then either something is wrong or the model is not worth pursuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_final_train.ravel()\n",
    "y_predict = best_cnn.predict(X_final_train).ravel()\n",
    "y_naive = np.concatenate((y_train_naive, y_validate_naive),axis=0)\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Training set predictions vs. naive baseline',\n",
    "              figname='cnn_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the efficacy of the model by predicting the validation data. Note that this is different from the \"testing\" set, or hold-out set, which is used for the final check of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_test_model.ravel()\n",
    "y_predict = best_cnn.predict(X_cnn_test_model).ravel()\n",
    "y_naive = y_test_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Testing set predictions vs. naive baseline',\n",
    "              figname='cnn_test_performance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5),sharex=True)\n",
    "(ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "_ = ax1.hist(best_cnn.get_weights()[0].ravel())\n",
    "_ = ax2.hist(best_cnn.get_weights()[1].ravel())\n",
    "_ = ax3.hist(best_cnn.get_weights()[2].ravel())\n",
    "_ = ax4.hist(best_cnn.get_weights()[3].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "I would like to test whether or not removing some of the less \"helpful\" data would train the CNN more accurately. I.e. take the subset of the data where all countries have at least a single presenting case; this can be sliced by the conditional statement \"data.days_since_first_case >= 1\". Alternatively, I coulud simply truncate all time series but a certain cutoff date. This would be more uniform and hopefully have the same effect. Because the number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
