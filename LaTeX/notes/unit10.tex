\subsection{Unit 7 Notes: Data Storytelling September 21th 2019}

\subsubsection{Bias and statistics}

Bias and variance are inversely related

Fisher weighting, make weighting
equal to inverse of variance.

Nate Silver Weighting Method
\begin{itemize}
\item exponential decay based on recency of a poll
\item sample size of poll
\item pollster rating
\end{itemize}

\HREF{http://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works}{fivethirtyeight}
multiple testing, Bonferroni

regression towards the mean
regression paradox.

In statistics the math of predicting
$y = rx$ does not switch to $x = y/r$ because
both are using the same model; would still
be $x = ry$.

Ordinary least squares (OLS)
Sample quantities versus population quantities
population quantities look better usually.

In regression, Gauss-Markov Theorem.
\beq
\beta = (X^' X)^{-1}X^' y
\eeq
minimizes the bias.

How do you quantify how well
model actually fits? $R^2$ value.
\beq
\text{var}(y) = \text{var}(X\hat{\beta}) + \text{var}(e)
\eeq

\beq
R^2 = \frac{\text{var}(X\hat{\beta})}{\text{var}(y)}
\eeq

This does not validate the model,
it increases monotonically, and
does \textit{not} say how well its
going to predict new things.

Logistic regression (useful binary distribution)
Odds ratio for measuring association, literally
just a ratio of the odds.
logistic regression model to predict confounding factors.

curse of dimensionality; high dimensional statistics
is hard. interpolation better than extrapolation,
in high dimensions the space is so sparse
extrapolation is the only thing thats possible.

Gelman hierarchical modeling.
Ridge regression and shrinkage.
Stein's paradox and shrinkage estimation.

James-stein estimator always beats the vector $y$,
this says that it is always better to solve a compound
problem then solve individual independent problems.

The result is called shrinkage, when is you shrink
towards the grand mean. An intuitive way of
thinking about this is merely regression towards
the mean.

LASSO and sparsity.
In a linear regression model, in place of minimizing
the sum of squared residuals, LASSO says to minimize
\beq
\sum (y_i \beta_0 -\sum_k \beta_j x_{ij})^2 + \lambda \sum_j |\beta_j| = \text{RSS} + \lambda \sum |\beta_j|
\eeq

LASSO will literally make the $\beta$'s zero as opposed to minimizing them;
there are nice algorithms for fitting.

Supervised learning
\begin{itemize}
\item kNN ($k$ nearest neighbors)
\item SVM
\item Decision Trees
\item Random Forests
\item Bagging
\item Boosting
\end{itemize}


Unsupervised learned
\begin{itemize}
\item PCA
\item MDS
\item Clustering
\end{itemize}

SIFT and object recognition

``choice of feature is most important choice''.

PCA algorithm
\begin{enumerate}
\item subtract mean from data
\item scale each dimension by its variance
\item compute covariance matrix $S = \frac{1}{N}X^{\top}X$
\item compute $k$ largest eigenvectors of $S$
\item eigenvectors are $k$ principal components
\end{enumerate}

weight features by either sampling less data of
that type or use algorithm.

eigenfaces paper. (M. Turk and A. Pentland).

MDS related to PCA; Decide on metric,
compute all pairwise distance, put into
matrix then perform PCA.


