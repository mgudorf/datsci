\subsection{Mike Badescu(5) Wednesday, October 2nd 2019}

\subsubsection{project proposal}

Regarding the project. First, see if there has
been anyone that has done something like this.

Diagnosis itself, herding hospitals. Hospitals
are penalized for the number of admissions, basically
because the number of older patients is related to the
amount of medicare/medicaid being used. This penalty
has increased a lot. There are companies who are
trying to do this; heart, kidney, not just pneumonia. The medical field is not moving
very fast.

The story of scoring. Basically dealing
with ``prehistoric analytics''. Credit score is
constructed by summing risk factors. Based on this model 
Show how it was constructed instead; take questions and make a work flow (directed graph?). This case has subjectivity

The weights are kind of arbitrary here because the categories are subjective. The automation is the real target. Repeated regressions.

The difference that explains versus predicts. Example, loans, banks have to tell you why your
credit is rejected. How do they get those
reasons, looking at the important variables.
They use logistic regression. 

Want to automate these things; need specific
manner to record data. In the scope of this
project? 

Perhaps do not focus on the scoring system;
just look at prediction. Black box, black magic
to use diagnostic tool. Why does this work is
another question. 

Decide on a base; mortality; how do you decide
on the level of care, time period, etc. Assume
we have a metric; first classifier is that all
people survive. This has an associated accuracy
which is going to be relatively high. This
isn't always easy to beat with a useful classifier. Second base is a simple linear and
or discrete regression. What is the best model after regression, this is also going to weight things, gives an idea about how powerful the model is. Neural network better beat this baseline model. \textbf{Should always have baselines} as they are the ``null hypothesis''

spore vector machines? random forest. No clear
indication of topology of the network which
is highly related to which neural network one
should use. Can't \textit{really} use convolution because this doesn't look like a signal. Assume there is no time dimension; in this case we need to take inputs from everything; dense neural network. Several layers
which are highly interconnected. Need to decide on width and height of neural networks. Need to try $x$ layers which is sort of greedy, not
guaranteed to work on non standard data sets. 

The way to approach data science and all of the
models: there is always going to be a black box model. Even if there is a new model, not going to understand how or why it works.

How do you know how much tuning to perform? ``adequate results''. 

Training, hold out, cross validation with the
data set. Projects always have a framework. Look
at summaries and correlation plots to get a
basic description of the data set. Should be able to come up with some sort of weak takeaways. Try to get these cheap insights because they will help you with create a weak
diagnostic. Or just eliminate certain classes of people. 

This falls under ``explanatory data analysis''
then comes the ``modeling''. Pay attention to
any types of time series values. 
small 1000
medium 100000
large 100000, depends on the number of columns
For deep learning the minimum is kind of 100000. 
What is important is that you don't waste time waiting; want to be able to perform experiments
and get a better idea. 
Might observe something in exploratory data analysis.  

More on research questions

Might have more than one goal; not just mortality but also the level of care. 
Can have multiple goals in mind, keep it
general, make it actionable. Patient specific
treatment plans.

Want something actionable because in this case
the penalty is readmission. 

To determine an adequate result:

Switching algorithms may cause jumps in the results. This kind of jump is an indicator that
one should tune.

Data quality is one of the most important qualities for the determination of these algorithms. The next best improvement is by
adding another dataset. Incorporate time, other
types of data. With neural network each link could link to a different types of dataset. The
weights all propagate. How complicated the neural networks can get is surprising.

Last thing to do is fine tuning; this is the most annoying and tedious part. Spend a little
bit of time to beat baselines; linear extrapolation? Do it after springboard even. 

If you can't read it aloud don't show it. 