{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Datasets currently not used\n",
    "\n",
    "\n",
    "\n",
    "## [The COVID tracking project testing data.](#source1)\n",
    "[https://covidtracking.com/api](https://covidtracking.com/api)\n",
    "\n",
    "## [Delphi-epidata ](#delphi) which contains \n",
    "       Facebook surveys, google surveys, doctor visits, google health trends, quidel test data\n",
    "[https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)\n",
    "\n",
    "I have not dove into this dataset too thoroughly but it contains information from facebook and google\n",
    "surveys regarding COVID as well as doctor visits; the doctor visit data attempts to make distinctions between\n",
    "those sick with the annual influenza and those with COVID.\n",
    "\n",
    "\n",
    "## [IHME hospital data](#ihme)\n",
    "\n",
    "**Data available at:**\n",
    "[http://www.healthdata.org/covid/data-downloads](http://www.healthdata.org/covid/data-downloads)\n",
    "\n",
    "The IHME hospital data is one of the more unique datasets I've discovered with \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 0.5)\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "#     if normalization_method=='minmax':\n",
    "        # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "        # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "        # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "        # be repeated for each validation, test frame for each country for each timestep.\n",
    "    X_min = X_train.min(axis=(2))\n",
    "    X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "    X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "                    X_max[:, :, np.newaxis, :])\n",
    "    X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "                           X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "    if train_test_only:\n",
    "    # Normalize the training data by each frame's specific mean and std deviation. \n",
    "        splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    else:\n",
    "        X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "                                   X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "        splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "#     else:\n",
    "        \n",
    "#         X_mean = X_train.mean(axis=(1,2))\n",
    "#         X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "\n",
    "#         X_train_scaled = normal(X_train, \n",
    "#                                 X_mean[:, np.newaxis, np.newaxis, :],\n",
    "#                                 X_std[:, np.newaxis, np.newaxis, :])\n",
    "#         X_test_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "#         if train_test_only:\n",
    "#         # Normalize the training data by each frame's specific mean and std deviation. \n",
    "#             splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "#         else:\n",
    "#             X_validate_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "#             splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME hospital data\n",
    "<a id='ihme'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "[JHU CSSE](#csse) \n",
    "<font color='red'>\n",
    "### Has all USA states but only 32 countries which overlap with other data; stash this dataset for now. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_df = column_or_index_string_reformat(pd.read_csv(\n",
    "    './IHME_hospital_data/2020_04_12.02/Hospitalization_all_locs.csv').rename(columns={'location_name':'location'}))\n",
    "ihme_df.loc[:, 'date'] = pd.to_datetime(ihme_df.loc[:,'date']).dt.normalize()\n",
    "ihme_df = ihme_df.set_index(['location', 'date']).sort_index()\n",
    "ihme_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delphi-epidata\n",
    "<a id='delphi'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "data_source\tname of upstream data source \n",
    "(e.g., fb-survey, google-survey, ght, quidel, doctor-visits)\tstring\n",
    "\n",
    "signal\tname of signal derived from upstream data (see notes below)\tstring\n",
    "\n",
    "time_type\ttemporal resolution of the signal (e.g., day, week)\tstring\n",
    "\n",
    "geo_type\tspatial resolution of the signal (e.g., county, hrr, msa, dma, state)\tstring\n",
    "\n",
    "time_values\ttime unit (e.g., date) over which underlying events happened\tlist of time values (e.g., 20200401)\n",
    "\n",
    "geo_value\tunique code for each location, depending on geo_type (county -> FIPS 6-4 code, HRR -> HRR number, MSA -> CBSA code,\n",
    "DMA -> DMA code, state -> two-letter state code), or * for all\tstring\n",
    "\n",
    "As of this writing, data sources have the following signals:\n",
    "\n",
    "fb-survey signal values include raw_cli, raw_ili, raw_wcli, raw_wili, and also four additional named with raw_* replaced by smoothed_* (e.g. smoothed_cli, etc).\n",
    "google-survey signal values include raw_cli and smoothed_cli.\n",
    "ght signal values include raw_search and smoothed_search.\n",
    "quidel signal values include smoothed_pct_negative and smoothed_tests_per_device.\n",
    "doctor-visits signal values include smoothed_cli.\n",
    "\n",
    "Delphi API data :\n",
    "doctor visits : 20200201-20200429 (as of 20200503)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Currently Unused ----------------------#\n",
    "\n",
    "def pull_delphi_data(data_source=['fb-survey', 'google-survey', 'ght', 'quidel', 'quidelneg', 'doctor-visits'], \n",
    "                     daterange=pd.date_range(start=\"20200101\",\n",
    "                                             end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d'),\n",
    "                     **kwargs):\n",
    "    \"\"\" Pull data from https://cmu-delphi.github.io/delphi-epidata/api/\n",
    "        https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for data in data_source:\n",
    "        signal_dict = {'fb-survey':'smoothed_cli',\n",
    "                       'google-survey':'smoothed_cli',\n",
    "                       'ght':'smoothed_search',\n",
    "                       'quidel':'smoothed_tests_per_device',\n",
    "                       'quidelneg':'smoothed_pct_negative',\n",
    "                       'doctor-visits':'smoothed_cli'}\n",
    "        \n",
    "        signal = signal_dict[data]\n",
    "        if data=='quidelneg':\n",
    "            #change the proxy for the quidel signal\n",
    "            data = 'quidel'\n",
    "        for days in daterange:\n",
    "            resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "            day_data = resp.json().get('epidata', None)\n",
    "            if day_data is None:\n",
    "                pass\n",
    "            else:\n",
    "                var_number += pd.json_normalize(day_data).size\n",
    "                print(pd.json_normalize(day_data).shape)    \n",
    "                \n",
    "                \n",
    "# date_range_2020 = pd.date_range(start=\"20200101\",end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d')\n",
    "# var_number = 0 \n",
    "# for days in date_range_2020:\n",
    "# #     days='20200302'\n",
    "#     resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "#     day_data = resp.json().get('epidata', None)\n",
    "#     if day_data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         var_number += pd.json_normalize(day_data).size\n",
    "#         print(pd.json_normalize(day_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Unused as of now</font>\n",
    "\n",
    "\n",
    "## Repeat of the above calculations for United States only data.\n",
    "\n",
    "\n",
    "\n",
    "The United States' data merits separate investigation 1. because of the case number 2. because the IHME dataset is only really\n",
    "properly defined for the statewide description of the U.S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_us_timeseries_df.groupby(level=1).sum().to_csv('csse_united_states.csv')\n",
    "\n",
    "usa_data = [\n",
    "    csse_us_timeseries_df,\n",
    "    ihme_df,\n",
    "    owid_df,\n",
    "    oxcgrt_df,\n",
    "    testtrack_df]\n",
    "\n",
    "us_table = multiindex_to_table(csse_us_timeseries_df)\n",
    "today = us_table[us_table.date==us_table.date.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "#Pull the data using their API (for whatever reason this data set is different from the manual download).\n",
    "# url_to_present_date = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-01-02/' \\\n",
    "#                         + str(datetime.now().date())\n",
    "# response = requests.get(url_to_present_date)\n",
    "# response_json = response.json()\n",
    "# response_json_nested_dict = response_json['data']\n",
    "\n",
    "# response_api_df = pd.DataFrame.from_dict({(i,j): response_json_nested_dict[i][j] \n",
    "#                            for i in response_json_nested_dict.keys() \n",
    "#                            for j in response_json_nested_dict[i].keys()},\n",
    "#                        orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 10, 0.25)\n",
    "sigmoid = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "A = np.vstack([x, np.ones(len(x))]).T\n",
    "m, c = np.linalg.lstsq(A, sigmoid, rcond=None)[0]\n",
    "\n",
    "over = np.where(((m * x + c - sigmoid) > 0)  & (x<=0))\n",
    "under = np.where(((m * x + c - sigmoid) < 0) & (x>=0))\n",
    "xover_width = x[over].max() - x[over].min() \n",
    "xunder_width = x[under].max() - x[under].min()\n",
    "\n",
    "sigmoidmin, sigmoidmax = [-0.05, 1.05]\n",
    "\n",
    "# fig, (ax,ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "_ = ax.plot(x, sigmoid, color='k')\n",
    "_ = ax.plot(x, m * x + c, '--',color='k')\n",
    "_ = ax.set_ylim([-0.05, 1.05])\n",
    "_ = ax.vlines(x[over].min(), ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.vlines(0, ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.vlines(x[under].max(), ymin=sigmoidmin, ymax=sigmoidmax)\n",
    "_ = ax.hlines(0, xmin=-10, xmax=10)\n",
    "_ = ax.add_patch(Rectangle((x[over].min(), 0), xover_width, sigmoidmax, \n",
    "                           angle=0.0, alpha=0.5, color='k', label='Overestimation'))\n",
    "_ = ax.add_patch(Rectangle((0, 0), x[under].max(), sigmoidmax, \n",
    "                           angle=0.0, alpha=0.25, color='gray', label='Underestimation'))\n",
    "\n",
    "_ = ax.set_title('Possible explanation for regression prediction behavior')\n",
    "_ = ax.legend(loc=(0,1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "#the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "    df_list.append(tmp_df)\n",
    "\n",
    "daily_reports_df = pd.concat(df_list, axis=0)\n",
    "daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values\n",
    "\n",
    "daily_reports_df.loc[:, 'combined_key'] = (daily_reports_df.province_state.astype('str').replace(to_replace='nan', value='')+' '+ daily_reports_df.location.astype('str')).values\n",
    "\n",
    "daily_reports_df = daily_reports_df.drop(columns=['province_state', 'last_update', 'fips', 'admin2']).set_index(['location','date'])\n",
    "\n",
    "daily_reports_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from matplotlib.patches import Rectangle\n",
    "import re\n",
    "sns.set()\n",
    "\n",
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook uses a variety of different COVID-19 related datasets to explore the behavior\n",
    "of the multiple time series'. This notebook also creates new features that attempt to encapsulate the\n",
    "time dependent (and time delayed) nature of the problem; these will be used during the model creation\n",
    "project which makes time dependent forecasting models. \n",
    "\n",
    "\n",
    "# Table of contents\n",
    "\n",
    "## [Function definitions](#generalfunctions)\n",
    "\n",
    "## [Data](#imports)\n",
    "\n",
    "## [Exploratory Data Analysis](#EDA)\n",
    "\n",
    "## [Feature production](#newfeatures)\n",
    "\n",
    "## Function definitions <a id='generalfunctions'></a>\n",
    "\n",
    "def rolling_means(df, features, roll_widths):\n",
    "    new_feature_df_list = []\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values \n",
    "        rollmean = pd.DataFrame(df.groupby(by='location').rolling(window).mean().fillna(value=0.))\n",
    "#         rollstd = pd.DataFrame(df.groupby(by='location').rolling(window).std().fillna(value=0.))    \n",
    "#         new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        new_features = rollmean\n",
    "        new_cols = features +'_rolling_mean_' + str(window)\n",
    "#         rsind = features +'_rolling_std_' + str(window)\n",
    "#         new_cols = rmind.append(rsind)\n",
    "        new_features.columns = new_cols\n",
    "        new_feature_df_list.append(new_features)\n",
    "    return new_feature_df_list\n",
    "\n",
    "def tsplot(data, roll_width, **kw):\n",
    "    rollmean = datatmp.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "    rollstd  = datatmp.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "    cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(range(len(datatmp)), cis[0], cis[1], alpha=0.5)\n",
    "    ax.plot(range(len(datatmp)), rollmean, color='k', **kw)\n",
    "    return ax\n",
    "\n",
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "\n",
    "def column_search(df, name):\n",
    "    return df.columns[df.columns.str.contains(name)]\n",
    "\n",
    "def country_groupby_indices_list(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n",
    "\n",
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool|\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "def add_time_indices(data_table):\n",
    "    n_cases_pos = data_table.loc[:,'new_cases'].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "    country_groupby_indices = country_groupby_indices_list(data_table)\n",
    "    country_groupby_indices_dropped_nan = country_groupby_indices_list(n_cases_pos)\n",
    "    days_since = []\n",
    "    for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "        nonzero_list = list(range(len(c)))\n",
    "        zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "        days_since += list(zero_list)+nonzero_list\n",
    "\n",
    "    data_table.loc[:, 'time_index'] = days_since\n",
    "    data_table.loc[:, 'date_proxy'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))\n",
    "    return data_table\n",
    "\n",
    "## Data <a id='imports'></a>\n",
    "\n",
    "Differences in reporting\n",
    "Differences in government responses\n",
    "differences in time series.\n",
    "\n",
    "testing vs cases vs deaths\n",
    "log-log plot for current growth trends\n",
    "bollinger bands\n",
    "histogram of trending upwards, flat, downwards\n",
    "tools, different plots, correlation plots scatter matrix plots. \n",
    "\n",
    "Going to remove microstates like San Marino. \n",
    "\n",
    "data = pd.read_csv('eda_data.csv', index_col=0)\n",
    "data.sample(5)\n",
    "\n",
    "data.set_index(['location', 'date']).isna().groupby(level=1).sum().loc[:, column_name_search(data, 'new_test')].iloc[:,[0,-1]].plot()\n",
    "\n",
    "Estimation of actual death rates, assuming 50% of people are asymptomatic and do not actually get tested.\n",
    "\n",
    "today_data = data[data.date_proxy == data.date_proxy.max()]\n",
    "\n",
    "today_data.loc[:, 'estimation_death_rate'] = 100 * today_data.loc[:, 'deaths_jhucsse'].values / (2. * today_data.loc[:, 'cases_jhucsse'].values)\n",
    "\n",
    "(weighted_dr).plot.hist(bins=50)\n",
    "\n",
    "data.loc[:,column_name_search(data, 'long').tolist() + column_name_search(data, 'lat').tolist()].iloc[:, [1,3]].plot.hist()\n",
    "\n",
    "data.loc[:, 'population_owid'] = data.loc[:, 'population_owid'].fillna(method='bfill').fillna(method='ffill')\n",
    "data.loc[:, 'population_density_owid'] = data.loc[:, 'population_density_owid'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "Reporting inconsistencies\n",
    "\n",
    "data.loc[:, 'new_cases_per_million_owid'] = data.loc[:, 'new_cases_owid'] / (data.loc[:, 'population_owid'] / 1000000.)\n",
    "data.loc[:, 'new_cases_per_million_ttc'] = data.loc[:, 'new_cases_ttc'] / (data.loc[:, 'population_owid'] / 1000000.)\n",
    "\n",
    "ax = (data.groupby('location').sum().loc[:, ['new_cases_per_million_ttc','new_cases_per_million_owid']].diff(axis=1)).plot(label='tt')\n",
    "# data.groupby('location').sum().loc[:, 'new_cases_per_million_owid'].plot(ax=ax, label='owid')\n",
    "plt.legend()\n",
    "\n",
    "# # key words account for multilevel indices\n",
    "# data = pd.read_csv('data.csv', index_col=[0,1], header=[0,1])\n",
    "# data.sample(5)\n",
    "\n",
    "## Feature production <a id='newfeatures'></a>\n",
    "\n",
    "function ```append_rolling_values``` is not working. Need to compute rolling averages for each\n",
    "countries time series' individually but want to store them in the multi index DataFrame. \n",
    "\n",
    "\n",
    "\n",
    "Before interpolation and backfilling, I used to prune countries which did not have cases prior\n",
    "to responses (i.e. \"early responders\" were not included)\n",
    "To make my life easier, I'm only taking data which had cases before all government mandates so the rates before and after are well defined. We can think of these as being \"late responders\"\n",
    "\n",
    "initial_response_data = data.replace(to_replace=[0,0.], value=np.nan)\n",
    "\n",
    "time_dependent= model_data.set_index('location').loc[:, :'time_index'].iloc[:, :-2].reset_index()\n",
    "time_independent = model_data.loc[:, 'time_index':'cvd_death_rate'].iloc[:, 1:].fillna(0)#.columns\n",
    "flag_and_misc = model_data.loc[:, 'c1_flag_0':'date']\n",
    "\n",
    "roll_widths = [3, 7, 14]\n",
    "# roll_widths = list(range(2,15))\n",
    "\n",
    "\n",
    "rolling_predictors = rolling_features(time_dependent, time_dependent.columns.drop('location'), roll_widths)\n",
    "# This drops the first category, Afghanistan.\n",
    "location_one_hot = pd.get_dummies(model_data.loc[:, 'location'], drop_first=True)\n",
    "# this specifically drops the NaN category\n",
    "tests_units_one_hot = pd.get_dummies(model_data.loc[:, 'tests_units'])\n",
    "tests_units_one_hot.columns = 'test_units_' + reformat_values(tests_units_one_hot, category='columns')\n",
    "\n",
    "df = pd.concat((data.loc[:, 'date_proxy':'time_index'], time_dependent.drop(columns='location'), \n",
    "                pd.concat(rolling_predictors, axis=1).reset_index(drop=True), time_independent,\n",
    "                   location_one_hot, tests_units_one_hot, flag_and_misc),axis=1)\n",
    "\n",
    "df[df.location=='United States'].loc[:, df.columns[df.columns.str.contains('new_cases')]].plot(legend=False, figsize=(10,10))\n",
    "\n",
    "Data has the following partitions:\n",
    "\n",
    "columns until and not including 'population' are time series variables.\n",
    "        'population':'Albania' : time independent, continuous variables\n",
    "        'Albania':'location' : one-hot encoded variables\n",
    "        'location:' location and date, not encoded.\n",
    "        \n",
    "Time series variables with drift as baseline model : any columns with 'new' (cases, deaths, tests)\n",
    "Time series variables with naive as baseline model : The complement to the drift baseline variables. \n",
    "\n",
    "\n",
    "## Exploratory Data Analysis<a id='EDA'></a>\n",
    "Ideas for the inclusion or creation of new columns.\n",
    "\n",
    "Moving averages\n",
    "fourier\n",
    "signal\n",
    "flags for lots of different things\n",
    "\n",
    "hardest hit countries\n",
    "\n",
    "days since\n",
    "\n",
    "extrapolated, actual, interpolated\n",
    "\n",
    "which dataset it came from\n",
    "\n",
    "humans view, interpret and forecast things in a way which are not available to robots. \n",
    "data driven, time dependent manner of modeling. Really trying to encapsulate the time dependence. \n",
    "\n",
    "government_responses = pd.concat((data.loc[:, ['location','date']],\n",
    "                                  data.loc[:, column_search(data, 'oxcgrt')].drop(\n",
    "                                      columns=column_search(data, 'flag')).iloc[:,2:10]), axis=1)\n",
    "\n",
    "government_responses = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "government_responses.loc[:, 'date'] = pd.to_datetime(government_responses.loc[:, 'date'], format='%Y%m%d')\n",
    "# oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')\n",
    "\n",
    "government_responses = pd.concat((government_responses.loc[:, ['location', 'date']], government_responses.iloc[:,3:18:2]), axis=1)\n",
    "\n",
    "government_responses.loc[:, 'date'] = government_responses.date.dt.normalize().values\n",
    "\n",
    "\n",
    "\n",
    "countries_with_all_responses = None\n",
    "for i, country_indices in enumerate(country_groupby_indices_list(government_responses)):\n",
    "    government_responses.loc[country_indices,:] = government_responses.loc[country_indices,:].fillna(method='ffill')\n",
    "    has_all_flags = (government_responses.loc[country_indices,:].max() == 0).sum()\n",
    "    if has_all_flags != 0:\n",
    "        pass\n",
    "    else:\n",
    "        if countries_with_all_responses is None:\n",
    "            countries_with_all_responses = government_responses.loc[country_indices,:]\n",
    "        else:\n",
    "            countries_with_all_responses = pd.concat((countries_with_all_responses, government_responses.loc[country_indices,:]),axis=0)\n",
    "\n",
    "# government_responses = government_responses.dropna(axis=0).iloc[:,:10]\n",
    "\n",
    "countries_with_all_responses.loc[:, 'date'] = pd.to_datetime(countries_with_all_responses.date).dt.normalize()\n",
    "\n",
    "see_if_country_samples =countries_with_all_responses.groupby('location').count().sort_values(by='date') \n",
    "drop_these_countries = np.unique(np.where(~(see_if_country_samples == 148))[0])\n",
    "countries_with_all_responses = countries_with_all_responses[countries_with_all_responses.location != 'Kosovo']\n",
    "\n",
    "def add_time_indices(data_table, index_column='cases'):\n",
    "    indexer = data_table.loc[:, ['location',index_column]].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "    country_groupby_indices = country_groupby_indices_list(data_table)\n",
    "    country_groupby_indices_dropped_nan = country_groupby_indices_list(indexer)\n",
    "    days_since = []\n",
    "    for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "        nonzero_list = list(range(len(c)))\n",
    "        zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "        days_since += list(zero_list)+nonzero_list\n",
    "\n",
    "    data_table.loc[:, 'time_index'] = days_since\n",
    "    data_table.loc[:, 'date_proxy'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))\n",
    "    return data_table\n",
    "\n",
    "response_ranges = []\n",
    "for country_indices in country_groupby_indices_list(countries_with_all_responses):\n",
    "    tmp = countries_with_all_responses.loc[country_indices,:].replace(to_replace=[0,0.], value=np.nan)\n",
    "    for c in tmp.columns[2:]:\n",
    "        active_range = tmp.set_index('date').loc[:,c].dropna()\n",
    "        response_ranges.append(pd.IndexSlice[active_range.index.min():active_range.index.max()])\n",
    "\n",
    "response_slices_df = pd.DataFrame(np.array(response_ranges).reshape(countries_with_all_responses.location.nunique(), -1))\n",
    "\n",
    "country_list = []\n",
    "slice_list = []\n",
    "\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    before_list = []\n",
    "    after_list = []\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "#         after = effective_range.reset_index().Date.max()\n",
    "        slice_list += [before]   \n",
    "        \n",
    "enacted_ended_df = pd.DataFrame(np.array(slice_list).reshape(len(country_list), -1), index=country_list, columns=all_responses.columns)\n",
    "\n",
    "all_responses = response_df.iloc[:, [0, 1, 2, 3, 5, 6]]\n",
    "country_list = []\n",
    "minmax_list = []\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "        after = effective_range.reset_index().Date.max()\n",
    "        minmax_list += [before, after]   \n",
    "\n",
    "start_end_columns = np.array([[x+'_start', x+'_end'] for x in all_responses.columns.tolist()]).ravel()\n",
    "start_end_df = pd.DataFrame(np.array(minmax_list).reshape(len(country_list), -1), index=country_list, columns=start_end_columns)\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]\n",
    "\n",
    "first_response_dates = start_end_df.min(axis=1).sort_index()\n",
    "first_response_dates.head(10)\n",
    "\n",
    "first_case_dates = test_multiindex_df.reset_index(level=1).groupby(level=0).Date.min().sort_index()\n",
    "first_case_dates.head(10)\n",
    "\n",
    "dates_with_test_data = test_multiindex_df.tests_cumulative.dropna()\n",
    "dates_with_test_data.head()\n",
    "\n",
    "min_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "\n",
    "first_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "last_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.max()\n",
    "\n",
    "first_testing_dates.reset_index()\n",
    "\n",
    "# convert entire dataframe to index so it can be used to slice testing data, dataframe\n",
    "first_tmp =  first_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "last_tmp =  last_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "first_tmp.head()\n",
    "\n",
    "test_min = test_multiindex_df.loc[first_tmp.index, :]\n",
    "test_max = test_multiindex_df.loc[last_tmp.index, :]\n",
    "\n",
    "# reset index so we can subtract datetime variables.\n",
    "test_max_reset = test_max.reset_index(level=1)\n",
    "test_min_reset = test_min.reset_index(level=1)\n",
    "time_differential = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "testing_rates = np.log(test_max_reset.tests_cumulative / test_min_reset.tests_cumulative)# / time_intervals\n",
    "\n",
    "\n",
    "test_final_test_initial_time_intervals = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "\n",
    "case_response_differential = (first_case_dates-first_response_dates).dt.days\n",
    "\n",
    "late_response = case_response_differential < 0\n",
    "late_response\n",
    "\n",
    "states_to_inspect = ['Michigan', 'Georgia', 'New York', 'Texas']\n",
    "\n",
    "dead=us_deaths[us_deaths['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed=us_cases[us_cases['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed.head()\n",
    "\n",
    "\n",
    "since_first_case_normalized_u = u.replace(to_replace=[0,0.], value=np.nan)\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n",
    "death_rate_df = since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].copy()\n",
    "death_rate_normalized = 100 * since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].values / since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "death_rate_df.loc[:, :] = death_rate_normalized\n",
    "\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n",
    "\n",
    "\n",
    "first_case_dates.astype('category').cat.codes.plot.hist(bins=50)\n",
    "\n",
    "pd.concat(new_feature_df_list,ignore_index=False).sort_index(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "death_rate_df.transpose().plot().legend(bbox_to_anchor=(1, 1))\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "plt.grid(True, axis='both')\n",
    "plt.title('Death rate by state')\n",
    "plt.savefig('death_rate_NY_MI_GA.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax,ax2) = plt.subplots(1, 2, sharey=True,  figsize=(20,5), dpi=200)\n",
    "confirmed.loc[:, '2/21/20':].transpose().plot(ax=ax).legend(bbox_to_anchor=(0.2, 1))\n",
    "dead.loc[:, '2/21/20':].transpose().plot(ax=ax2).legend(bbox_to_anchor=(0.2, 1))\n",
    "ax.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax.set_title('Number of confirmed cases vs. time')\n",
    "ax2.set_title('Number of diseased vs. time')\n",
    "ax.grid(True, axis='both')\n",
    "ax2.grid(True, axis='both')\n",
    "plt.savefig('cases_vs_dead_comparison_GA_NY_MI.png', bbox_inches='tight')\n",
    "\n",
    "def top_5_counties(state_df, state_name):\n",
    "    state = state_df[(state_df.Province_State==state_name)]\n",
    "    state = state.drop(columns=['UID','iso2','iso3','code3','FIPS','Country_Region','Lat','Long_','Combined_Key','Province_State'])\n",
    "    top5_counties = state.groupby(by='Admin2').sum().sum(axis=1).sort_values(ascending=False)[:5].index.tolist()\n",
    "    state_info = state[state.Admin2.isin(top5_counties)].set_index('Admin2').transpose()\n",
    "    state_info.columns.name = 'County'\n",
    "    return state_info\n",
    "\n",
    "### Global COVID data\n",
    "\n",
    "global_recovered_dates_only = global_recovered.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_confirmed_dates_only = global_confirmed.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_dead_dates_only = global_dead.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "\n",
    "global_dead['type']='Dead'\n",
    "global_confirmed['type']='Confirmed'\n",
    "global_recovered['type']='Recovered'\n",
    "\n",
    "dead=global_dead[global_dead['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "confirmed=global_confirmed[global_confirmed['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "\n",
    "global_dead = global_dead.sort_index(axis=1)\n",
    "global_confirmed = global_confirmed.sort_index(axis=1)\n",
    "global_recovered = global_recovered.sort_index(axis=1)\n",
    "\n",
    "skr = global_confirmed.groupby('Country/Region').sum().iloc[143, :].loc['1/22/20':'4/28/20']\n",
    "skr.head()\n",
    "\n",
    "top10 = global_confirmed.groupby('Country/Region').sum().loc[:, '1/22/20':'4/28/20'].sort_values(by='4/28/20').iloc[-10:, :]\n",
    "skr = global_confirmed.groupby('Country/Region').sum().loc['Korea, South', '1/22/20':'4/28/20']\n",
    "\n",
    "top10_and_south_korea = pd.concat((top10, skr.to_frame(name='South Korea').transpose()),axis=0).sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "for i, country_time_series in enumerate(top10_and_south_korea.replace(to_replace=[0,0.], value=np.nan).values):\n",
    "    nan_count = np.sum(np.isnan(country_time_series))\n",
    "    days_since_first = np.roll(country_time_series, -nan_count)\n",
    "    plt.plot(days_since_first, label=top10_and_south_korea.index[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "global_dead_dates_only\n",
    "\n",
    "dsum = global_dead_dates_only.sum()\n",
    "csum = global_confirmed_dates_only.sum()\n",
    "drsum = 100*dsum/csum\n",
    "drsum.plot()\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "_ = plt.title('Average global death rate vs. time')\n",
    "plt.grid(True, axis='both')\n",
    "plt.savefig('death_rate_global.png', bbox_inches='tight')\n",
    "\n",
    "first_case_dates = case_df.reset_index().set_index(['Country','date']).total_cases.replace(\n",
    "                           to_replace=0,value=np.nan).dropna().reset_index(level=1).groupby(level=0).date.min()\n",
    "\n",
    "first_response_dates = response_df.min(axis=1)\n",
    "tmp = response_df.copy()\n",
    "dt = pd.DataFrame(np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1])))\n",
    "diff_df = tmp - np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1]))\n",
    "num_miss=diff_df.where(diff_df > pd.Timedelta(days=0)).isna().sum(1).sort_values(ascending=False)\n",
    "countries_with_cases_before_responses = num_miss.where(num_miss==0).dropna().index\n",
    "\n",
    "Just using the endpoints of each interval is not going to work as well, because if the endpoints represent outliers then they\n",
    "will not capture the overall trend. Therefore, I will do the following: average the two intervals before and after the quarantine measure (average the cases/((1M people)(100k tests)) and then compare the averages with the value at the quarantine date. I believe this is fair because it's being applied equally to both intervals.\n",
    "\n",
    "\n",
    "data = case_multiindex_df.join(test_multiindex_df, lsuffix='_x', rsuffix='_y').sort_index(axis=1, ascending=False)\n",
    "\n",
    "To ensure that total cases is a cumulative variable, replace zeros with np.nan and then backwards interpolate\n",
    "Growth rate calculations require values greater than zero, so remove all dates where there are zero confirmed cases, per country.\n",
    "\n",
    "# Normalize the time series, fill in with missing values with nan. \n",
    "data = data.reindex(pd.MultiIndex.from_product([data.index.levels[0], \n",
    "                    data.index.get_level_values(1).unique().sort_values()], names=['Country', 'Date']), fill_value=np.nan)\n",
    "\n",
    "# Don't use zeros this messes things up.\n",
    "data.loc[:, 'total_cases'] = data.loc[:, 'total_cases'].replace(to_replace=[0,0.], value=np.nan)\n",
    "# instantiate with copy so that we can iterate over DataFrame groupby\n",
    "data.loc[:, 'total_cases_interpolated'] = data.loc[:, 'total_cases'].copy()\n",
    "data.loc[:, 'tests_cumulative_interpolated'] = data.loc[:, 'tests_cumulative'].copy()\n",
    "\n",
    "for country, country_df in data.groupby(level=0):\n",
    "    data.loc[country, 'total_cases_interpolated'] = country_df.loc[:, 'total_cases'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'tests_cumulative_interpolated'] = country_df.loc[:, 'tests_cumulative'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'population'] = country_df.loc[:, 'population'].fillna(method='backfill')\n",
    "\n",
    "data.loc[:, 'cases_per_1M_people_per_100k_tests'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).values\n",
    "data.loc[:, 'cases_per_1M_people'] = (data.total_cases_interpolated / ((data.population/1000000.))).values\n",
    "\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).cumsum().apply(np.log)\n",
    "\n",
    "before_minus_after = response_multiindex_df.applymap(multiindex_response_date_to_average_rates).replace(to_replace=0., value=np.nan).sort_index()\n",
    "\n",
    "before_minus_after_residual_values =  before_minus_after.values - np.tile(before_minus_after.mean(1).values.reshape(-1,1), (1, 5))\n",
    "before_minus_after_residual_df = pd.DataFrame(before_minus_after_residual_values.reshape(-1, 5), columns=before_minus_after.columns, index=before_minus_after.index)\n",
    "before_minus_after_residual_df.head()\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / data.tests_cumulative_interpolated).cumsum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 1):\n",
    "#     # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "#     frame_data = model_data[(time_index <= max_date_in_window) & \n",
    "#                             (time_index > max_date_in_window-frame_size)]\n",
    "#     #     print(frame_data.shape)\n",
    "#     # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "#     reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#     #     print(reshaped_frame_data.shape)\n",
    "#     # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "#     # the first axis is always the default iteration axis. \n",
    "#     # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "#     resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "#     frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "#     if max_date_in_window == start_date:\n",
    "#         X = frame_data_4D.copy()\n",
    "#     else:\n",
    "#         X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "\n",
    "        \n",
    "# y = target_data.values[-X.shape[0]:].reshape(-1,1)\n",
    "# y_time_index = time_index.values[-X.shape[0]:].reshape(-1,1)\n",
    "# # y = target_data.values.transpose()[-X.shape[0]:,:]\n",
    "# # y_time_index = time_index.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:]\n",
    "\n",
    "# y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "# y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "# X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "# y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "# X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "# y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "# X_test = X[-n_test_frames:, :, :, :] \n",
    "# y_test = y[-n_test_frames:, :]\n",
    "# splits =  (X_train, y_train, X_validate, y_validate,\n",
    "#            X_test, y_test)\n",
    "\n",
    "# y_train_time = y_time_index[:-(n_validation_frames+n_test_frames),:]\n",
    "# y_validate_time = y_time_index[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "# y_test_time = y_time_index[-n_test_frames:, :]\n",
    "\n",
    "# Before normalization, plot the splits \n",
    "\n",
    "#     # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "#     # of steps to predict in the future, this is only slicing the frames. \n",
    "#     if train_test_only:\n",
    "#         (X_train, y_train, X_test, y_test) = splits\n",
    "#     else:\n",
    "#         (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "# #     if normalization_method=='minmax':\n",
    "#         # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "#         # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "#         # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "#         # be repeated for each validation, test frame for each country for each timestep.\n",
    "#     X_min = X_train.min(axis=(2))\n",
    "#     X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "#     X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "#                     X_max[:, :, np.newaxis, :])\n",
    "#     X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "#                            X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "#     if train_test_only:\n",
    "#     # Normalize the training data by each frame's specific mean and std deviation. \n",
    "#         splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "#     else:\n",
    "#         X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "#                                    X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "#         splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "#     # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "#     frame_data = model_data[(time_index <= max_date_in_window - 1) & \n",
    "#                             (time_index >= max_date_in_window-frame_size)]\n",
    "#     #     print(frame_data.shape)\n",
    "#     # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "#     reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#     #     print(reshaped_frame_data.shape)\n",
    "#     # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "#     # the first axis is always the default iteration axis. \n",
    "#     # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "#     resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "#     frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "# #     break\n",
    "#     if max_date_in_window == start_date:\n",
    "#         print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "\n",
    "#         X = frame_data_4D.copy()\n",
    "#     else:\n",
    "#         X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "# print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "# y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minmax(X, X_min, X_max):\n",
    "#     # X_min and X_max need to have already been made into 4-d tensors with np.newaxis\n",
    "#     tile_shape = np.array(np.array(X.shape) / np.array(X_min.shape), dtype=int)\n",
    "#     denominator = np.tile(X_max, tile_shape) - np.tile(X_min, tile_shape)\n",
    "#     denominator[denominator==0] = 1\n",
    "#     X_scaled = (X - np.tile(X_min, tile_shape)) / denominator\n",
    "#     return X_scaled\n",
    "\n",
    "# def normal(X, X_mean, X_std):\n",
    "#     tile_shape = np.array(np.array(X.shape) / np.array(X_mean.shape), dtype=int)\n",
    "#     mean_ = np.tile(X_mean, tile_shape)\n",
    "#     std_ =  np.tile(X_std, tile_shape)   \n",
    "#     std_[np.where(std_==0.)] = 1\n",
    "#     X_scaled = ((X - mean_) /  std_)\n",
    "#     return X_scaled\n",
    "\n",
    "# def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "#     \"\"\" Split into training, validation and test data.\n",
    "#     \"\"\"\n",
    "#     min_, max_ = (0, 1)\n",
    "#     # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "#     # of steps to predict in the future, this is only slicing the frames. \n",
    "#     if train_test_only:\n",
    "#         (X_train, y_train, X_test, y_test) = splits\n",
    "#     else:\n",
    "#         (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "#     if normalization_method=='minmax':\n",
    "#         # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "#         # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "#         # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "#         # be repeated for each validation, test frame for each country for each timestep.\n",
    "#         X_min = X_train.min(axis=(2))\n",
    "#         X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "#         X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "#                         X_max[:, :, np.newaxis, :])\n",
    "#         X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "#                                X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "#         if train_test_only:\n",
    "#         # Normalize the training data by each frame's specific mean and std deviation. \n",
    "#             splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "#         else:\n",
    "#             X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "#                                        X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "#             splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "#     else:\n",
    "        \n",
    "#         X_mean = X_train.mean(axis=(1,2))\n",
    "#         X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "\n",
    "#         X_train_scaled = normal(X_train, \n",
    "#                                 X_mean[:, np.newaxis, np.newaxis, :],\n",
    "#                                 X_std[:, np.newaxis, np.newaxis, :])\n",
    "#         X_test_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "#         if train_test_only:\n",
    "#         # Normalize the training data by each frame's specific mean and std deviation. \n",
    "#             splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "#         else:\n",
    "#             X_validate_scaled = normal(X_test, \n",
    "#                                X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "#                                X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "#             splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "#     return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "#     if train_test_only:\n",
    "#         (X_train, y_train, X_test, y_test) = splits\n",
    "#         X_train = np.concatenate(X_train, axis=0)\n",
    "#         y_train = np.concatenate(y_train, axis=0)\n",
    "#         X_test = np.concatenate(X_test, axis=0)\n",
    "#         y_test = np.concatenate(y_test, axis=0)\n",
    "#         concat_splits = (X_train, y_train, X_test, y_test) \n",
    "#     else:\n",
    "#         (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "#         X_train = np.concatenate(X_train, axis=0)\n",
    "#         y_train = np.concatenate(y_train, axis=0)\n",
    "#         X_validate = np.concatenate(X_validate, axis=0)\n",
    "#         y_validate = np.concatenate(y_validate, axis=0)\n",
    "#         X_test = np.concatenate(X_test, axis=0)\n",
    "#         y_test = np.concatenate(y_test, axis=0)\n",
    "#         concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "#     return concat_splits\n",
    "\n",
    "# def transpose_for_separable2d(splits, train_test_only=False):\n",
    "#     if train_test_only:\n",
    "#         (X_train, y_train, X_test, y_test) = splits\n",
    "#         X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "#         X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "#         transpose_split = (X_train, y_train, X_test, y_test) \n",
    "#     else:\n",
    "#         (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "#         X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "#         X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "#         X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "#         transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "#     return transpose_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def n_step_model_predictions(model_data, target_data, time_index, model_generator, frame_size, start_date, n_countries,\n",
    "#                              n_validation_frames, n_test_frames, predict_steps, f, k, epochs, learning_rate, batch_size,\n",
    "#                              train_test_only=False, Xy_truncation=None,verbose=0, train_or_test='train'):\n",
    "    \n",
    "#     \"\"\" wrapper for iteration loop \n",
    "    \n",
    "#     data : DataFrame of very specific make\n",
    "    \n",
    "#     model : one of my custom models, sequential_Conv1D_model, SeparableConv2D_model, parallel_Conv1D_model\n",
    "    \n",
    "    \n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     for n_days_into_future in predict_steps:\n",
    "#         X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "#         splits = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "#                                   train_test_only=train_test_only)\n",
    "        \n",
    "#         A_splits = concatenate_4d_into_3d(splits, train_test_only=train_test_only) \n",
    "#         if train_test_only:\n",
    "#             X_train, y_train, X_test, y_test = A_splits\n",
    "#             X_validate, y_validate = X_test, y_test\n",
    "#         else:\n",
    "#             X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "#         model = model_generator(X_train, f, k)\n",
    "#         model.compile(loss='mae', optimizer=Adam(learning_rate=learning_rate))\n",
    "#         # fit network\n",
    "#         history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "#                   batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "#         ### analysis\n",
    "#         if train_or_test == 'train':\n",
    "#             y_true = y_validate.ravel()\n",
    "#             y_naive = y[-(n_validation_frames+n_test_frames+n_days_into_future):-(n_test_frames+n_days_into_future), :].ravel()\n",
    "#             y_predict = model.predict(X_validate).ravel()\n",
    "#             # evaluate model\n",
    "#         else:\n",
    "#             y_true = y_test.ravel()\n",
    "#             y_naive = y[-(n_test_frames+n_days_into_future):-n_days_into_future, :]\n",
    "#             y_predict = model.predict(X_test).ravel()\n",
    "\n",
    "#     return y_test, y_naive, y_predict, model, history, A_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 1):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window) & \n",
    "                                (time_index > max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    # y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "                   \n",
    "    return splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_day_forecasting(data, n_test_days, n_days_into_future, col_transformer=StandardScaler(), n_prune=2, train_or_test='train'):\n",
    "    # X has already been shifted by n_days_into future, so the last day in X is predicting that last day + n_days_into_future. \n",
    "    #     y_naive = X.new_cases_weighted[X.time_index > X.time_index.max() - n_days_into_future]\n",
    "    n_countries = data.location.nunique()\n",
    "    chronological_data = data.sort_values(by=['time_index','location'])\n",
    "    # chronological_data = chronological_data[chronological_data.days_since_first_case >0].reset_index(drop=True)\n",
    "\n",
    "    X = chronological_data[(chronological_data.time_index \n",
    "                            <=  chronological_data.time_index.max() - n_days_into_future)].reset_index(drop=True)\n",
    "    y = chronological_data[(chronological_data.time_index  \n",
    "                            >= n_days_into_future)].loc[:, ['time_index','new_cases_weighted']].reset_index(drop=True)\n",
    "\n",
    "    train_indices = X[X.time_index <= X.time_index.max() - n_test_days].index\n",
    "    test_indices = X[X.time_index > X.time_index.max() - n_test_days].index\n",
    "\n",
    "    # 2 slices date, location, 4 slices date,location,time_index, days_since_first_case\n",
    "    X_train = X.iloc[train_indices, n_prune:]#.apply(lambda x : np.log(x+1))\n",
    "    y_train = y.loc[train_indices]\n",
    "\n",
    "    X_test = X.iloc[test_indices, n_prune:]#.apply(lambda x : np.log(x+1))\n",
    "    y_test =  y.loc[test_indices]\n",
    "\n",
    "    _ = col_transformer.fit(X_train)\n",
    "    X_train_normalized =  col_transformer.transform(X_train)\n",
    "    X_test_normalized =  col_transformer.transform(X_test)\n",
    "\n",
    "    n_step_models = []\n",
    "\n",
    "    model = Ridge(alpha=0.01, fit_intercept=False, tol=1e-12) \n",
    "    _ = model.fit(X_train_normalized, y_train)\n",
    "    \n",
    "    if train_or_test == 'train':\n",
    "        y_naive = X_train.new_cases_weighted\n",
    "        y_true, y_predict, mae = classifier_analysis(model, X_train_normalized, y_train, plot=False, metric='mae')\n",
    "    else:\n",
    "        y_naive = X_test.new_cases_weighted\n",
    "        y_true, y_predict, mae = classifier_analysis(model, X_test_normalized, y_test, plot=False, metric='mae')\n",
    "\n",
    "        \n",
    "    return  (X_train_normalized, X_test_normalized), (y_true, y_naive, y_predict), model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def normalize_Xy_splits(splits, feature_range=(0., 0.5), normalization_method='minmax',\n",
    "#                         train_test_only=False, feature_indices=None):\n",
    "#     \"\"\" Split into training, validation and test data.\n",
    "#     \"\"\"\n",
    "#     min_, max_ = (0, 0.5)\n",
    "#     (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "#     for i in range(1, X_train.shape[0]+1):\n",
    "#         # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "#         # each time step in the frame. \n",
    "#         up_to_current_frame_min = X_train[:i,:,:,:].min((0,1,2))\n",
    "#         up_to_current_frame_max = X_train[:i,:,:,:].max((0,1,2))\n",
    "#         latest_min_array = np.tile(up_to_current_frame_min[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "#         latest_max_array = np.tile(up_to_current_frame_max[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "#         if i == 1:\n",
    "#             frame_min_array = latest_min_array\n",
    "#             frame_max_array = latest_max_array\n",
    "#         else:\n",
    "#             frame_min_array = np.concatenate((frame_min_array, \n",
    "#                                                    latest_min_array)\n",
    "#                                                   ,axis=0)\n",
    "#             frame_max_array = np.concatenate((frame_max_array, \n",
    "#                                                    latest_max_array)\n",
    "#                                                   ,axis=0)\n",
    "\n",
    "#     # frame_min_array = np.tile(frame_min_array, (1, n_countries, 1, 1))\n",
    "#     # frame_max_array = np.tile(frame_max_array, (1, n_countries, 1, 1))\n",
    "\n",
    "#     minmax_denominator = (frame_max_array-frame_min_array)\n",
    "#     minmax_denominator[np.where(minmax_denominator==0)]=1\n",
    "#     X_train_scaled = (max_-min_)*(X_train - frame_min_array) / minmax_denominator\n",
    "#     # Use the latest min and max for test scaling.\n",
    "\n",
    "#     latest_minmax_denominator = latest_max_array - latest_min_array\n",
    "#     latest_minmax_denominator[np.where(latest_minmax_denominator==0)] = 1\n",
    "#     X_validate_scaled = (max_- min_)*((X_validate - np.tile(latest_min_array,(n_validation_frames,1,1,1)))\n",
    "#                                         / np.tile(latest_minmax_denominator,(n_validation_frames,1,1,1)))\n",
    "#     X_test_scaled = (max_- min_)*((X_test - np.tile(latest_min_array,(n_test_frames,1,1,1))) \n",
    "#                                         / np.tile(latest_minmax_denominator,(n_test_frames,1,1,1)))\n",
    "#     scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "\n",
    "#     train_norm_arrays =  (frame_max_array, frame_min_array, minmax_denominator)\n",
    "#     validate_and_test_norm_arrays = (latest_max_array,latest_min_array,latest_minmax_denominator)\n",
    "\n",
    "#     return scaled_splits, train_norm_arrays, validate_and_test_norm_arrays\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tile_shape = np.array(np.array(X_cnn_train.shape)/np.array(frame_denom.shape),int)\n",
    "# minmax_inverse_train = X_train - (((X_cnn_train / 0.5) * np.tile(frame_denom, train_tile_shape)) + np.tile(frame_min, train_tile_shape))\n",
    "\n",
    "\n",
    "# validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_validate = X_validate - (((X_cnn_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "\n",
    "\n",
    "# test_tile_shape = np.array(np.array(X_cnn_test.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_test = X_test - (((X_cnn_test / 0.5) * np.tile(latest_denom, test_tile_shape)) + np.tile(latest_min, test_tile_shape))\n",
    "# print(np.linalg.norm(minmax_inverse_train),np.linalg.norm(minmax_inverse_validate),np.linalg.norm(minmax_inverse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "# X_validate - (((X_cnn_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "# (latest_max, latest_min, latest_denom) = latest_minmax\n",
    "\n",
    "# print(X_regression_train.max(),X_regression_validate.max(),X_regression_test.max())\n",
    "\n",
    "# print(X_regression_train.shape,X_regression_validate.shape,X_regression_test.shape)\n",
    "\n",
    "# print(y_regression_train.shape,y_regression_validate.shape,y_regression_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tile_shape = np.array(np.array(X_regression_train.shape) / np.array(frame_denom.shape),int)\n",
    "# minmax_inverse_train = X_train - (((X_regression_train / 0.5) * np.tile(frame_denom, train_tile_shape)) + np.tile(frame_min, train_tile_shape))\n",
    "# np.linalg.norm(minmax_inverse_train)\n",
    "\n",
    "# validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_validate = X_validate - (((X_regression_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "# np.linalg.norm(minmax_inverse_validate)\n",
    "\n",
    "# test_tile_shape = np.array(np.array(X_regression_test.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_test = X_test - (((X_regression_test / 0.5) * np.tile(latest_denom, test_tile_shape)) + np.tile(latest_min, test_tile_shape))\n",
    "# np.linalg.norm(minmax_inverse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10000\n",
    "# learning_rate = 0.001\n",
    "# kernel = 4\n",
    "# N = 8\n",
    "# FC = 8\n",
    "# batch_size = X_cnn_train.shape[0]\n",
    "\n",
    "# f1, f2 = 64, 8\n",
    "# k1, k2 = 4, 4\n",
    "\n",
    "# cnn_model = Sequential()\n",
    "# # kernel_initializer0=RandomUniform(minval=0.0, \n",
    "# #                                  maxval= 1.0,\n",
    "# #                                  seed=0,\n",
    "# #                                 dtype=float)\n",
    "\n",
    "# cnn_model.add(Conv1D(filters=int(f1), kernel_size=int(k1),\n",
    "#                  padding='valid',\n",
    "#                  input_shape=X_cnn_train.shape[2:],\n",
    "#                  use_bias=False,\n",
    "# #                 kernel_constraint=non_neg(),\n",
    "# #                  activation='relu',\n",
    "# #                 kernel_initializer=kernel_initializer0\n",
    "#                 )\n",
    "#          )\n",
    "\n",
    "# # cnn_model.add(A/veragePooling1D(pool_size=2))\n",
    "# # cnn_model.add(Activation('relu'))\n",
    "# # kernel_initializer1=RandomUniform(minval=0.0, \n",
    "# #                                  maxval= 1.0,\n",
    "# #                                  seed=0,\n",
    "# #                                 dtype=float)\n",
    "\n",
    "# cnn_model.add(Conv1D(filters=int(f2), \n",
    "#                  kernel_size=int(k2), \n",
    "#                  padding='valid',\n",
    "#                  use_bias=False,\n",
    "# #                  kernel_constraint=non_neg(),\n",
    "\n",
    "# #                  activation='relu',\n",
    "# #                 kernel_initializer=kernel_initializer1\n",
    "# #                   use_bias=False\n",
    "#                 )\n",
    "#          )\n",
    "\n",
    "\n",
    "# # cnn_model.add(AveragePooling1D(pool_size=2))\n",
    "# # cnn_model.add(Activation('relu'))\n",
    "# cnn_model.add(Flatten())\n",
    "\n",
    "# # kernel_initializer2=RandomUniform(minval=0.0, \n",
    "# #                                  maxval= 1.0,\n",
    "# #                                  seed=2,\n",
    "# #                                 dtype=float)\n",
    "\n",
    "# cnn_model.add(Dense(cnn_model.output.shape[1], \n",
    "# #                 activation='relu',\n",
    "#                 use_bias=False,\n",
    "# #                  kernel_constraint=non_neg(),\n",
    "# #                 kernel_initializer=kernel_initializer2\n",
    "#                )\n",
    "#          )\n",
    "\n",
    "# # kernel_initializer=RandomUniform(minval=0.0, \n",
    "# #                                  maxval= 1.0,\n",
    "# #                                  seed=0,\n",
    "# #                                 dtype=float)\n",
    "\n",
    "# cnn_model.add(Dense(1, \n",
    "#                     activation='relu',\n",
    "#                     use_bias=False,\n",
    "# #                      kernel_constraint=non_neg(),\n",
    "# #                     kernel_initializer=kernel_initializer\n",
    "#                    ))\n",
    "# cnn_model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tile_shape = np.array(np.array(X_cnn_train.shape)/np.array(frame_denom.shape),int)\n",
    "# minmax_inverse_train = X_train - (((X_cnn_train / 0.5) * np.tile(frame_denom, train_tile_shape)) + np.tile(frame_min, train_tile_shape))\n",
    "\n",
    "\n",
    "# validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_validate = X_validate - (((X_cnn_validate / 0.5) * np.tile(latest_denom,validate_tile_shape)) + np.tile(latest_min,validate_tile_shape))\n",
    "\n",
    "\n",
    "# test_tile_shape = np.array(np.array(X_cnn_test.shape)/np.array(latest_denom.shape),int)\n",
    "# minmax_inverse_test = X_test - (((X_cnn_test / 0.5) * np.tile(latest_denom, test_tile_shape)) + np.tile(latest_min, test_tile_shape))\n",
    "# print(np.linalg.norm(minmax_inverse_train),np.linalg.norm(minmax_inverse_validate),np.linalg.norm(minmax_inverse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 nonneg\n",
    "\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,390.12602832774525]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.6970863460225101]\n",
    "\n",
    "# 2 nonneg, pool, relu\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,13586.662088]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.0]\n",
    "\n",
    "# 2 nonneg, pool\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,556.4749063236223]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.4443304826760618]\n",
    "\n",
    "\n",
    "# pool + only 1 nonneg and relu at very end. probably could use more than 1500 epochs\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,476.8907287109961]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5434063478478779]\n",
    "\n",
    "\n",
    "\n",
    "# negative, pooling, relu at very end\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,256.32871004595506]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.6796489686915346]\n",
    "\n",
    "\n",
    "# only nonneg and relu at very end.\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,138.01389460401262]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5543860897517913]\n",
    "\n",
    "\n",
    "# only relu at very end.\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,140.32871993958562]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5113786900493877]\n",
    "\n",
    "# 2 dense relu\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,216.5053047822524]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.4716586801209258]\n",
    "\n",
    "# 2 nonneg, 2 relu\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,396.11096463299015]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.6519816604307848]\n",
    "\n",
    "# average pooling, relu at very end.\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,426.28375726972956]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.6618604718747367]\n",
    "\n",
    "# elimination round\n",
    "\n",
    "\n",
    "\n",
    "# 2 nonneg and relu at very end.\n",
    "\n",
    "\n",
    "# only nonneg and relu at very end.\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,139.36171575910947]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5208973986926813]\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,138.42094987360994]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5469134697530336]\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,140.77506300920305]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5274845291582406]\n",
    "\n",
    "# only  relu at very end.\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,140.42387636891903]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5162934749412229]\n",
    "\n",
    "There were 0 negative predictions\n",
    "1-step MSE [Naive, CNN model] = [170.80669257142873,141.2069144587355]\n",
    "1-step R^2 [Naive, CNN model] = [0.2994526992709322,0.5047534149090478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_data = data.iloc[:,2:].copy()\n",
    "# new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "# n_countries = data.location.nunique()\n",
    "# target_data = data.new_cases_per_million\n",
    "# time_index = data.time_index.astype(int)\n",
    "# frame_size = 28\n",
    "# start_date = frame_size #+ time_index.min()\n",
    "# # start_date = 50\n",
    "# n_validation_frames = 7\n",
    "# n_test_frames = 1\n",
    "# n_days_into_future = 1\n",
    "# train_or_test = 'train'\n",
    "\n",
    "# frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "#                         (time_index >= max_date_in_window-frame_size)]\n",
    "# #     print(frame_data.shape)\n",
    "# # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "# reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "\n",
    "# X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "# n_features = X.shape[-1]\n",
    "# last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "# splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "# (train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "# (X_train, y_train, X_validate,\n",
    "#  y_validate, X_test, y_test) = splits\n",
    "\n",
    "# X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "# y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "# y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "# y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "# # y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# # y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# # y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()\n",
    "\n",
    "\n",
    "\n",
    "# flat_splits = flatten_Xy_splits(splits)\n",
    "# (X_regression_train, y_regression_train, X_regression_validate,\n",
    "#  y_regression_validate, X_regression_test, y_regression_test) = flat_splits\n",
    "\n",
    "# X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "# y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "# scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "# regression_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "# _ = regression_model.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The normalization is alot easier if I split into train, validate, test but this makes other parts more annoying\n",
    "# scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,1), \n",
    "#                                                                   normalization_method='minmax',\n",
    "#                                                                   train_test_only=False,\n",
    "#                                                                   feature_indices=None)\n",
    "\n",
    "# (latest_max, latest_min, latest_denom) = latest_minmax\n",
    "# (frame_max, frame_min, frame_denom) = frame_minmax\n",
    "\n",
    "# flat_splits = flatten_Xy_splits(scaled_splits)\n",
    "# (X_regression_scaled_train, y_regression_scaled_train, X_regression_scaled_validate,\n",
    "#  y_regression_scaled_validate, X_regression_scaled_test, y_regression_scaled_test) = flat_splits\n",
    "\n",
    "# X_regression_scaled = np.concatenate((X_regression_scaled_train, X_regression_scaled_validate),axis=0)\n",
    "# y_regression_scaled = np.concatenate((y_regression_scaled_train, y_regression_scaled_validate),axis=0)\n",
    "\n",
    "# scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "# regression_scaled_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "# _ = regression_scaled_model.fit(X_regression_scaled, y_regression_scaled)\n",
    "\n",
    "# y_predict_train = regression_scaled_model.predict(X_regression_scaled_train).ravel()\n",
    "# model_analysis(y_regression_scaled_train, y_train_naive, y_predict_train, n_countries,\n",
    "# title='Ridge', suptitle='Scaled predictor training set performance')\n",
    "\n",
    "# y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "# model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate, \n",
    "# n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')\n",
    "\n",
    "# y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "# model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate,\n",
    "# n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')\n",
    "\n",
    "\n",
    "# # The normalization is alot easier if I split into train, validate, test but this makes other parts more annoying\n",
    "# scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,1), \n",
    "#                                                                   normalization_method='minmax',\n",
    "#                                                                   train_test_only=False,\n",
    "#                                                                   feature_indices=None)\n",
    "\n",
    "# (latest_max, latest_min, latest_denom) = latest_minmax\n",
    "# (frame_max, frame_min, frame_denom) = frame_minmax\n",
    "\n",
    "# flat_splits = flatten_Xy_splits(scaled_splits)\n",
    "# (X_regression_scaled_train, y_regression_scaled_train, X_regression_scaled_validate,\n",
    "#  y_regression_scaled_validate, X_regression_scaled_test, y_regression_scaled_test) = flat_splits\n",
    "\n",
    "# X_regression_scaled = np.concatenate((X_regression_scaled_train, X_regression_scaled_validate),axis=0)\n",
    "# y_regression_scaled = np.concatenate((y_regression_scaled_train, y_regression_scaled_validate),axis=0)\n",
    "\n",
    "# scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "# regression_scaled_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "# _ = regression_scaled_model.fit(X_regression_scaled, y_regression_scaled)\n",
    "\n",
    "# y_predict_train = regression_scaled_model.predict(X_regression_scaled_train).ravel()\n",
    "# model_analysis(y_regression_scaled_train, y_train_naive, y_predict_train, n_countries, title='Ridge', suptitle='Scaled predictor training set performance')\n",
    "\n",
    "# y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "# model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')\n",
    "\n",
    "# y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "# model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')\n",
    "\n",
    "# train_or_test = 'train'\n",
    "# n_test_days = 1\n",
    "# n_days_into_future = 14\n",
    "# n_prune = 4\n",
    "# # X has already been shifted by n_days_into future, so the last day in X is predicting that last day + n_days_into_future. \n",
    "# #     y_naive = X.new_cases_weighted[X.time_index > X.time_index.max() - n_days_into_future]\n",
    "# n_countries = data.location.nunique()\n",
    "# model_data = full_data\n",
    "# # model_data = data.drop(columns=column_search(data, 'test'))\n",
    "\n",
    "# chronological_data = model_data.sort_values(by=['time_index','location'])\n",
    "\n",
    "# X = chronological_data[(chronological_data.time_index \n",
    "#                         <=  chronological_data.time_index.max() - n_days_into_future)].reset_index(drop=True)\n",
    "\n",
    "# y = chronological_data[(chronological_data.time_index  \n",
    "#                         >= n_days_into_future)].loc[:, ['time_index','new_cases_weighted']].reset_index(drop=True)\n",
    "\n",
    "# # X = X[(X.days_since_first_case > 0) & (X.time_index > 40)]\n",
    "# # X = X[(X.days_since_first_case > 0)]\n",
    "# # X = X[X.time_index > 40]\n",
    "\n",
    "# y = y.loc[X.index,:]\n",
    "# train_indices = X[X.time_index <= X.time_index.max() - n_test_days].index\n",
    "# test_indices = X[X.time_index > X.time_index.max() - n_test_days].index\n",
    "\n",
    "# pd.DataFrame(data[data.time_index.isin(list(range(7)))].values.reshape(-1, 7*24))\n",
    "\n",
    "# data.set_index('time_index')#.transpose()#[list(range(7))]#.transpose()\n",
    "\n",
    "# chronological_data.set_index('time_index').transpose()[list(range(7))].transpose()\n",
    "\n",
    "# chronological_data.set_index('time_index').transpose()\n",
    "\n",
    "# # 2 slices date, location, 4 slices date,location,time_index, days_since_first_case\n",
    "\n",
    "# X_train = X.loc[train_indices, :]#.apply(lambda x : np.log(x+1))\n",
    "# y_train = y.loc[train_indices,['time_index','new_cases_weighted']]\n",
    "\n",
    "# X_test = X.loc[test_indices,:]#.apply(lambda x : np.log(x+1))\n",
    "# y_test =  y.loc[test_indices,['time_index','new_cases_weighted']]#.values.ravel()\n",
    "\n",
    "# # if train_or_test == 'train':\n",
    "# y_train_naive = X_train.loc[:, ['time_index','new_cases_weighted']]\n",
    "# # else:\n",
    "# y_test_naive = X_test.loc[:, ['time_index','new_cases_weighted']]\n",
    "# X_train = X_train.iloc[:,n_prune:]\n",
    "# X_test = X_test.iloc[:,n_prune:]\n",
    "# col_transformer = MinMaxScaler()\n",
    "# _ = col_transformer.fit(X_train)\n",
    "# X_train_normalized =  col_transformer.transform(X_train)\n",
    "# X_test_normalized =  col_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_size = 7\n",
    "# folds = [X.date.unique()[fold_size*i:fold_size*(i+1)] for i in range(1 + len(X.date.unique())//fold_size)]\n",
    "# folds_indices = [X[X.date.isin(fold)].index for fold in folds]\n",
    "# folds_series = pd.Series(folds_indices).to_frame(name='folds')\n",
    "\n",
    "# fold_indices = folds_series.values.flatten()[:-2]\n",
    "# test_indices = folds_series.values.flatten()[-2:]\n",
    "# test_indices = test_indices[0].append(test_indices[1]).sort_values()\n",
    "\n",
    "# train_indices = fold_indices[0]\n",
    "# for indices in fold_indices[1:]:\n",
    "#     train_indices = full_index.union(indices)\n",
    "\n",
    "# for i, epochs in enumerate(folds_series.values.flatten()):\n",
    "#     start, end = model_data.loc[epochs, 'date'].min(), model_data.loc[epochs, 'date'].max()\n",
    "#     print('Epoch {} spans the dates {} to {}'.format(i,start,end))\n",
    "\n",
    "# start, end = model_data.loc[train_indices, 'date'].min(), model_data.loc[train_indices, 'date'].max()\n",
    "# print('Training data spans the dates {} to {}'.format(start,end))\n",
    "\n",
    "# start, end = model_data.loc[test_indices, 'date'].min(), model_data.loc[test_indices, 'date'].max()\n",
    "# print('Testing data spans the dates {} to {}'.format(start, end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
