% pdflatex blog; bibtex blog
%\input{inputs/inclOnly}        % process only the file you are editing

% siminos/spatiotemp/blog.tex  pdflatex blog; biber blog
                        %% logical setup, no need to edit %%%%%%%%%%
                        \newif\ifpaper \newif\ifPDF               %%
                        \newif\ifOUP \newif\ifboyscout            %%
                        \newif\ifdasbuch \newif\ifarticle         %%
                        \newif\ifsolutions                        %%
                        \newif\ifsvnmulti			              %%
%                         \svnmultitrue %% turn on svn-multi	      %%
                        \dasbuchtrue %% DasBuch, not QFT lectures %%
                        \boyscouttrue %% commented, WWW/boyscouts %%
                        \solutionstrue %% include solutions       %%
                        \paperfalse\PDFtrue %% hyperlinked        %%
                        \OUPfalse \articlefalse  %% ChaosBook %%%%%%

\documentclass[letter,10pt,openany]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bm}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{alltt}
\usepackage{ifthen}
\usepackage{float}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}     % was [latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{palatino,lettrine}  %first letter of chapters, Gossens ed 2, p.99
\usepackage{dsfont}
\usepackage[pdftex]{graphicx}
\usepackage[nooneline]{caption} % 2011-07-23 consider switching to floatraw
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{array}
%\usepackage{eufrak} % TEMPORARY, used by L\'{o}pez\rf{Lopez2015}
\usepackage[pdftex,colorlinks]{hyperref}
\usepackage{verbatim}
\graphicspath{{../figs/}}  %% directories with graphics files
      %% page layout, ChaosBook environments
%\input{../inputs/biblatex}

\input{../latexinputs/editsDasbuch}   %% editing comments, DasBuch style
\input{../latexinputs/def}            %% do not edit; update from dasbuch/book/inputs/def.tex
\input{../latexinputs/defsSpatiotemp}

\hypersetup{
   pdfauthor=Matt Gudorf
   pdfkeywords=network science,
   pdftitle=CS 7280: Network Science}

\begin{document}

\title{CS 7280 Project Progress Report 1}
\author{Matt Gudorf}
\date{September 15th, 2019} \Private{\date{\today}}

\maketitle
%\thispagestyle{empty}
%\newpage
%\tableofcontents
%\newpage

%\section{requirements}

\subsection{Proposal}

\subsection{Introduction}
    Large datasets and computational power are more available today than
ever before. Ironically, a common problem is not knowing what to calculate
or use the data for. In light of this it's important to develop
techniques that can distill and tease out important features from complicated
data.
Machine learning seems to fill this role perfectly and such methods have come
into very high demand. It is beneficial, therefore, to produce representations
of data which can better leverage machine learning; this is referred to
as ``representation learning of graphs'' \cite{RepLearning}.
Techniques which reduce the intrinsic dimensionality are especially
desirable. Simple linear techniques perform poorly
when applied to complex nonlinear datasets \cite{Maaten08}. A simple
explanation is that these techniques
do not utilize intrinsic properties of the data. For this project I hope
to address both of these points by applying a combination
of algorithms to extract persistent topological
information from network datasets. Specifically, I will investigate
network's ``community structure'' by first
mapping the network data to a continuous vector space and then
computing the persistent homology of the transformed data.
I anticipate some head shaking at this moment; there are no doubt
innumerable community detection algorithms which are effective and efficient;
the difference lies in what specifically is meant by ``communities''.
Instead of the typical notion of having a single centrality metric
determined partition, this method will
find a hierarchy of communities
of different ``dimension'' which can hopefully answer
the question as to whether there is any hidden structure in the network.

\subsection{Motivation}
Persistent homology, a form of topological data analysis,
has been somewhat trendy tool for data exploration which provides topological
information. This method is also becoming fashionable in the study of dynamical
systems (nonlinear PDEs) as a tool for detecting ``coherent structures''
\cite{ComplexNetworks}
(hurricanes are a good example; they persist in the atmosphere and have a well defined
shape). It has also been used in the context of social networks (swarms) to
detect structure in seemingly unstructured biological data \cite{Topaz}.
I currently study dynamical systems, I am training for a career in data
science, and the method fits well with graph theory.
It seemed a relatively natural choice to implement this method and
see what it is capable of. In order to apply this method, however, I need to first
transform the data which is the reason for the representation learning portion of the
algorithm. This computation is in the spirit of community detection; I
do not fully comprehend the implications of hubs, communities, etc. but I imagine
that any structure that distinguishes the network apart from a randomized
graph is likely important.
\subsection{Methodology}
The data required should only be an undirected, unweighted network (a requirement
to use \texttt{DeepWalk}). I am unsure as to the performance of these separate
packages with regards to memory so I believe it is prudent to apply this to multiple
network datasets. The output of the \texttt{DeepWalk} algorithm will be a representation
of the network in a continuous vector space.
An example of the mapping of network data to a continuous
vector space is given in \reffig{ZKC}, where the network data
of ``Zachary's karate club'' is mapped to points in
$\mathbb{R}^2$. The figure is colored
to suggest that the communities of the network are mapped to clusters
of data points (in terms of Euclidean distance).
\begin{figure}[t]
\begin{minipage}[height=.5\textheight]{.9\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{ZKC}
\end{minipage}
\caption{\label{ZKC}
Network visualization of ``Zachary's karate club'' (left) and
the representation learned (right) by application of the DeepWalk algorithm
\cite{DeepWalk}.
}
\end{figure}
Once the data is mapped to a continuous vector space,
we can compute the persistent homology of the new data.
This process can be handled by
either \texttt{GUDHI}\cite{GUDHI} or \texttt{Perseus} \cite{Perseus}
which are both
publicly available packages.
There is a lot of formal mathematics behind persistent homology \cite{Mischaikow} but
I will attempt to present the ideas in a manner which is actually understandable.
Persistent homology detects the
number of $k$ dimensional holes (i.e. circles, 2-tori, etc.) and
connected components in a data set. Put very plainly it
combines a thresholding procedure with a rule for how to connect
data points. An example is the Vietoris-Rips complex displayed in
\reffig{rips}. Treat each data point as the center of a circle. Slowly
increase the radii uniformly; this is the thresholding procedure. The ``rule''
is that if a pair of circles overlap an edge is added between their
circles. When three circles collectively overlap the
corresponding triangle is filled in, for four
circles its a ``tetrahedron''.
When scanning through the threshold values the existence of structures,
determined by the threshold values at which they are born and at which they die, are tracked
and the structures that \textit{persist} over a large range of values
are deemed to be the important ones. For example, a one dimensional hole would
be a circle formed by edges. It is born when the circle
is first formed by edges, it dies when it is
completely filled in.
\begin{figure}[t]
\begin{minipage}[height=.5\textheight]{.9\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{rips}
\end{minipage}
\caption{\label{rips}
An example of the Vietoris-Rips simplicial complex at a particular
threshold value (circle radius). \cite{Topaz}
}
\end{figure}

\subsection{Production timeline}
The work flow of this project is given by the following list. A step
that is not included but is pertinent is to derive a null hypothesis
with which the results of my investigation can be compared.
\begin{enumerate}
\item Parse and convert data to format required by \texttt{DeepWalk}
\item Learn the basics of the \texttt{DeepWalk} API
\item Produce learned representation data via \texttt{DeepWalk}
\item Parse and convert data to format required by \texttt{Perseus} or \texttt{GUDHI}
\item Learn the basics of persistent homology package API
\item Analyse and visualize the persistent homology data.
\item Compare the results with a null hypothesis or some other
means of validation
\end{enumerate}
I'm unsure if this seems like a lot but I think it is manageable because I am
not writing any of the algorithms myself. I have also already been exposed to
the ideas of persistent homology so I should not have too much trouble
understanding the results; interpreting them is another story.

\subsection{Literature review}
I am not focused on testing the efficacy of \texttt{DeepWalk} so I will
refrain from discussing their results. In terms of persistent homology (topological data
analysis) I think this in line with work that has been done in \cite{Topaz}, \cite{ComplexNetworks}.
In all honesty it seems that the tool is becoming popular because of how many problems it
can be applied to. Essentially whenever your data can be described as a cloud of points
in some space then persistent homology can be applied (using a simplicial complex). If the data
is of a different type then a different complex would likely be needed. While this method
has an intimate relationship with graphs, I have not actually seen it applied to an actual network.
I'm assuming this is because it seems to rely on the notion of Euclidean distance; in my limited
experience, most network data is not spatially ordered in this manner.
Therefore I think my proposal is a unique but straightforward combination
of ideas already used in the literature.


\section{Final Report}


\section{Introduction}

Networks, graphs consisting of nodes and edges, are one of the
most fundamental mathematical constructions for modeling
the real world. It should be no surprise then that machine
learning is applied to networks for an enumerable number of
reasons. The format of the data required is typically described
via a two dimensional array (matrix) where the rows represent
observations and the columns represent features. In this sense, machine learning
is an abstraction of systems to a vector space (typically of high dimension).

One manner in which to transform network data to fit this form is
to perform what is known as representation learning which maps
the nodes into a vector space where spatial proximity of nodes
is demonstrative of adjacency in the network.
Therefore, the clustering of points in this vector space should be
representative of the community structure of nodes in the network.

The goal of this study is to investigate the spatial ordering
of the transformed network, specifically with the tools of persistent homology.
The specifics of persistent homology calculations can vary but the general
idea is to find the persistent events of a filtration of a simplicial complex.
In layman's terms, the simplicial complex is a rule for how to add simplicies (line segments)
to the graph based on the filtration value (threshold value). The simplicial complex
used in this investigation is the Vietoris-Rips simplicial complex. In this instance,
a sphere whose radius is the filtration value is centered on each data point.
The dimension of the spheres are the same dimension as the vector space.
As the filtration value is increased, a simplex is added between the centers
of spheres whenever the surfaces intersect with a center. At this moment we
say that a simplex is birthed. Analogously, a simplex "dies" when it disappears,
that is, when it becomes a component in a higher-dimensional simplex (i.e. when
three edges combine to make a triangle, the three edges "die" and a triangle is born).
\begin{figure}[H]
\centering
\begin{minipage}[height=.5\textheight]{.5\textwidth}
\includegraphics[width=1.0\textwidth]{deepwalk_2d_rips_circles}
\end{minipage}
\caption{\label{deepwalkcircles}
(a) The colored representation but instead of having network edge
data imposed onto it, the edges of the graph now represent simplices of the skeleton of the topological space at a particular, arbitrary filtration value. (b) The same as (a),
but with the radii of the circles (how the edges are determined)
superimposed. Circles with overlap other nodes indicate that an edge will be added.
}
\end{figure}
The idea behind \textit{persistent} homology is to find the simplicies that
have the longest lifetimes, i.e. the ones that \textit{persist}. The combination
of these two ideas sets up this project, which looks at how they can be combined for community
detection.

\section{Methodology}

This project focuses on community structure determined by persistent homology
and as such we shall avoid anything but an incredibly brief description of
the various open source code used.
Firstly, for the hypothesis testing utilizes randomized graphs which preserve
the degree sequence and the joint independent edge degree and triangle degree sequence,
respectively. Specifically we use the NetworkX "configuration model" and "random clustered graph"
graph generators to generate a collection of one hundred random graphs of each type.
The algorithms we will be comparing to are  the Clauset-Newman-Moore
greedy modularity algorithm \cite{greed} and the Louvain method
\cite{louvain}. These are used to produce two null hypotheses with which to test
our hypotheses.

The next component of this method is to learn representations of these randomized graphs.
The representation learning algorithm used here is called
DeepWalk; it utilizes random walks to learn a latent representation of the network \cite{DeepWalk}.
Examples of projections of the learned representations
with each network's edges included are displayed in \reffig{deepwalkwithedges}.

\begin{figure}[H]
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{deepwalkwithedges}
\\ \small{\texttt{(a)}}
\end{minipage}
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{deepwalkwithedgesr}
\\ \small{\texttt{(b)}}
\end{minipage}
\centering
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{deepwalkwithedgesrc}
\\ \small{\texttt{(c)}}
\end{minipage}
\caption{\label{deepwalkwithedges}
Plots of learned representation with original network edges.
(a) Original network data, (b) randomized graphs, (c) randomized clustered
Colored by greedy modularization community detection.
}
\end{figure}

The persistent homology package used is called Gudhi; this project specifically uses
the Vietoris-Rips simplicial complex to compute the filtration and persistent homology. An example
of the filtration of the simplicial complex at a particular filtration value is displayed
in \reffig{deepwalkcircles}
For the persistent homology calculation, we limit the maximum dimension of
the simplicial complex to four (a four dimensional simplex, i.e. a 4D triangle) and
the maximum filtration value to the 25th quantile of the distribution of pairwise
distances of the nodes. The reason for this is that the number of simplicies grows
very quickly for larger filtration values.

The partitions are generated in the following manner. First, the filtration of the
simplicial complex is computed for some less-than-maximal filtration value. This choice is motivated
by the bimodal distribution; if there were many different scales and hierarchies of communities
the distribution is believed to be $n$-modal (a scale free network) we would have to be more careful.
The upper bound on the filtration value is taken to be
at most the 10th quantile of the pairwise distances ($L_2$ norm) between the nodes in the
vector space. This attempts to capture only connections to only close neighbors.
Using this as an upper bound, we scan through the range of possible filtration values
and take the set of simplicies of highest dimension that produce the best value of our quality
metric. The reason
for using the highest dimension simplicies is that the entire graph quickly becomes connected by the
1-simplicies and hence has no disjoint components.  It is this
maximum that we shall consider the result of the method.
This method is applied to the collection of randomized
graphs. This produces a pair of distributions of quality values (performance plus coverage)
of the randomized graphs.
There are a large number of parameters which we could
explore with but typically everything other than
the dimension of the learned representation is of fixed value.

The metric used here for comparisons of community detection of networks
is the sum of the "performance" and "coverage" metrics \cite{qualitymetric}.
The performance is defined as
the ratio of the number of intra-community edges plus inter-community with the total
number of potential edges and the coverage is the ratio of the number
of intra-community edges to the total number of edges in the
graph. Towards these ends, the null hypothesis shall be that
\textit{Partitions created with persistent homology do not have higher quality than greedy algorithm
of Louvain method partitions.}

In summary the general process is as follows.
\begin{itemize}
\item Learn representations of randomized network data with DeepWalk
\item Compute the filtration of the Vietoris-Rips simplicial complex with Gudhi.
\item With the filtration of the simplicial complex, scan through filtration
values to find partition of maximal quality.
\item Create a distribution of these values and compare to a distribution of greedy modularity
algorithm and Louvain algorithm quality values.
\item Compute statistics
\end{itemize}

\subsection{Results}

To explore the persistent homology, the density
of the simplicies' births are plotted in \reffig{fig:simplices}. This is a distribution
of when each 1-simplex gets added to the filtration.
\begin{figure}[H]
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{1simplicies}
\\ \small{\texttt{(a)}}
\end{minipage}
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{1simpliciesrandom}
\\ \small{\texttt{(b)}}
\end{minipage}
\centering
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{1simpliciesrandomclustered}
\\ \small{\texttt{(b)}}
\end{minipage}
\caption{\label{simplices}
Distribution of 1-simplicies for each of the learned representetations.
(a) Original network data, (b) randomized graphs, (c) randomized clustered
graphs.
}
\end{figure}

As can be seen, the distribution of 1-simplex births
as a function of filtration value for the original network data is a bimodal distribution
while the others are unimodal. Roughly speaking,
 for a spatially homogeneous distribution of points
a unimodal distribution would be expected. If spatially homogeneous,
then as the filtration value is increased the number of simplicies should increase in turn as the
density is constant. Why then are we presented with a bi-modal distribution?
The reason is that the spatial distribution is not uniform.
In fact it can be explained by the presence of clustering.
Qualitatively, the number of simplicies
increases until the intra-cluster size is reached,
 at which point the empty spaces between clusters
implies a decrease in the density until the filtration
value reaches the inter-cluster distance. We can actually use
persistent homology to demonstrate this.
In \reffig{barcode}, three barcode diagrams are plotted, one
for each type of network (real, random, random clustered). The
barcode diagrams show the lifetime of
different simplicies, colored by different dimension. As can be seen, for the actual network there is an "island" of persistent events around a filtration value of 1.1.
This is indicative of the intra-community connections previously mentioned.
This argument is relatively informative, and the same reasoning should generalize to higher dimensions, but this is not the focus for now.
\begin{figure}[H]
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{barcode}
\\ \small{\texttt{(a)}}
\end{minipage}
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{barcoder}
\\ \small{\texttt{(b)}}
\end{minipage}
\begin{minipage}[height=.5\textheight]{.48\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{barcoderc}
\\ \small{\texttt{(b)}}
\end{minipage}
\caption{\label{barcode}
The persistence barcode diagrams for three example graphs (a) the original network,
(b) randomized graph, (c) randomized clustered graph.
}
\end{figure}

Unfortunately, the lack of a metric for measuring these higher order communities means that
not much regarding the significance of these higher order clusters can be mentioned. The other issue is that while the idea is enticing, these higher dimensional simplicies are by definition dependent
on the one dimensional simplicies, hence the focus on 1-simplicies for now.
The bimodal distribution indicates that we need not look at the entire filtration of simplicies
to detect structure (in fact looking at the total collection would be meaningless as every node would
be collected to every other node). The idea that is proposed here is to partition the nodes
by finding an upper bound on the filtration value which partitions the nodes but also maximizes some metric.

\begin{figure}[H]
\begin{minipage}[height=1.0\textheight]{1.0\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{partition3dedges}
\end{minipage}
\caption{\label{deepwalk_2d_single}
A three dimensional project of a network partition (colored by nodes) created
with the persistent homology method. The edges in this plot are the simplicies from the
filtration to show how the nodes were grouped
}
\end{figure}

The distribution of the quality values for various dimensions of the learned representation are plotted
in \reffig{quality}.
\begin{figure}[H]
\begin{minipage}[height=.5\textheight]{.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{random_dist}
\\ \small{\texttt{(a)}}
\end{minipage}
\begin{minipage}[height=.5\textheight]{.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{random_clustered_dist}
\\ \small{\texttt{(b)}}
\end{minipage}
\caption{\label{quality}
Distribution of the quality (performance plus coverage) for each
type of randomized graph and for each learned representation dimension.
}
\end{figure}

\subsection{Statistical Inference}
The mean values of the distributions in \reffig{quality} will be compared to the
distribution of quality values produced by
applying the greedy modularity metric to the same randomized graphs. These statistics
are produced for a range of learned representation dimension sizes.
The distribution of quality values resulting from both the greedy modularity algorithm
and persistent homology are approximately gaussian and so z-statistics were used for hypothesis testing.
Specifically, first the mean of the sample (for each dimension) the p value is calculated
by conversion of the z-scores
\begin{equation}
z_n(\bar{x}) = \frac{\bar{x} - \mu_g}{\sigma_g}
\end{equation}

The p-values corresponding to the mean of each distribution are essentially equal to zero or one.
For the random clustered graph data, this does not change even when the statistics are computed
for the 5th quantile of the quality. This indicates that almost certainly this algorithm will
perform better than the greedy modularity algorithm when applied to random clustered graphs.
The sample means and confidence intervals are displayed in \reffig{confidence}.
\begin{figure}[H]
\begin{minipage}[height=.7\textheight]{.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{confidencegreed}
\\ \small{\texttt{(a)}}
\end{minipage}
\begin{minipage}[height=.7\textheight]{.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{confidencelouvain}
\\ \small{\texttt{(b)}}
\end{minipage}
\caption{\label{confidence}
The mean values and confidence intervals plotted with the addition of the
mean of the greedy and Louvain algorithm distributions
}
\end{figure}
\subsection{Conclusions}
Put concisely, this method for community detection really fails (accepted null hypothesis)
for  the randomized graphs generated by the configuration model but really works (rejected
null hypothesis) for the
randomized graphs generated by the random clustered graph method.
A quick caveat, looking at \reffig{quality} it almost seems like I have accidentally swapped
the data for the persistent homology calculations' randomization types. I ensure you that
this is not the case and that this figure is accurate.
The reason
for this is believed to be the fact that a randomized graph has a more uniform spatial
distribution of points which is essentially the worst case scenario for persistent homology.
Rather, it is an example of when persistent homology really has no uses.
For the random clustered graphs, it seems this preservation of the triangle sequence corresponds
to spatial clustering in the learned representation, such that the persistent homology
calculation yields good results. Interestingly, for the original data, the greedy modularity
algorithm is essentially on par with the simplicial complex method. The reason
for the discrepancy when applied to randomized networks is believed to be because of what modularity
is actually defined as, which is the fraction of edges that fall within the given group
minus the expected number if it were distributed at random. indicating that the
persistent homology calculation is truly better in the presence of clustering

This investigation implicitly only looks for clustering of neighbors so
the original goal of finding hierarchical clustering of various dimensions has not
been obtained, but it seems that persistent homology in the scope of community detection has
some merit.

%\begin{figure}[t]
%\begin{minipage}[height=.5\textheight]{.48\textwidth}
%\centering
%\includegraphics[width=1.0\textwidth]{nsimplicies}
%\\ \small{\texttt{(a)}}
%\end{minipage}

%\caption{\label{nsimplicies}
%Distribution of all simplices of the original network data stratified by
%their dimension.
%(a) Original network data, (b) randomized graphs, (c) randomized clustered
%graphs.
%}
%\end{figure}

While I tried a large number of things I became too obsessed with visualization
such that I think the actual science and investigations suffered. I believe I was too
concerned about making figures that were really interesting rather than the
actual core science. I regret this and I believe that there is still many more interesting
things to learn and discover.




%\section*{References}
    % from ctan.org/tex-archive/biblio/bibtex/contrib/iopart-num/ :
\bibliographystyle{iopart-num}       % APS-like style for physics
\bibliography{../bibtex/networkscience}

%    \ifboyscout
%    \clearpage %\newpage
%\input{inserts}
%    \clearpage %\newpage

\end{document}