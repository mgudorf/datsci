{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform, RandomNormal\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prototyping\n",
    "\n",
    "# Table of contents\n",
    "<a id='toc'></a>\n",
    "## [Introduction](#intro)\n",
    "## [Data](#data)\n",
    "## [COVID-19 Case number modeling](#model)\n",
    "## [Fully connected Neural Network](#NN)\n",
    "## [Convolutional Neural Network](#CNN)\n",
    "## [Ridge Regression](#regression)\n",
    "## [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id='intro'></a>\n",
    "The goal of this notebook is to get a feel for the performance of the two different types of models to be trained.\n",
    "Specifically, comparison will be made between a naive baseline model, a ridge regression model, and a convolutional neural network model. Because this notebook is simply prototyping, a very small subset of the available data will be used; a single \n",
    "feature's time series for a single country, the United States. This single feature is all that will be available to the CNN and Ridge regression models; because eventually the feature data used in the regression is much larger than that used in the CNN training, this might be an unfair comparison.\n",
    "\n",
    "The main issue for using a very small subset of data is that I believe it will negatively affect the neural networks more than the regression, but this notebook is to mainly get a feeling for the Keras API and how the data must be formatted.   \n",
    "I debated between using MAE vs. MSE but decided it would be better to account for outliers during the preprocessing phase rather than using MAE. I use ridge regression as opposed to say, Lasso, because there will not be many features used and so the added benefit of feature selection via the $L_1$ norm is really as useful. Usage of the non-default scoring for Ridge means I have to use RidgeCV, which means I need to provide my own folds in order to respect time ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_slice(data, locations):\n",
    "    # used as convenience function with pandas\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    # used as convenience function with pandas, used to slice custom time index\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    # plot time series for all countries\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    # plot Series for all times (not useful, I meant to do something else)\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    # Due to redundant data and large number of columns, remembering the\n",
    "    # exact names can be a chore. \n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    # plot the residuals\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=20,alpha=0.7)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_frame in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        frame_data = model_data[(time_index <= max_date_in_frame-1) & \n",
    "                                (time_index >= max_date_in_frame-frame_size)]\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_frame == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_frame-frame_size, max_date_in_frame - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_frame-frame_size, max_date_in_frame - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def flatten_Xy_splits(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict, n_countries, title='',suptitle='',figname=None, scale=None, s=None):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    # compute scores \n",
    "    mse_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_naive, r2_predict))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    \n",
    "    ax1.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_naive,3)),fontsize=14)\n",
    "    ax1.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_naive,3)),fontsize=14)\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "             \n",
    "    ax2.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_predict,3)),fontsize=14)\n",
    "    ax2.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_predict,3)),fontsize=14)\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "\n",
    "    \n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    ax3.set_title('')\n",
    "    ax4.set_title('')\n",
    "    ax3.set_ylabel('Residual')\n",
    "    ax4.set_ylabel('Residual')\n",
    "    ax3.grid(True)\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    if figname is not None:\n",
    "        plt.savefig(figname, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    Normalize with respect to some absolute max, just choose 2*absolute max of training set. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "global_data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prototyping of the neural networks I will only use a single time series, new cases per million, for a single country, the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = global_data[global_data.location=='United States'].reset_index(drop=True)\n",
    "data = data.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "<a id='data'></a>\n",
    "\n",
    "There are five main steps in the data preprocessing stage.\n",
    "    \n",
    "    1. Feature selection\n",
    "    2. Creation of time frames\n",
    "    3. Splitting the data into different sets for validation\n",
    "    4. Rescaling of the data\n",
    "    5. Reshaping to abide by keras/scikit-learn conventions \n",
    "\n",
    "### 1. Feature Selection\n",
    "\n",
    "This does not really play a role in this notebook because I am prototyping using only a single feature for the neural networks. For the regression this is different and so I will explain my rationale in regards to feature selection. I do not use quantitative means for feature selection (No recursive feature elimination, PCA, etc.) rather I use a qualitative argument for my choices, but I will leave that for the main model creation notebooks.\n",
    "\n",
    "### 2. Creation of time-frames\n",
    "\n",
    "The data will be formatted in the following manner for both the neural networks and the regression models. \n",
    "The original table format of the data is that each row of the table ```data``` (DataFrame, technically) contains\n",
    "a single day's information for a single country. In order to better suit the time series nature of the features utilized, I will increase the dimension (columns) of each sample by concatenating multiple days worth of data. Specifically, I format the data such that each row is comprised of 28 days of feature day for a single country. Hereafter I refer to this concatenation of \n",
    "multiple time steps as 'frames' of time, or simply time frames. These can be thought of as sliding a step-function window across the feature data to create a collection of time frames. In the splitting of the data into different sets, I identify each frame by its most recent date. \n",
    "\n",
    "To help the reader understand this concept, I include a hand drawn image of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"time_frames.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over possible leading window dates a 4-d tensor with dimensions given by the following is created:\n",
    "\n",
    "```X.shape = (n_frames, n_countries, n_time_steps, n_features)```\n",
    "\n",
    "\n",
    "### 3. Splitting the data into train, validate, test.\n",
    "Now, splitting X intro train, validate, test, is as easy as slicing the first axis. \n",
    "Going backwards in time, the testing set contains the next to last most recent day's worth of values. It's not\n",
    "the last day because the last day has no \"future\" with which to compare, and so no worthwhile prediction can be\n",
    "made. The test set is defined by slicing X for its last row(s) (plural if n_test_frames > 1). The validation set that will \n",
    "be used is the next 7 days worth of values and the training data is everything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras API expects a three dimensional tensor, and so after the slicing/partitioning into different sets, the first axis\n",
    "is concatenated such that the effect dimensions are ```(n_frames*n_countries, n_time_steps, n_features)```.\n",
    "This is done via the function ```flatten_Xy```. Technically, when initializing the model it expects a 2-d shape. This is because the data is organized as ```(batch_size, time_steps, features)```. In other words, when initializing the model, one gives the api a shape of a single sample. This is visualized in the image below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"3d_tensor_input.jpg\" width=\"300\" height=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Rescaling\n",
    "\n",
    "The fourth preprocessing step is to rescale the data, For this purpose, I will use MinMax scaling, except instead of using the maximum values in the training set I instead create some arbitrary maximum and\n",
    "use this to rescale values to the interval [0, 1] (such that the fictitious maximum equals 1, I elect to use twice the maxima of the training sets). The use of this arbitrary maximum is to account for the possibility of future growth. \n",
    "The reasons why MinMax scaling is used: all of the independent variables take non-negative values and so maintaining this property helps interpretability. The numerical reason is because of how neural networks typically respond negatively to wide ranges of scales, because it can make weights large in magnitude.   \n",
    "\n",
    "### 5. Reshaping\n",
    "\n",
    "And finally, the last preprocessing step is to reshape the data to abide by the various conventions.\n",
    "For the Keras API, the (1d) convolutional layers expect a tensor of the shape (trying to stick to their notation)\n",
    "\n",
    "```Conv1d_input_shape = (batch_size, time_steps, n_features)``` .\n",
    "\n",
    "In this instance, batch size represents a collection of samples: the individual time frames for each country. Therefore if it helps, the batch is a collection of time frames, even one iterated through the batch sequentially, it would produce a collection of slide shows, one for each country. This can be achieved by simply collapsing the first axis such that \n",
    "\n",
    "```(n_frames*n_countries, n_time_steps, n_features)``` is the final shape of a 3-d tensor.\n",
    "\n",
    "\n",
    "For the Ridge regression we must go one step further, as it expects a matrix, or 2-d array, for the set of predictors. Each\n",
    "frame will represent a row of this matrix, such that by combining the last two axes, we get the correct form. In other words, the final shape is to be \n",
    "\n",
    "```(n_frames*n_countries, n_time_steps*n_features)``` is the final, two-dimensional shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Case number modeling\n",
    "<a id='model'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "# 1. Fully connected neural network model\n",
    "<a id='NN'></a>\n",
    "For the purpose of this project I will maintain a constant architecture for the neural networks and mainly perform tuning via the parameters therein. Investigations of new architectures are left for future work. The neural network architecture is drawn in the following figure.\n",
    "\n",
    "<div>\n",
    "<img src=\"nn.jpg\" width=\"600\" height=\"300\">\n",
    "</div>\n",
    "\n",
    "The output is a single number, the predictive output for the number of new cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single out the only feature used for the prototyping.\n",
    "model_data = data.copy().loc[:, 'new_cases_per_million'].to_frame()\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "model_data = model_data.iloc[:, new_cases_index].to_frame(); new_cases_index=0\n",
    "\n",
    "# query some parameters to be used for convenience and formatting.\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "# get the start date for the frames, and the frame size\n",
    "frame_size = 28\n",
    "start_date = frame_size + data.time_index.min()\n",
    "\n",
    "# determine the size of the splits \n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 154 181\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the data that will be used for training my prototype models, there are:\n",
      "(155, 1, 28, 1)\n",
      "A total of 155 frames\n",
      "A total of 1 countries\n",
      "A total of 28 time steps in each frame\n",
      "A total of 1 features\n"
     ]
    }
   ],
   "source": [
    "print('In the data that will be used for training my prototype models, there are:')\n",
    "print(X.shape)\n",
    "print('A total of {} frames'.format(X.shape[0]))\n",
    "print('A total of {} countries'.format(X.shape[1]))\n",
    "print('A total of {} time steps in each frame'.format(X.shape[2]))\n",
    "print('A total of {} features'.format(X.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the data \n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_nn_train, y_nn_train, X_nn_validate, y_nn_validate, X_nn_test, y_nn_test) = scaled_splits\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data has been split correctly and is aligned with the original values, I plot the scaled data's splits based on color, as well as the original supposed order. The order of the values is correct if the original values are black circles directly behind the colored train-validate-test splits circles'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4cc5689e6c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m ax.scatter(range(len(scaled_splits[0])+len(scaled_splits[2]), len(X)), \n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mscaled_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musa_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_cases_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m            , s=30, label='test')\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2sAAAEvCAYAAADb41nDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df3Rkd3nn+c+3pCup1VKZnmnTtLvtMQi3Z01W2EHjJszZwSTOoUXGTXJcxBBLC044alvGZjWdJbCzJ5mwm5OsZzVK7GbBoGAYCeIBtZltJ5bZMCG7Z45x222ClbEZd7oYfshujDsBqWS1pFvSs39IJZfU9eOqft1bVe/XOT7uunW76lt9dUv3ud/n+zzOzAQAAAAAiJZY2AMAAAAAAFyKYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiqDWsN967d69dffXVYb09AAAAAITqmWeeuWBml+d7PrRg7eqrr9aZM2fCensAAAAACJVz7geFnicNEgAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AgAaWTCY1PDyseDyuWCymeDyu4eFhJZPJsIcGACgiULDmnDvinHvBOXfOOffxHM9/yDn3inPuOxv/fbjyQwUAADsxPT2t3t5ejY+PK5VKycyUSqU0Pj6u3t5eTU9Phz1EAEABrcV2cM61SPqUpF+WNCvpaefcKTN7ftuu/8HMPlKFMQIAgB1KJpNKJBJaXFxUS/de7XnnrWrff0jL589q/vRJLaYuKJFIaGZmRj09PWEPFwCQQ5CZtRslnTOz75nZiqSHJb23usMCAADlGB0dle/7auneq/13PKDu64+o/Ypr1X39Ee2/4wG1dO+V7/saGxsLe6gAgDyCBGsHJP0o6/HsxrbtbnXOzTjnppxzV1ZkdAAAoCSTk5PyfV/xw7cq1tYh1+JJklyLp5jXofjhW+X7viYmJkIeKQAgnyDBmsuxzbY9flTS1WbWK+kbkr6Y84WcG3LOnXHOnXnllVd2NlIAABDYwsKCJKl9/6HNQC3DtXpq339oy34AgOgJEqzNSsqeKTso6aXsHczs781seePh5yS9LdcLmdlnzazPzPouv/zyUsYLAAAC6OrqkiQtnz8rW/W3PGdpX8vnz27ZDwAQPUGCtaclXeOce6Nzrk3S+yWdyt7BObc/6+FRSd+t3BABAMBODQwMyPM8zZ8+qbWVpc2AzdK+1vwlzZ8+Kc/zNDg4GPJIAQD5OLPtGY05dnLuPZL+WFKLpM+b2R845z4p6YyZnXLO/aHWg7S0pH+QdJeZ/ddCr9nX12dnzpwp+wMAAIBLJZNJ9fb2blaDjB/eWg1yNXVBnZ2dVIMEgBA5554xs768zwcJ1qqBYA0AgOqanp5WIpGQ7/vy/ddSIT3Pk+d5mpqaUn9/f4gjBIDmVixYC9QUGwAA1J/+/n7NzMxoaGhI8XhcsVhM8XhcQ0NDmpmZIVADgIhjZg0AAAAAQsDMGgAAAADUIYI1AAAAAIgggjUAAAAAkZRMJjU8PLxl3e3w8LCSyWTYQ6sJgjUAAAAAkTM9Pa3e3l6Nj48rlUrJzJRKpTQ+Pq7e3l5NT0+HPcSqI1gDAAAAECnJZFKJREKLi4vatyut+4+068kP79b9R9q1b1dai4uLSiQSDT/DRrAGAAAAIFJGR0fl+74Oxp2evXO3jvW16fCBFh3ra9Ozd+7WwbiT7/saGxsLe6hVRbAGAAAAIFImJyfl+74+9o42dbU5tbU4SVJbi1OX5/Sxd7TJ931NTEyEPNLqIlgDAAAAECkLCwuSpBsPtm4GahltrU43Hmjdsl+jIlgDAAAAECldXV2SpKdm01pZtS3PraRNT72Y3rJfoyJYAwAAABApAwMD8jxP9z2xooUV2wzYVtKmBd903xMr8jxPg4ODIY+0ugjWAAAAAETK8ePH5XmeZudNb/3Mq3rwzIpOz67qwWdW9NbPvKrZeZPneRoZGQl7qFXVGvYAAAAAACBbT0+PpqamlEgk9PJFX/c+vixpWZLkeZ46Oz1NTU2pp6cn3IFWGTNrAAAAACKnv79fMzMzGhoaUjweVywWUzwe19DQkGZmZtTf3x/2EKvOmVnxvaqgr6/Pzpw5E8p7AwAAAEDYnHPPmFlfvueZWQMAAACACCJYAwAAAIAIIlgDAAAAUDeSyaSGh4e3rGMbHh5WMpkMe2gVR7AGAAAAoC5MT0+rt7dX4+PjSqVSMjOlUimNj4+rt7dX09PTYQ+xogjWAAAAAEReMplUIpHQ4uKi9u1K6/4j7Xryw7t1/5F27duV1uLiohKJREPNsBGsAQAAAIi80dFR+b6vg3GnZ+/crWN9bTp8oEXH+tr07J27dTDu5Pu+xsbGwh5qxRCsAQAAAIi8yclJ+b6vj72jTV1tTm0tTpLU1uLU5Tl97B1t8n1fExMTIY+0cgjWAAAAAETewsKCJOnGg62bgVpGW6vTjQdat+zXCIVICNYAAAAARF5XV5ck6anZtFZWbctzK2nTUy+mN/drlEIkBGsAAAAAIm9gYECe5+m+J1a0sGKbAdtK2rTgm+57YkWe5+no0aObhUh839/yGr7v11UhEoI1AAAAAKErlrZ4/PhxeZ6n2XnTWz/zqh48s6LTs6t68JkVvfUzr2p23uR5nszskiBtu3opROLMrPheVdDX12dnzpwJ5b0BAAAARMf09LQSiYR8398SaHmeJ8/zNDU1pf7+/kD73XbbbUqlUkXfMx6Pa25uriqfJyjn3DNm1pfveWbWAAAAAIRmJ/3T+vv7NTMzo6GhoS0zcENDQ5qZmVF/f/9mgZFigu4XJmbWAAAAAIRmeHhY4+Pj2rcrrWfv3L1Zln9l1bSwsp7y+PLFVg0NDenEiRNFXy8ejzOzBgAAAADlqnT/tEwhkkI8z9Pg4GDZY682gjUAAAAAodlp/7RiMoVICvE8TyMjIyWMtrYI1gAAAACEZif904Lo6enR1NSUOjs7LwnaPM9TZ2enpqam1NPTU4HRVxfBGgAAAIDQBO2ftpO0xSCFSOoBBUYAAAAAhCaZTKq3t1eLi4s6GF9fo3bjgVY99WJa9z2xotl5U2dnp2ZmZupiNmwnihUYaa3lYAAAAAAgWyZtMZFI6OWLvu59fFnSsqRM2qJXN2mLlUYaJAAAAIBQNUraYqUFCtacc0eccy8458455z5eYL+Ec86cc3mn8gAAAABgu56eHp04cUJzc3NaXV3V3NycTpw4EWhGLZlManh4eEugNzw8rGQyWYORV0/RYM051yLpU5L6JV0n6QPOuety7Nct6V5Jpys9SAAAAADIZXp6Wr29vRofH1cqlZKZKZVKaXx8XL29vZqeng57iCULMrN2o6RzZvY9M1uR9LCk9+bY73+TdJ+kpQqODwAAAABySiaTSiQSWlxc1L5dad1/pF1Pfni37j/Srn270lpcXFQikajbGbYgwdoBST/Kejy7sW2Tc+4GSVea2Z9XcGwAAAAAkNfo6Kh839fBuNOzd+7Wsb42HT7QomN9bXr2zt06GHfyfV9jY2NhD7UkQYI1l2PbZr1/51xM0pik40VfyLkh59wZ59yZV155JfgoAQAAAGCbyclJ+b6vj72jTV1tTm0t66FLW4tTl7feBsD3fU1MTIQ80tIECdZmJV2Z9figpJeyHndL+jlJf+2c+76kt0s6lavIiJl91sz6zKzv8ssvL33UAAAAAJrewsKCJOnGg62bgVpGW6vTjQdat+xXb4IEa09LusY590bnXJuk90s6lXnSzObMbK+ZXW1mV0t6UtJRM6PjNQAAO9SoFc0AoBq6urokSU/NprWyalueW0mbnnoxvWW/elM0WDOztKSPSPq6pO9K+oqZPeec+6Rz7mi1BwgAQLNo5IpmAFANAwMD8jxP9z2xooUV2wzYVtKmBd903xMr8jxPg4ODIY+0NM7Miu9VBX19fXbmDJNvAABI6zNqvb29WlxcVEv3XsUP36r2/Ye0fP6s5k+f1Grqgjo7OzUzMxOo5xAANIPs786D8fU1ajceaNVTL6Z13xMrmp23SH93OueeMbO8PaoDNcUGAADVlalo1tK9V/vveEDd1x9R+xXXqvv6I9p/xwNq6d5b1xXNAKAaenp6NDU1pc7OTr18sVX3Pr6st//pq7r38WW9fLFVnZ2dmpqaimSgFgTBGgAAEZCpaBY/fKtibR1yLZ4kybV4inkdih++ta4rmgFAtfT392tmZkZDQ0Nb1vsODQ1pZmZG/f39YQ+xZK1hDwAAALxWqax9/6HNQC3DtXpq339oy34AgNf09PToxIkTOnHiRNhDqShm1gAAiIBMpbLl82dlq/6W5yzta/n82S37AQAaH8EaAAARkKloNn/6pNZWljYDNkv7WvOXNH/6ZF1XNAMA7BzBGgCgLjVaP7Ljx4/L8zytpi7o/EP3KPWdx7X80gtKPfu4zj90j1ZTF+R5nkZGRsIeKgCgRijdDwCoO9PT00okEvJ9X77/Wsqg53nyPE9TU1N1uaC8UT8XACA3SvcDABpKMplUIpHQ4uKi1jou056bj+kNg6Pac/MxrXVcpsXFRSUSibqcYWvkimYAgJ1jZg0AUFeGh4c1Pj6utY7LtP+OBzbL3Nuqr7WVJZ1/6B7FluY0NDTUcFXBAACNhZk1AEBDoR8ZANSXRltjXEsEawCAukI/MgCoH9PT0+rt7dX4+LhSqZTMTKlUSuPj4+rt7dX09HTYQ4w0gjUAQNlqedeUfmQAUB+y1xjv25XW/Ufa9eSHd+v+I+3atyutxcVFvec972G2rQCCNQBAWWp915R+ZAAQjp3emBsdHZXv+zoYd3r2zt061temwwdadKyvTc/euVsH406SmG0rgAIjAICSJZNJ9fb2anFxUS3dexU/fKva9x/S8vmzmj99UqupC+rs7NTMzIx6enrKep/R0VFNTk4qlUptbq/me6I82cdsYWFBXV1dGhgY0PHjxzkuQB0qpbVIPB5XKpXS/UfadayvTW0tbvO5lbTpS3+7ooUV6caDrXpqNq37nljR7Lw11Xd4sQIjBGsAgJLVojJjvguEXOhHFg30iwMaS6k35mKxmMxMT354tw4faLnkddNrpjWT2lqcVlZNCyumt37mVb18sbVpKvpSDRIAUDXVrsxYqKdaS/fezf2cc/Qji4hG7oMHNKtMOmNL917tv+MBdV9/RO1XXKvu649o/x0PqKV7r3zf19jY2Ja/l1k7/NRsWiurWyeIVtdMTtqcbWtrcerynD72jjYq+mYhWAMAlKwalRmz10S8+c1v3ryTm+8CwfM8DQ8Pa25uTidOnGiKtJkoK/WiDkB0lXpjLrPG+L4nVrSwYpsB20raZJJaYm7L/m2tTjceaJVERd8MgjUAQMkqXZlxe7GSDHqq1Q/64AGNp9Qbc8ePH5fneZqdX09vfPDMik7PrurBZ1Y0MeNfMtu2kjY99WJaEhV9MwjWAAAlq2RlxkLpcx0Hr6u7nmrN2gSWPnhA4yn1xlxPT4+mpqbU2dmply+26t7Hl/X2P31V9z6+rN/95vIls20Lvum+J1ao6JuFYA0AULLMXdPV1AWdf+gepb7zuJZfekGpZx/X+Yfu0WrqgjzP08jISNHXKpQ+1/qPrpStprfsH+Weas3cBJY+eEDjKefGXH9/v2ZmZjQ0NKR4PC7n1lMfc822vfUzr2p23gL/3mgGVIMEAOzI9pLsHR0dmxX/0unXAqqdVv7LlHjec/MxdV9/ZMusjKV9mUzOufVqkxsXCJWoNllptWpnEFV5K4RG+JgBKKzS32tUjH0NpfsBABWT7xdsa2urzEzt7e1aWlpSV1eXBgcHNTIyEjggyZR4fsPgqNqvuPaS55d/nNTyi89HPvCpRTuDKGv2YBVoVJUOsJLJpMbGxjQxMbHZi3GnvzcaAcEaAKAiqn0RXmxmLfXs4/rpNx7c3BbVO7BBP0c8Htfc3FyII60e7poDjYkAq/LoswYAqIhql2QPsiYiI8o91SiwcekalUyBlageMwDB9PT06MSJE5qbm9Pq6iotU2qAmTUAQCDVnjGql/S57Wv2urq6NDAwoOPHj6unp4eZNQBAYMysAQAqotozRtklnmNLc/rpNx7UjyeO66ffeFCxpTl1dnZqamoq1EAtSJXHSrYzAIB61KytS6qBYA0AEEgtSrJHOX2uUB+4tY7LtLi4qEQioUQiUbF2BgBQb5q5dUk1kAYJAAik2Uuy76TK46/8yq9QYANA08lOZz8Yd/rYO9p048FWPTWb1n1PrGh23iKRzh4lVIMEAFREvawpq5adVKvs7u7WLbfcIuecHn30UaqmAWgKmZta+3al9eydu9XV5tTW4rSyalpYWW+C/fLF1oa9qVcK1qwBACqi3DVludYw3H777RoYGKiLdQ1B1+xJUiqV0le/+lV97Wtf08MPP0zVNABNYXJyUr7v62PvaNsM1CSprcWpy1ufafN9XxMTEyGPtH4wswYA2JFS+uzk67uVS1RTBYvNrC08/9cyf6mpZhsBIFssFpOZ6ckP79bhAy2XPH96dlVv/9NXFYvFtLq6GsIIo4c0SABAqIqlT0qKbEpldpn+VColSZt95ras2Usvy8nJeW1517GR8gOg0WVuat1/pF3H+to2Z9YkaSVtevCZFd37+DKtS7KQBgkACFXBZtq/eUJX/OanKt5guxK2VzTLyFXlcfHstzYDNUlyLZ5iXofih29tipQfynQDjaeU8zrTuuS+J1a0sGJaWV2fFFpJmxZ8031PrNC6ZIeYWQMAVFXB9MG1VUlOLvbavcMoNI4OUkwl2xsGR9V+xbWXvM7ySy/oxxPHGzrlJ1+Ka1TTWQEUV+p5nbMa5IFWPfUi1SDzKTaz1lrLwQAAmk/BwhyxS9c0VKLBdrm2zwZmUh7b9r1Ju6+7abNfWsby+bNq2/emS9axVaL3XJRl955r6d6rPe/cGtQupi4okUhwYQbUkXLO60whqkQioZcv+rr38WVJy5LWA73OTq9gISpcijRIAEBVFWymvbYqW1vbui0CQU6moln88K2bgZq0Nb1Reu0u8/zpk1pbWdr8fJnec/OnTzZ0yk/BFNdt6aykSgL1YSfndS79/f2amZnR0NDQlvN9aGhIMzMzzLTvEGmQAICqKthMe3thjog02M5UNCuW3uic065du+g9V6T33K5du+ScI1USqANBz2uKhFRGRQqMOOeOOOdecM6dc859PMfzdzrn/tY59x3n3H92zl1XzqABANFQidmQ48ePy/O8nIU5zn/+I3rp83dv3baRYuh5nkZGRqr46fIrOBuYNfPX3d1dVu+5ehe099zFixe1uLiotY7LtOfmY3rD4Kj23HxMax2XaXFxUYlEghk2ICKCntdhpak3m6LBmnOuRdKnJPVLuk7SB3IEY182s//ezK6XdJ+kf1fxkQIAamp7NUQzUyqV0vj4uHp7ezU9PR3odQo1015NXdBq6kLkgpxMRbMg6Y3NnPITNKiVVHJKFYDaCnpeN+pa3KgJMrN2o6RzZvY9M1uR9LCk92bvYGbzWQ93SwontxIAUBHZC8wrMRuSL6AZGBjQ7bffXrEgp1LrogrOBuaY+evp6dGJEyc0Nzen1dVVzc3N6cSJEw07o5YRJKjNKLT+r1LtDVgXB5RvJzerUH1F16w55xKSjpjZhzceD0o6bGYf2bbf3ZL+laQ2Sb9oZn9X6HVZswYA0ZV3nVmEmz1XuoQ8JemL20mLg2q3N+B4AZUR5Lxu5LW4tVaJNWsux7ZLIjwz+5SZ9Uj6HUn/a57BDDnnzjjnzrzyyisB3hoAEIYg1RCj1Oy50jOBUm0qmtX7TFChFNdMOuuuXbskVTelqhrHH2hWQc7rRl6LGzVBgrVZSVdmPT4o6aUC+z8s6VdzPWFmnzWzPjPru/zyy4OPEgBQU/W2wLzcUtP5VDO9sVJrAsNWLKj90Ic+VPWUqmodf6BZbL9xdNttt+lXf/VX9eu//utNtxY3aoKkQbZKOivplyS9KOlpSb9hZs9l7XNNJu3ROXeLpN8rNJ0nkQYJAFFWb6Wbg45XWq/gODAwoOPHj4d2Z7iZ0oxq8Vnr7ecViBJSiMNVdhqkmaUlfUTS1yV9V9JXzOw559wnnXNHN3b7iHPuOefcd7S+bu2DFRg7ACAk9bbAPOhMoKRIzF7V80zQTlM3a5FSVW8zwUBUkEIcfYH6rJnZY2Z2yMx6zOwPNrb9rpmd2vjzR83sLWZ2vZm9K3vWDQBQH7Ivwj/96U/L9/3A1RDDVqzU9MqFH0bqIqTe1gRmlJq6We31f1EsNV7v6xHRHOr5xlGzKJoGWS2kQQJAdORLg8kliqkxeatXpn2tpZfl5OS8toIVLZPJpEZHRzU5OamFhQV1dXVVLV0yFovJzKpeIbGSopy6WfD4+0ubNxik2qTBklaGekEKcfgqUQ0SANDACqXBtHTv3dzPORfZBeaF+qItnv3WZqAm5Z69qnWxj7BngkqZ9YnyHfggffEyqp0GS1oZ6gkpxNFHsAYATS7IRbjneRoeHo5ss+dC66La9l5V8CIklUrV/OK6EmsCS02zKzUwjXLqZqHjv5q6oJbuvTULmqIc1ALbhX3jCMURrAFAk4vyRfhObF8XlVHsIqS1tbXmF9dBZoIKrQksNeAqZ9Yn6nfg8x3/WgdNjXI+oTnUWzGpZkSwBgBNLuoX4TuR3RftrrvuCnQRIqnmF9flVEgsJ+AqZ9anHu7AZx//7u5uSap50NRI5xMaX7k3jlB9BGsA0OTq4SK8FEEvQtLptKTaX1yXWiGxnICrnFmfersDH1bQ1KjnExpTLVproDwEawDQ5OrtIjyooBchYV5cZ88Era6uBloTWE7AVU4AU2934MM6ro16PqE2wmj5UG5rDdpUVBfBGgA0uXq7CN+JIBch9XZxXU7AVU4AU2934IMcV0man5+v6MVlkPPJ93194QtfqMkFLRfS9SPfWtTPfe5zuvbaa9fPvQocw1w/E6OjoxoZGdnRjaNCY65mxdWmY2ah/Pe2t73NAADR8Nhjj1lnZ6d5nmeSNv/zPM86Ozvtscce2/Frnjt3zu666y7r7u4255x1d3fbXXfdZefOnavCJyjduXPnrLOz0yRZS/de23PzMXvD4KjtufmYtXTvNUnW2dkZmXF3d3ebJNtz8zG76re/Zv/kd/5887+rjn/N9tx8zCRZPB6/5O/edddd5nmetXTvtYP3/tnm37/q+Nfs4L1/Zi3de83zPLv77rvzvv+5c+fs7rvvtng8brFYzOLxuN19992R+ffJCHJcK/Wzvl2+86ma77mTcVT7fYOql++IWqjVz2slfybq7bszqiSdsQIxE02xAaAJ5WoAfcstt8g5p0cffXRz2+DgoEZGRnY8WxL1psDbP39HR8fmODNr2KTojDdbkAbQ2c2+s0W5sXU1FGr2Xu3Pn0wmNTY2pi9+8Yubs5y1/DeP+rGO2ndEru/EajdPz5b3vF71tbaypJ989fe0+y3vKusYVvpnotiYC30X4TXFmmITrAFAk6n2RVK9XiS2trbKzNTe3q6lpaWygtVqKvffN0oXybW4QM4ETRMTE5qfn5f0Wjn/WlxchnVBG+UL6ah9R0ThnIjH40qlUtpz8zF1X39kS4qzpX2ZTM65so5hpX8mio059ezj+uk3HlQ8Htfc3Fxp/zBNgGANALCpFhdJXCTmf+9KBSblXlxmBzDlzqKWKowL5DAuLsO6oI3yhXSUviOiEjjGYjGZmd4wOKr2K6695Hmz9WBt83EJx7DSPxPFxrz80gv68cRxxWIxra6uFn29ZlUsWKPACAA0kXLKvgcV5abAtfj8uVR6EX651dtKqUJZSeX0iitHGOX8g75npQudRLnfW5S+I8L6TtiuYPGftbUtgZpU2jGs9M8EbSpqg2ANAJpILS6SuEjcqlqBSdgBVznCukAO4+Iy6HtKqmgVvaDvu7a2VvMKkVH6johK4FioeqmtpmWr6S37l/LzWumf/3qrpFuvCNYAoInU4iIpyndbw7hIjMqd+1JUq+x7JS6QSxlbGBeXxd7z1ee+WZWZxaCtC6TKBolBROk7IiqBY6GWDy//2ce1tnKx7J/XSv/8N3LblyhhzRoANJFarGMpp1phtTXTmqVyVXNN2U7XulSqemcY65MKveerz31Tr3/f71dlzVaxzyoptPVZUfqOiNL5We3qpdX4+Y9CcZZ6x5o1AMCmWswsRPluaxgzK1G5c78T1V5TtpOZlVzr/S5evKh0Oi3b9bodjS2Mxt6F3nP3W95VtdS7Qu8rKdRZ3ih9R0QplS/XWtRdu3aptbVV7uLPSvp5zZ6Bvuaaa2RmZb1ekDHvZP0simNmDQCaSK1mFqJ6tzWMmZUo3bkPqtrV+oLOrNx222165JFHKj4rFUY1zFwtBGpRRS/X+0bhZ7FYC422trbNFhrV7HcWlWqQxcZYys9rvbcpaRaU7gcAbFGrQCoK5eFzqXUgGaWUr6CqHWAGvUD+tV/7NX3lK1/JGTTamsnFYnItrRUdWy1U69+3WHuIqJVa3/4d0d7eHkpz+qjeXCpHPQShWEcaJABgi1qlrUS1WmGt03ailPIVVLVTN4OmI546dSpvIRLX0rolUKvU2GqhGql3QdpDRKmwh7T1O+Ls2bNyzpWU3lqusFL5qlXAR6rvwkbYipk1AACqrN7u3NcqdbPY7GstGgWHodKzHmXPVEZgljdKjbJrodrfCfWYft2smFkDACBk9bAIP/sufyqVkqSqF13INfs6MjKi0dFRxeNxZW4o55sJslW/Lns7VbrQSdBZFOdcZGd5o9LvrBZq0RS+HgsbITeCNQAAaiCqaaHSpSl0GbW+qM83jnxB48tf/nikAo6dqGQAHzTQOXXqVM2rYQZVL8FFJVIXa5GiGLWUV5SONEgAAJpYkBS6bNVK3Sy1L1gtxrZ9nIWKeIShlL51USv+E8W0vUr199uu2ftdYiuqQQIAgLyCrBXKBEXxeHofsnUAAB/ISURBVLxqF/U7GYcUTvnxsNYeFgsQoxjo7FTUgotqNqiuRVVOqkHWD4I1AACQV1Qu9IOOI7NvrWeCwrr4DRIgPvroo5EKdHIpFnBGKbgoNJZy+vtl1Oqcq7fCRs2KAiMA0OSqWR4a9S8qa4WCjiMWi4Wy3i+MUuhBC1EkEonIFg6RgrUVqHTRlXIUOtb7PvBHirXtKqsASjVaN+RSD4WNUBzBGgA0sCAXSWgcpQTmUSlEEJVx5BNGtcKgAeIjjzxSdqBTrZs6O6l8GJXgotCxrkR/v1r2XoxyYSMEQ7AGAA2qFuWhER2lBua1ustfTFTGkU8YM5A7CRDLCXSqeVNnpzOSQYOLcoLLYn+34LHeWG+Wbac3E6I0i4joY80aADSoZmsy28zKWe8TlbVCURlHPjtZU9fd3V2RCpGNUIiiGuuzylmLFeTv3nbbbQXHbFpvxl7uusAoVuVE7bFmDQCaVDM1mW125aynispd/qiMI58gM38ZlZqVqkVqaJCfncXFRb35zW8uKTWy0jOS5WQMBP27R48eLXisS+nvl2s2b3R0VCMjI6QooiCCNQBoUFEpHIHqKzcwj8paoaiMI5di64wkVTzVuBapoUF+djJKCUIrHXCWE1wGvanhnCt4rFfOn93RzQTWDqMcpEECQIOKSkl2VF8t0uUaXZBm1/lS6DIX/5VONa5FamjRn50fJ7X84vMlv2+l+6ftJB1V2ll6Y/Z34sMPP5zzWO+0v1/U03sRPtIgAaBBFVskH/WCDaicqFdSjLqgMx/bZ/4yqpVqXCg11F38mVpaWmRmuuaaa0qu3ljsZ8fbe2VZbQoqXfmwaMbAgevyznDuJNsg3yzvsWPH9MILL+jVV18NlLpYy5YPtGlpTMysAUAdCrJI/tChQ9zRbRKVnr1oJuXMfNRqRnN7IYr29vbN8z6dTm/uV0qz40I/O2YmF4ttKVVfyqx8uc2Zs2c9U6mUJAUr/rFthlOSfN+vabYBDbBRDDNrANBggi6SlxTpgg2onFr2bWo05cx81GpGM7uc/dmzZ+WcUzqdlu16Xdnr5Ar97KT/4Udl9xSTyluLuH3WMyNXxoCZyblY3hnOTBBTy2yDaq0dzp5Fc87pPe95D21aGhTBGgDUmZ1cXEa5YAMqJ+qVFKOsnOIsYaQaVzqtrtDPztLs8xULQktpzlzoxpSkHQeXGbW8qVGNgD5fAFuLVEvUXqA0SOfcEUl/IqlF0riZ/dG25/+VpA9LSkt6RdJvmtkPCr0maZAAUBoKhyAf+jbtXDmpjGEUj6jW+Z/9szM/Py8pR+GUGqfVBukVuZq6sLl/vn+Thef/WuYvXXJsslUrXbDSKcqFfuac16Gut9zE74Q6UywNsmiw5pxrkXRW0i9LmpX0tKQPmNnzWfu8S9JpM1t0zt0l6SYzu63Q6xKsAUBpqPwHVE65wU+l1woVq0oZlUbZkuScU1dXl2655RY553Tq1Km8lTRLsdPKjzmDy/SynJyc15Y30IvH41W7qVHpgL5QACsXk4u1XPJ3+J0QbZVYs3ajpHNm9j0zW5H0sKT3Zu9gZt80s8WNh09KOljqgAEAl8pen5C5yUblP6B85aYyVjLVOEhVylqskyuUGpk9I5UZ35e//GV96Utf2jLmz33uc7r22mvXX6PEyoRB13s559TZ2ZkzvXHx7Lc2AzXp0v5xmSC8nGbUuaow3n777RoYGNANN9ygxcX1S+S1hb8vO0W5UNquJNna2pb9+Z1Q/4IEawck/Sjr8ezGtnx+SxLd/QCgQnaywJ6S/MDOVKI4SynrsbYLWjjo6NGjNVkntz0Idc5tPtfSvXfL+Fq6916yzXa9Tqurq7p48WLJTaCDBqbd3d15g8u2vVdVtLDHdvkC7OwAdjvnXMkBfcEANtYiyfid0GCCpEG+T9K7zezDG48HJd1oZvfk2HdA0kckvdPMlnM8PyRpSJKuuuqqt/3gBwWXtQFA0yuWQiOpaIpSJdKRgEYWhbLnQdZnxZbmdNttt+mRRx6peUuOguPzc6ca/uSrv6fdb3lXzRpq51p3V821vaV+P5dzfIqlhuZbn0ebluiqxJq1X5D0b8zs3RuPPyFJZvaH2/a7WdIDWg/UflJsYKxZA4DidrrAPhf67ADF5SrOcvToUZlZxddi5bKTtXMPP/xwzYPLguNbW5Xk5GKvJWwV63kWpKBGOeu9atF7sJQAttz3DfK5sn8n8P0ffZUI1lq1XmDklyS9qPUCI79hZs9l7XODpClJR8zs74IMjGANAIrb6QJ7STTABiqg1rNtOy0cUuvKn8XGl8t637PX0idr2VC7FpU6Swlgy53R20nxl+7ubqrB1oFiwVprvicyzCztnPuIpK9rvXT/583sOefcJyWdMbNTkv6tpC5JX904KX9oZkcr8gkAoIkFXWAvrV+8bL/b2rbvTdp93U06/9A98pfmNDY2VtVS20AjyF4/1tK9V3veufWCeDF1QYlEoqI3P7q6upRKpbR8/qza9r3pkhsz24tEZNbJ1ep8Lji+XIHJ2tqWx1J5DbV3GphmiqQkEgn5G+vYMjzPU3sFeg8WXz+2VSXWygX9XMyiNY5ATbHN7DEzO2RmPWb2BxvbfncjUJOZ3Wxm+8zs+o3/CNQAoAKCLrCXVHJjX0RfrmpzO62sh+Aq3Xg6iDAabFdsfCsXZSsXt2yz1bRsNb3lNWrZUFuqbKXOXAp+P6+tVq0yY7U/F6IlUFPsaiANEgCKC7ruIpMeRO+1xhOF4hfNJozG82E02K7k+KStxTRefe6bev37fj+0htq1UPD7eXt/twb8/KiMstesVQvBGgAUF/QCzjmnV199taYXl6i+qF/AN6qwGs9HPTDPN758Gv1nNoxqkGg8Za9ZAwDUVjKZ1OjoqCYnJ7WwsKCOjg61trZKF3+Wd33Co48+qvHxcc2fPqnd192kWJu23M0NO4UKpdmejsdaxNrY6fqxSil1fVat5Btfpmrmo48+qoWFBbW3t68HcwW+s8L+LJVQaP1YRiN/ftQGM2sAECH57ly3trbKzNTe3q6lpaVLLuCYgWlMYaTjoTZl3xtdratVhqlQ24dMANvInx/lIQ0SAOpEuQFX1FOosHNhpeM1O25+AKiVYsFaoGqQAIDqK7cCHRXCGk/QaqCVTsdrdpn0ts7OTsU20tt+PHFcP/3Gg4otzamTVDYANUKwBgARMTk5WXb5/VJLXCOaol7OvZFx8wNAFJAGCQARQcobtiMdDwAaG9UgAaBOhFWBDtFVqNocleUAoPGRBgkAEUHKG3IhHQ8AmhdpkAAQEaS8AQDQXEiDBIA6QcobAADIRhokAEQIKW8AACCDNEgAAAAACAFNsQEAAACgDhGsAQAAAEAEEawBQIiSyaSGh4e3rE8bHh5WMpkMe2gAACBkBGsAEJLp6Wn19vZqfHxcqVRKZqZUKqXx8XH19vZqeno67CECAIAQUbofAEKQTCaVSCQ2e6rteefWnmqLqQtKJBL0VAMAoIkxswYAIRgdHZXv+2rp3qv9dzyg7uuPqP2Ka9V9/RHtv+MBtXTvle/7GhsbC3uoAAAgJARrABCCyclJ+b6v+OFbFWvrkGvxJEmuxVPM61D88K3yfV8TExMhjxQAAISFYA0AQrCwsCBJat9/aDNQy3Ctntr3H9qyHwAAaD4EawAQgq6uLknS8vmzslV/y3OW9rV8/uyW/QAAQPMhWAOAEAwMDMjzPM2fPqm1laXNgM3Svtb8Jc2fPinP8zQ4OBjySAEAQFicmYXyxn19fXbmzJlQ3hsAwpZMJtXb27tZDTJ+eGs1yNXUBXV2dlINEgCABuace8bM+vI9T+l+AAhBT0+PpqamlEgk5C/N6affeHDzOc/z1N7ZqampKQI1AACaGGmQABCS/v5+zczMaGhoSPF4XLFYTPF4XENDQ5qZmVF/f3/YQwQAACEiDRIAAAAAQlAsDZKZNQAAAACIIII1AKiSZDKp4eHhLSmOw8PDSiaTYQ8NAADUAYI1AKiC6elp9fb2anx8XKlUSmamVCql8fFx9fb2anp6OuwhAgCAiKMaJABUWDKZVCKR2CzLv+edW8vyL6YuKJFIUJYfAAAUxMwaAFTY6OiofN9XS/de7b/jAXVff0TtV1yr7uuPaP8dD6ile69839fY2FjYQwUAABFGsAYAFTY5OSnf9xU/fKtibR1yLZ4kybV4inkdih++Vb7va2JiIuSRAgCAKCNYA4AKW1hYkCS17z+0GahluFZP7fsPbdkPAAAgF4I1AKiwrq4uSdLy+bOyVX/Lc5b2tXz+7Jb9AAAAciFYA4AKGxgYkOd5mj99UmsrS5sBm6V9rflLmj99Up7naXBwMOSRAgCAKAsUrDnnjjjnXnDOnXPOfTzH8//COfdt51zaOZeo/DABoH4cP35cnudpNXVB5x+6R6nvPK7ll15Q6tnHdf6he7SauiDP8zQyMhL2UAEAQIQVLd3vnGuR9ClJvyxpVtLTzrlTZvZ81m4/lPQhSb9djUECQD3p6enR1NSUEomE/KU5/fQbD24+53me2js7NTU1Rdl+AABQUJCZtRslnTOz75nZiqSHJb03ewcz+76ZzUhaq8IYAaDu9Pf3a2ZmRkNDQ4rH44rFYorH4xoaGtLMzIz6+/vDHiIAAIi4IMHaAUk/yno8u7ENAFBAT0+PTpw4obm5Oa2ururb3/621tbWdMMNN2wGb8PDw0omk2EPFQAARFCQYM3l2GalvJlzbsg5d8Y5d+aVV14p5SUAoC5NT0+rt7dX4+PjSqVSMjOlUimNj4+rt7dX09PTYQ8RAABETJBgbVbSlVmPD0p6qZQ3M7PPmlmfmfVdfvnlpbwEAERSMpnU8PDwlpTHzKxZMplUIpHQ4uKi1jou056bj+kNg6Pac/MxrXVcpsXFRSUSCWbYAADAFkGCtaclXeOce6Nzrk3S+yWdqu6wAKB+FJs1++hHPyrf99XSvVf773hA3dcfUfsV16r7+iPaf8cDauneK9/3NTY2FvZHAQAAEVI0WDOztKSPSPq6pO9K+oqZPeec+6Rz7qgkOef+mXNuVtL7JD3onHuumoMGgKgIMmv2F3/xF/J9X/HDtyrW1iHX4kmSXIunmNeh+OFb5fu+JiYmQv40AAAgSoqW7pckM3tM0mPbtv1u1p+f1np6JAA0ldHR0S2zZplgrG3fm7T7ups2+6pJUvv+Q5uBWoZr9dS+/5AkaWFhoebjBwAA0RWoKTYAILfJycmis2YZy+fPylb9LX/f0r6Wz5+VJHV1ddVu4AAAIPII1gCgDJnZsGKzZpI0f/qk1laWNgM2S/ta85c0f/qkPM/T4OBg7QYOAAAij2ANAHYou/Kj2Xonk2KzZpK0mrqg8w/do9R3HtfySy8o9ezjm2mSnudpZGSkpp8DAABEW6A1awCAddPT00okEvJ9X77/WnA2f/qkdl93k2Jt6ymQ22fN3v3ud+uv/uqv5C/N6affeHDz73mep/bOTk1NTamnpyeMjwQAACLKZe4K11pfX5+dOXMmlPcGgFIkk0n19vZqcXFRLd17FT98q9r3H9Ly+bOaP31Ski7Ztpq6oM7OTs3MzEiSxsbGNDExoYWFBXV1dWlwcFAjIyMEagAANCHn3DNm1pf3eYI1AAhmeHhY4+PjWuu4bEvlR1v1tbaytKXyo7Q+a+Z5nqamptTf3x/iyAEAQBQVC9ZYswYAAe2k8mM8HtfQ0JBmZmYI1AAAQElYswYAAQWt/BiLxTQ3N1fz8QEAgMbCzBoABJTpg0a/NAAAUAsEawAQ0MDAgDzPo18aAACoCdIgAaCAZDKp0dFRTU5OKpVKrW/01/ul5ar82N7ZSb80AABQEQRrAJBHvp5q0nqDa/qlAQCAaiJYA4AcksmkEonEZk+1Pe+8dBZNkpxz6u7upl8aAACoOII1ANiQK+WxpXvvlp5qbfvepN3X3aTzD92j2NKchoaGdOLEiZBHDgAAGhEFRgBA6ymPvb29Gh8ff21tmlSwp5rv+5qYmAhryAAAoMExswag6RVKeew4eF3BnmqZ3msAAACVRrAGoOmNjo7K9/2cKY+2ZrLVtFzLa1+X9FQDAAC1QBokgKY3OTkp3/dzpjw652S2Rk81AABQc8ysAWh6mVTG9v2HcqY8rvw4qeUXn6enGgAAqCmCNQBNr6urS6lUSsvnz6pt35u2BGyW9rX84vP0VAMAADVHGiSApjcwMCDP8zR/+qTWVpZypjxmxONxDQ0NaWZmRv39/WENGQAANAFnZqG8cV9fn505cyaU9waAbMlkUr29vZvVIOOHL22A3dnZqZmZGWbSAABAxTjnnjGzvnzPkwYJoCllN8BeWFhQR0eHWltbpYs/I+URAABEAsEagKYzPT2tRCIh3/fl++spjxcvXlRra6vMTJ2dnVpaWlJXV5cGBwc1MjJCoAYAAGqOYA1AUynUADuT8ihJZ8+eJUADAAChosAIgKayvQF29/VH1H7Fteq+/oj23/GAWrr3yvd9jY2NhT1UAADQ5AjWADSVQg2wY16H4odvle/7mpiYCHmkAACg2RGsAWgqxRpgt+8/tGU/AACAsBCsAWh4yWRSw8PDisfjyrQrWT5/drOfWoalfS2fPytpvVE2AABAmCgwAqCh5ar8KEnzp09q93U3Kda2ngKZ3QDb8zwNDg6GOGoAAACaYgNoYMWaXUuiATYAAAgNTbEBNK3tlR8zBUXa9r1Ju6+7SecfuocG2AAAILJYswagLmSvO4vFYorH47r99ts1MDCQd9unP/3popUfM+LxuIaGhjQzM6P+/v6wPiYAAMAmZtYARF6udWepVEpf/vKXt+yXa5tUvPJjLBbT3NxclUYPAABQGoI1AKFKJpMaHR3V5OSkFhYW1NXVpVtuuUXOOZ06dUqpVGpz35buvdrzzuLrzrZvW7nwQ7Xte9OWgI3KjwAAIOoI1gBsyhU4DQwM6Pjx41vWcBULsPJt6+zs1NVXX63vf//7WlxcVEdHx+ZMWTqdlpR/diznurO3vEtOTs5rK7htzV+W+SuSqPwIAADqB9Ugtyn1IrTUi9VKv37Ut/H5o/v5cwVOktTa2iozU1tbm5aWlvLuV44glRqd16Gut9y0dXZsbVWSk4vFCm9L+1p4/q9l/hKVHwEAQGQUqwYZKFhzzh2R9CeSWiSNm9kfbXu+XdK/l/Q2SX8v6TYz+36h14xisJavHxPQTHIFTqupC4H2k4qnJL763De1+y3v2vL49e/7/c0ZM1v1teYvb5kds1VfcjG5WEvJn2v5pRf044njm489z5PneZqamqKgCAAACEXZwZpzrkXSWUm/LGlW0tOSPmBmz2ftMyyp18zudM69X9KvmdlthV43asFaKf2YSt22/WK10q8f9W18/uh+/pyB08qSfvLV3yspwMoZdMVapbVVuZZW2aovWzO5WEyu5bWs7MAzZjuYWUs9+/hmmf54PK7BwUGNjIwwowYAAEJTiWDtFyT9GzN798bjT0iSmf1h1j5f39jnW865Vkk/lnS5FXjxqAVrw8PDGh8f11rHZVvWxezkIrTUi9VKv37Ut/H5o/v5cwZOaV8mk3Ou8H5BAydbf63X9lnb8nwhtrYq2dpr687S2z5Dvm3+ks4/dI9iS3MaGhrSiRMnSv+yAAAAqJBKNMU+IOlHWY9nJR3Ot4+ZpZ1zc5L+saRLc6cianJyUr7va887c/RjcjFlX3CWu83MNi9yq/H6Ud/G54/u55e7NHByrZ6UFWDl3S9HimLObVmB2vo+sRwBXPB1Z1KwWcTV1AW1d3ZqZGTkkjEBAABEUZBgzeXYtn3GLMg+cs4NSRqSpKuuuirAW9fOwsKCpDz9mIJehJZ8sVrh14/6Nj5/NMaRc1uuwClXYBYwwAoys7Yxc6fMzF3W7Ji2zY7N/ecv5Vw/l0lvzLfN8zy1d3ZqamqKtEcAAFA3ggRrs5KuzHp8UNJLefaZ3UiDvEzSP2x/ITP7rKTPSutpkKUMuFq6urqUSqW0fP7spf2YKr1+ptSL3EbZxueP7ufPETiZmWS5UyOLBlg5tqmlVbaaXk8D3QjCtq+JKzQ7Jq0HvN3d3Tp69KjMTI8++uhmlctc21ifBgAA6lGQYO1pSdc4594o6UVJ75f0G9v2OSXpg5K+JSkh6a8KrVeLooGBAY2Pj2v+9Entvu4mxdq044vQki9WK/z6Ud/G54/w588ROF1STGSHAdb2bbkKrKymLmhlo0F1tnyzY1RvBAAAzSBo6f73SPpjrZfu/7yZ/YFz7pOSzpjZKedch6QJSTdofUbt/Wb2vUKvGbUCI1SDpBoin39nZfpz7VeKTB+39vZ2LS0tMTsGAACaRkX6rFVD1II1iT5rwPbAqb29vWCj7GIB1vZtuZqCE4QBAIBmVYlqkE2jv79fMzMzGhsb08TERME1MKVuy3WxWsnXj/o2Pn+0P3+uwCmZTF5yThBgAQAAVB8zawAAAAAQgmIza8E60QIAAAAAaopgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiiGANAAAAACKIYA0AAAAAIohgDQAAAAAiyJlZOG/s3CuSfhDKmwe3V9KFsAcBjkOEcCyigeMQHRyLaOA4RAfHIho4DtFR7Fj8EzO7PN+ToQVr9cA5d8bM+sIeR7PjOEQHxyIaOA7RwbGIBo5DdHAsooHjEB3lHgvSIAEAAAAgggjWAAAAACCCCNYK+2zYA4AkjkOUcCyigeMQHRyLaOA4RAfHIho4DtFR1rFgzRoAAAAARBAzawAAAAAQQQRrOTjnjjjnXnDOnXPOfTzs8TQT59yVzrlvOue+65x7zjn30Y3t/8g595fOub/b+P+esMfaDJxzLc65v3HO/fnG4zc6505vHIf/4JxrC3uMzcA59zrn3JRz7r9unBu/wDlRe865kY3vpf/inPsz51wH50RtOOc+75z7iXPuv2Rty3kOuHX3b/wOn3HO/Xx4I28seY7Dv934bppxzn3NOfe6rOc+sXEcXnDOvTucUTemXMci67nfds6Zc27vxmPOiSrJdxycc/ds/Nw/55y7L2v7js8JgrVtnHMtkj4lqV/SdZI+4Jy7LtxRNZW0pONm9t9Jerukuzf+/T8u6T+Z2TWS/tPGY1TfRyV9N+vx/yFpbOM4/FTSb4UyqubzJ5IeN7N/KumtWj8mnBM15Jw7IOleSX1m9nOSWiS9X5wTtfIFSUe2bct3DvRLumbjvyFJn67RGJvBF3TpcfhLST9nZr2Szkr6hCRt/O5+v6S3bPyd/2vjGguV8QVdeizknLtS0i9L+mHWZs6J6vmCth0H59y7JL1XUq+ZvUXS/7mxvaRzgmDtUjdKOmdm3zOzFUkPa/0fHDVgZufN7Nsbf05p/aL0gNaPwRc3dvuipF8NZ4TNwzl3UNKvSBrfeOwk/aKkqY1dOA414JyLS/oXkv5Uksxsxcx+Js6JMLRK2uWca5XUKem8OCdqwsz+P0n/sG1zvnPgvZL+va17UtLrnHP7azPSxpbrOJjZ/2Nm6Y2HT0o6uPHn90p62MyWzey/STqn9WssVECec0KSxiR9TFJ2UQrOiSrJcxzukvRHZra8sc9PNraXdE4QrF3qgKQfZT2e3diGGnPOXS3pBkmnJe0zs/PSekAn6fXhjaxp/LHWv/DXNh7/Y0k/y/qlzLlRG2+S9IqkhzZSUsedc7vFOVFTZvai1u+O/lDrQdqcpGfEORGmfOcAv8fD85uSpjf+zHGoMefcUUkvmtmz257iWNTWIUn/w0aK/P/rnPtnG9tLOg4Ea5dyObZRMrPGnHNdkk5K+p/MbD7s8TQb59y/lPQTM3sme3OOXTk3qq9V0s9L+rSZ3SDpVZHyWHMb66HeK+mNkq6QtFvrqUXbcU6Ej++qEDjn/rXWlzJ8KbMpx24chypxznVK+teSfjfX0zm2cSyqp1XSHq0v5/mfJX1lIzuppONAsHapWUlXZj0+KOmlkMbSlJxzntYDtS+Z2SMbm1/OTNlv/P8n+f4+KuKfSzrqnPu+1lOBf1HrM22v20gBkzg3amVW0qyZnd54PKX14I1zorZulvTfzOwVM/MlPSLpHeKcCFO+c4Df4zXmnPugpH8p6XZ7rScUx6G2erR+M+nZjd/dByV92zn3BnEsam1W0iMbaadPaT1Daa9KPA4Ea5d6WtI1GxW+2rS+EPBUyGNqGht3Hv5U0nfN7N9lPXVK0gc3/vxBSf93rcfWTMzsE2Z20Myu1vo58Fdmdrukb0pKbOzGcagBM/uxpB85567d2PRLkp4X50St/VDS251znRvfU5njwDkRnnznwClJ/+NGBby3S5rLpEui8pxzRyT9jqSjZraY9dQpSe93zrU7596o9eIWT4UxxmZgZn9rZq83s6s3fnfPSvr5jd8hnBO19R+1fpNbzrlDktokXVCJ50RrsR2ajZmlnXMfkfR1rVf7+ryZPRfysJrJP5c0KOlvnXPf2dj2v0j6I61PI/+W1i+a3hfS+Jrd70h62Dn3v0v6G20UvUDV3SPpSxs3kL4n6Q6t32zjnKgRMzvtnJuS9G2tp3r9jaTPSvoLcU5UnXPuzyTdJGmvc25W0u8p/++FxyS9R+uL9xe1fr6gAvIch09Iapf0l+v3MfSkmd1pZs85576i9ZsaaUl3m9lqOCNvPLmOhZnl+/7hnKiSPOfE5yV9fqOc/4qkD27MOJd0TrjXZqsBAAAAAFFBGiQAAAAARBDBGgAAAABEEMEaAAAAAEQQwRoAAAAARBDBGgAAAABEEMEaAAAAAEQQwRoAAAAARBDBGgAAAABE0P8PGSBJUoqI9NYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview the time series and its split. \n",
    "X_scaled_tmp = np.concatenate((X_nn_train,X_nn_validate,X_nn_test), axis=0)\n",
    "usa_indices = np.where(data.location.unique()=='United States')[0]\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "\n",
    "# plot the original data before slicing \n",
    "ax.scatter(range(len(X_scaled_tmp)), X_scaled_tmp[:, usa_indices, -1, new_cases_index], s=100, color='k', label='original')\n",
    "\n",
    "# training set plot\n",
    "ax.scatter(range(len(scaled_splits[0])), \n",
    "           scaled_splits[0][:, usa_indices,-1, new_cases_index]\n",
    "           , s=30, label='train')\n",
    "\n",
    "# validation set\n",
    "ax.scatter(range(len(scaled_splits[0]), len(scaled_splits[0])+len(scaled_splits[2])), \n",
    "            scaled_splits[2][:, usa_indices,-1, new_cases_index]\n",
    "           , s=30, label='validate')\n",
    "\n",
    "# testing set\n",
    "ax.scatter(range(len(scaled_splits[0])+len(scaled_splits[2]), len(X)), \n",
    "            scaled_splits[5][:, usa_indices,-1,new_cases_index]\n",
    "           , s=30, label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('United States new cases per million time series')\n",
    "plt.ylabel('New cases per million (rescaled)')\n",
    "plt.xlabel('Time index')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots of train, validate, test sets, by plotting all values in black first I can ensure that the points\n",
    "are correctly ordered and at the correct values because it makes the rest of the points look like they have black borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse the time axis used for slicing\n",
    "X_nn_train_model = np.concatenate(X_nn_train.reshape(X_nn_train.shape[0], \n",
    "                                                     X_nn_train.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_nn_validate_model = np.concatenate(X_nn_validate.reshape(X_nn_validate.shape[0], \n",
    "                                                           X_nn_validate.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_nn_test_model = np.concatenate(X_nn_test.reshape(X_nn_test.shape[0], \n",
    "                                                   X_nn_test.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "# reshape the dependent variable\n",
    "y_nn_train_model = y_nn_train.ravel()\n",
    "y_nn_validate_model = y_nn_validate.ravel()\n",
    "y_nn_test_model = y_nn_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the correct arrays for the naive baseline, take the values from X instead of y, as y is shifted and makes the process more confusing and or susceptible to mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "\n",
    "# Get the naive baseline values for each split\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility set the seeds of the weights in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel0 = RandomNormal(seed=0)\n",
    "kernel1 = RandomNormal(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize, compile the neural network, readying it for fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "nn_input = Input(shape=X_nn_train.shape[2:])\n",
    "flat = Flatten()(nn_input)\n",
    "dense0 = Dense(int(flat.shape[1].value), \n",
    "                use_bias=False,\n",
    "               kernel_initializer=kernel0,\n",
    "\n",
    "               )(flat)\n",
    "dense1 = Dense(1, \n",
    "                activation='relu',\n",
    "                use_bias=False,\n",
    "                kernel_initializer=kernel1,\n",
    "\n",
    "               )(dense0)\n",
    "\n",
    "nn = Model(inputs=nn_input, outputs=dense1)\n",
    "nn.compile(loss='mae', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of parameters to see that there are not too many (as there are not many samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the scaled training and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = nn.fit(X_nn_train_model, y_nn_train_model, epochs=epochs, validation_data=(X_nn_validate_model, y_nn_validate_model), \n",
    "          batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the training and validation loss vs epochs, to see if the model has been overtrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the data, look at some plots which compare the predictions for both the neural network and naive baseline vs the true values; first for the training and then the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  y_nn_train_model.ravel()\n",
    "y_predict = nn.predict(X_nn_train_model).ravel()\n",
    "y_naive = y_train_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='NN model',\n",
    "               suptitle='Naive baseline vs. predictions, training set',\n",
    "              figname='nn_prototype_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  y_nn_validate_model.ravel()\n",
    "y_predict = nn.predict(X_nn_validate_model).ravel()\n",
    "y_naive = y_validate_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='NN model',\n",
    "               suptitle='Naive baseline vs. predictions, validation set',\n",
    "              figname='nn_prototype_validate_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional neural network model\n",
    "<a id='cnn'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The convolution is with respect to time, specifically the time steps within frames of time of predetermined length. The idea behind usage of a convolutional neural network is to have the network identify the relevant patterns in the dataset. This brings along with it a number of important details is indeed in the right dimension and that time-ordering is being respected by the training and validation process. The manner in which this is handled also has to be compatible with the keras API. The preprocessing is the same as that for the neural network, only that this time the input is not flattened until after the convolutional layers. So, proceeding as before, ***I do some redundant calculations here for the sake of modularity.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single out the only feature used for the prototyping.\n",
    "model_data = data.copy().loc[:, 'new_cases_per_million'].to_frame()\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "model_data = model_data.iloc[:, new_cases_index].to_frame(); new_cases_index=0\n",
    "\n",
    "# query some parameters to be used for convenience and formatting.\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "# get the start date for the frames, and the frame size\n",
    "frame_size = 28\n",
    "start_date = frame_size + data.time_index.min()\n",
    "\n",
    "# determine the size of the splits \n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_cnn_train, y_cnn_train, X_cnn_validate, y_cnn_validate, X_cnn_test, y_cnn_test) = splits\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "X_cnn_train_model = np.concatenate(X_cnn_train.reshape(X_cnn_train.shape[0],\n",
    "                                                       X_cnn_train.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_cnn_validate_model = np.concatenate(X_cnn_validate.reshape(X_cnn_validate.shape[0],\n",
    "                                                             X_cnn_validate.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "X_cnn_test_model = np.concatenate(X_cnn_test.reshape(X_cnn_test.shape[0],\n",
    "                                                     X_cnn_test.shape[1], -1), axis=0)[:,:,np.newaxis]\n",
    "y_cnn_train_model = y_cnn_train.ravel()\n",
    "y_cnn_validate_model = y_cnn_validate.ravel()\n",
    "y_cnn_test_model = y_cnn_test.ravel()\n",
    "\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the CNN itself is two convolutional layers followed by two dense layers, ending with a ReLU activation layer. The parameters for the convolutional layers are the number of filters and the kernel size of the convolutions. The number of filters is much larger than the second because the desire is to pick up on many small patterns and then use them in combination to form larger ones. As far as I know this is the best way of doing so. There is no real argument for the convolutional kernel size other than they cannot be too large because the number of time steps are themselves small in number. \n",
    "\n",
    "Here is the visualization of this architecture.\n",
    "\n",
    "<div>\n",
    "<img src=\"cnn.jpg\" width=\"600\" height=\"300\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize with seeds for reproducibility, follow the same procedure as the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel0 = RandomNormal(seed=0)\n",
    "kernel1 = RandomNormal(seed=1)\n",
    "kernel2 = RandomNormal(seed=2)\n",
    "kernel3 = RandomNormal(seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "kernel = 4\n",
    "N = 8\n",
    "FC = 8\n",
    "batch_size = X_cnn_train.shape[0]\n",
    "\n",
    "f1, f2 = 64, 4\n",
    "k1, k2 = 4, 4\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "\n",
    "cnn.add(Conv1D(filters=int(f1), kernel_size=int(k1),\n",
    "                 padding='valid',\n",
    "                 input_shape=X_cnn_train.shape[2:],\n",
    "               kernel_initializer=kernel0,\n",
    "                )\n",
    "         )\n",
    "\n",
    "cnn.add(Conv1D(filters=int(f2), \n",
    "                 kernel_size=int(k2), \n",
    "                 padding='valid',\n",
    "               kernel_initializer=kernel1,\n",
    "                )\n",
    "         )\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "\n",
    "cnn.add(Dense(4,\n",
    "              kernel_initializer=kernel2,\n",
    "             )\n",
    "       )\n",
    "\n",
    "\n",
    "cnn.add(Dense(1, \n",
    "                activation='relu',\n",
    "              kernel_initializer=kernel3,\n",
    "\n",
    "                   ))\n",
    "\n",
    "cnn.compile(loss='mae', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "          batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss curves to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of predicting the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_train_model.ravel()\n",
    "y_predict =  cnn.predict(X_cnn_train_model).ravel()\n",
    "y_naive = y_train_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Naive baseline vs. predictions, validation set',\n",
    "              figname='cnn_prototype_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of predicting the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_true =  y_cnn_validate_model.ravel()\n",
    "y_predict = cnn.predict(X_cnn_validate_model).ravel()\n",
    "y_naive = y_validate_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='CNN model',\n",
    "               suptitle='Naive baseline vs. predictions, validation set',\n",
    "              figname='cnn_prototype_validate_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the network for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.a Ridge Regression with single feature\n",
    "<a id='regression'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The last model used is Ridge regression, or linear regression with $L_2$ regularization. I will actually deploy this model on two different subsets in this prototyping stage. The first subset is the same which is used for the neural networks which is to say only the ```new_cases_per_million``` feature. For the regression I start with this same data for comparison, but then expand into the realistic scenario wherein there are multiple features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = pd.read_csv('regression_data.csv', index_col=0)\n",
    "tmp = pd.read_csv('regression_data_full.csv', index_col=0)\n",
    "r_data = pd.concat((r_data,tmp.loc[:, column_search(tmp,'government_response')]),axis=1)\n",
    "r_data = r_data.drop(columns=['date'])\n",
    "r_data = r_data.drop(columns=column_search(r_data,'test'))\n",
    "r_data = r_data.drop(columns=column_search(r_data,'deaths'))\n",
    "r_data = r_data.drop(columns=column_search(r_data,'recovered'))\n",
    "r_data = r_data.drop(columns=column_search(r_data,'std'))\n",
    "r_data = r_data[r_data.location == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the sake of modularity, perform some redundant calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_features = ['new_cases_per_million', 'government_response_index', 'log_new_cases_per_million']\n",
    "model_r_data = r_data.copy().loc[:, modeling_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model_data = r_data.new_cases_per_million.to_frame().copy(); new_cases_index=0\n",
    "new_cases_index = column_search(r_model_data, 'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = r_data.location.nunique()\n",
    "target_data = r_data.new_cases_per_million\n",
    "time_index = r_data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size + time_index.min()\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_Xy(r_model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "\n",
    "flat_splits = flatten_Xy_splits(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use a different metric other than explained variance as a metric; the easiest way of doing so is to use RidgeCV and pass it a scoring function. I also provide the indices of the training and testing folds; of which there is only one, so that the data used to train this model and the neural networks are the same. The cross validation ranges over some alpha values (regularization constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression = np.concatenate((X_regression_train, X_regression_validate), axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate), axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "ridge = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = ridge.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_regression_train\n",
    "y_predict = ridge.predict(X_regression_train).ravel()\n",
    "y_naive = y_train_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge Regression model',\n",
    "               suptitle='Naive baseline vs. predictions of training set',\n",
    "              figname='regression_prototype_single_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_regression_validate\n",
    "y_predict = ridge.predict(X_regression_validate).ravel()\n",
    "y_naive = y_validate_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge Regression model',\n",
    "               suptitle='Naive baseline vs. predictions, validation set',\n",
    "              figname='regression_prototype_single_validate_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.b Regression with all features\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "This second regression model contains most of the time-dependent features as well as their corresponding rolling averages.\n",
    "\n",
    "This is partly a sneak peak into the notebook on regression, ```COVID19_regression.ipynb``` and partly a demonstration of evidence that supports the claim that regression performs much better with more feature information. ***Again, redundant calculations are performed for modularity.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_model_data = r_data.iloc[:,2:].copy()\n",
    "new_cases_index = column_search(r_model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = r_data.location.nunique()\n",
    "target_data = r_data.new_cases_per_million\n",
    "time_index = r_data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = create_Xy(r_model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "(X_train, y_train, X_validate,\n",
    " y_validate, X_test, y_test) = splits\n",
    "\n",
    "n_features = X.shape[-1]\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's check that the data is organized as we expect, as this time around we are using more than a single feature. First, get the indices of the 28 columns which we expect to represent 28 days of new case values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases_indices = np.ravel_multi_index([list(range(frame_size)),[new_cases_index]],(frame_size, n_features))\n",
    "new_cases_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy_splits(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first frame should have time values from 0 to 27.\n",
    "r_model_data.new_cases_per_million.iloc[:frame_size].values-X_regression_train[0, new_cases_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last row or last frame, should have values from time_index.max()-1-n_test_frames-n_validation_frames-frame_size to time_index.max()-1-n_test_frames-n_validation_frames\n",
    "(r_model_data.new_cases_per_million.iloc[-frame_size-n_test_frames-n_validation_frames-1:-n_test_frames-n_validation_frames-1].values\n",
    " -X_regression_train[-1, new_cases_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is at least ordered how I expect, proceed with the modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "ridge_all_features = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = ridge_all_features.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously seen, predict on the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = ridge_all_features.predict(X_regression_train).ravel()\n",
    "y_true = y_regression_train\n",
    "y_naive =y_train_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge Regression model',\n",
    "               suptitle='Naive baseline vs. predictions of training set',\n",
    "              figname='regression_prototype_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_regression_validate\n",
    "y_predict = ridge_all_features.predict(X_regression_validate).ravel()\n",
    "y_naive = y_validate_naive\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge Regression model',\n",
    "               suptitle='Naive baseline vs. predictions, validation set',\n",
    "              figname='regression_prototype_validate_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and conclusion\n",
    "<a id='conclusion'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "As a final measure of the potency of each prototype model, perform predictions on the test data. Because these models are only using a single countries data, I do not view this as snooping the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_naive = y_test_naive\n",
    "y_true = y_test\n",
    "y_predict_r = ridge_all_features.predict(X_regression_test)\n",
    "y_predict_c = cnn.predict(X_cnn_test_model)\n",
    "y_predict_n = nn.predict(X_nn_test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the ridge regression trained on all of the feature data performs very well on the training and validation sets, it does not perform well on the testing set. Presumably this is because I did not retrain the data on the combination of training and validation; i.e. it is using a model which is only trained up to a week into the past. I find this sufficient for these prototype models, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 200\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,5),sharey=True)\n",
    "\n",
    "ymax = np.max([y_true.max(), y_predict.max()])\n",
    "ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "ax1.scatter(y_true, y_predict_r, s=s)\n",
    "ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "ax2.scatter(y_true, y_predict_c, s=s)\n",
    "ax3.plot([0, ymax], [0, ymax],color='r')\n",
    "ax3.scatter(y_true, y_predict_n, s=s)\n",
    "\n",
    "ax1.set_xlabel('True value')\n",
    "ax1.set_ylabel('Predicted value')\n",
    "ax1.set_title('Ridge residual '+str(y_predict_r.ravel()[0]-y_true.ravel()[0]))\n",
    "\n",
    "ax2.set_xlabel('True value')\n",
    "ax2.set_ylabel('Predicted value')\n",
    "ax2.set_title('CNN residual '+str(y_predict_c.ravel()[0]-y_true.ravel()[0]))\n",
    "\n",
    "ax3.set_xlabel('True value')\n",
    "ax3.set_ylabel('Predicted value')\n",
    "ax3.set_title('NN residual '+str(y_predict_n.ravel()[0]-y_true.ravel()[0]))\n",
    "\n",
    "ax1.grid(True)\n",
    "ax2.grid(True)\n",
    "ax3.grid(True)\n",
    "plt.suptitle('Model comparison on hold-out set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Some comments on the relation between X slicing and the original data: \n",
    "\n",
    "    X[-1, :, -1, 2] = model_data.time_index.max() - n_days_into_future \n",
    "    X[-t, :, -1, 2] = (model_data.time_index.max() - n_days_into_future + 1) - t\n",
    "\n",
    "Because ```n_days_into_future == 1``` throughout this notebook, the most recent date in the last frame of X is data.time_index.max()-1.\n",
    "The maximum time_index of the data in y is data.time_index().max(). Therefore, to convert, we have\n",
    "    \n",
    "    X[-1,:,-1,:] = y[-2, :]\n",
    "    \n",
    "or, more generally, \n",
    "\n",
    "    X[-t,:,-1,:] = y[-(n+t), :]\n",
    "    \n",
    "This shows that the staggering has been done correctly. That is, each entry along the first axis of X predicts.\n",
    "Need to make sure that the naive predictions are being chosen correctly. To do so, just look at the instance\n",
    "where n_frames_validation == 1 and n_frames_test == 1 (to make slicing easier) for n_days_into_future == 7.\n",
    "\n",
    "Therefore, when n_validation_frames > 1, the breakdown of the data is as follows:\n",
    "\n",
    "    Naive predictions = X[validation_indices, :, -1, 2]\n",
    "    CNN predictions = model.predict(X[validation_indices, :, :, :])\n",
    "    True future values = y[validation_indices, :]\n",
    "    \n",
    "Where validation indices is assumed to be a correctly formatted array of indices. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
