{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. This process is kept track of by encoding the dates which have interpolated values.\n",
    "There are two measures to do so. Essentially its one hot encoding for the categories ['extrapolated', 'interpolated', 'actual']. The other measure is to track the \"days since infection\" where 0 represents the first day with a recorded\n",
    "case of COVID within that country. I leave the more complex feature creation to the exploratory data analysis portion\n",
    "of this project.\n",
    "\n",
    "Some of the data is currently not used but may be incorporated later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "\n",
    "**Data available at:**\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "\n",
    "**Data available via github**\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "\n",
    "**Data available at:**\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "\n",
    "\n",
    "**If API used to pull data (I elect not to because the datasets are different)**\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "<!-- **Website which lead me to dataset**\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/) -->\n",
    "\n",
    "**Data available at:**\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool|\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "def clean_DataFrame(df):\n",
    "    \"\"\" Remove all NaN or single value columns. \n",
    "    \n",
    "    \"\"\"\n",
    "    # if 0 then column is all NaN, if 1 then could be mix of NaN and a\n",
    "    # single value at most. \n",
    "    df = df.loc[:, df.columns[(df.nunique() > 0)]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    if len(df.index.names) == 1:\n",
    "        placeholder = df.index.name\n",
    "        df = df.reset_index()\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index(placeholder)#.sum()\n",
    "    else:\n",
    "        placeholder = df.index.names[0]\n",
    "        df = df.reset_index(level=0)\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index([placeholder, df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    try:\n",
    "        df_table.columns = df_table.columns.droplevel()\n",
    "        df_table.columns.names = ['']\n",
    "    except:\n",
    "        pass\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n",
    "\n",
    "\n",
    "def country_groupby_indices_list(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "\n",
    "def column_search(df, name):\n",
    "    return df.columns[df.columns.str.contains(name)]\n",
    "\n",
    "\n",
    "def add_time_indices(data_table, index_column='cases'):\n",
    "    indexer = data_table.loc[:, ['location',index_column]].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "    country_groupby_indices = country_groupby_indices_list(data_table)\n",
    "    country_groupby_indices_dropped_nan = country_groupby_indices_list(indexer)\n",
    "    days_since = []\n",
    "    for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "        nonzero_list = list(range(len(c)))\n",
    "        zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "        days_since += list(zero_list)+nonzero_list\n",
    "\n",
    "    data_table.loc[:, 'time_index'] = days_since\n",
    "    data_table.loc[:, 'date_proxy'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))\n",
    "    return data_table\n",
    "\n",
    "\n",
    "def regularize_time_series(df_list):\n",
    "    country_intersection = df_list[0].index.levels[0].unique()\n",
    "    dates_union =  df_list[0].index.levels[1].unique()\n",
    "    dates_intersection =  df_list[0].index.levels[1].unique()\n",
    "\n",
    "    for i in range(len(df_list)-1):\n",
    "        country_intersection = country_intersection.intersection(df_list[i+1].index.levels[0].unique())\n",
    "        dates_union = dates_union.union(df_list[i+1].index.levels[1].unique())\n",
    "        # not really intersection, this is the minimum date that at least one country has data for, in each dataset.\n",
    "        dates_intersection = dates_intersection.intersection(df_list[i+1].index.levels[1].unique())\n",
    "\n",
    "    df_list_intersected = [intersect_country_index(df, country_intersection) for df in df_list]\n",
    "\n",
    "    #This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "    time_normalized_global_data = [resample_dates(df, dates_intersection.normalize()) for df in df_list_intersected]\n",
    "    # To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "    return time_normalized_global_data, dates_intersection, country_intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only used in data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "#the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "#     df = column_or_index_string_reformat(pd.read_csv(x),columns=True,index=False)\n",
    "    df_list.append(tmp_df)\n",
    "\n",
    "daily_reports_df = pd.concat(df_list, axis=0)\n",
    "daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_df.loc[:, 'combined_key'] = (daily_reports_df.province_state.astype('str').replace(to_replace='nan', value='')+' '+ daily_reports_df.location.astype('str')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_df = daily_reports_df.drop(columns=['province_state', 'last_update', 'fips', 'admin2']).set_index(['location','date'])\n",
    "#daily_reports_df = daily_reports_df.groupby(['location','date']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>country_region</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>active</th>\n",
       "      <th>combined_key</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">United States</th>\n",
       "      <th>2020-05-24</th>\n",
       "      <td>US</td>\n",
       "      <td>104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.850652</td>\n",
       "      <td>-82.919891</td>\n",
       "      <td>101.0</td>\n",
       "      <td>Ohio United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>US</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.724964</td>\n",
       "      <td>-103.110817</td>\n",
       "      <td>269.0</td>\n",
       "      <td>Colorado United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>US</td>\n",
       "      <td>75.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.628888</td>\n",
       "      <td>-96.396357</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Oklahoma United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>US</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.076340</td>\n",
       "      <td>-83.067696</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ohio United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.385709</td>\n",
       "      <td>-95.669211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Texas United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         country_region  confirmed  deaths  recovered  \\\n",
       "location      date                                                      \n",
       "United States 2020-05-24             US      104.0     3.0        0.0   \n",
       "              2020-04-30             US      270.0     1.0        0.0   \n",
       "              2020-05-01             US       75.0     8.0        0.0   \n",
       "              2020-05-15             US        5.0     0.0        0.0   \n",
       "              2020-04-24             US        1.0     0.0        0.0   \n",
       "\n",
       "                          latitude  longitude        lat        long  active  \\\n",
       "location      date                                                             \n",
       "United States 2020-05-24       NaN        NaN  40.850652  -82.919891   101.0   \n",
       "              2020-04-30       NaN        NaN  40.724964 -103.110817   269.0   \n",
       "              2020-05-01       NaN        NaN  36.628888  -96.396357    67.0   \n",
       "              2020-05-15       NaN        NaN  39.076340  -83.067696     5.0   \n",
       "              2020-04-24       NaN        NaN  33.385709  -95.669211     1.0   \n",
       "\n",
       "                                    combined_key  \n",
       "location      date                                \n",
       "United States 2020-05-24      Ohio United States  \n",
       "              2020-04-30  Colorado United States  \n",
       "              2020-05-01  Oklahoma United States  \n",
       "              2020-05-15      Ohio United States  \n",
       "              2020-04-24     Texas United States  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_reports_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_df_list = []\n",
    "\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :4]\n",
    "    datecols = tmp_df.iloc[:, 4:]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  reformat_values(catcols.loc[:, 'country_region'], category='location').values\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    global_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))\n",
    "\n",
    "\n",
    "\n",
    "csse_global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "csse_global_time_series_df.index.names = ['location','date']\n",
    "csse_global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "csse_global_time_series_df.columns = ['cases', 'deaths', 'recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df_list = []\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :np.where(tmp_df.columns == '1/22/20')[0][0]]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  catcols.loc[:, 'province_state'].values\n",
    "    \n",
    "    datecols = tmp_df.iloc[:,np.where(tmp_df.columns == '1/22/20')[0][0]:]\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    usa_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))\n",
    "    \n",
    "usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "usa_time_series_df.index.names = ['location','date']\n",
    "usa_time_series_df.columns.names = ['csse_us_timeseries']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df =pd.read_csv('./covid-19-data/public/data/owid-covid-data.csv')\n",
    "owid_df= owid_df[owid_df.new_cases_per_million > 0]#.new_cases_per_million.\n",
    "owid_df = regularize_names(owid_df, datekey='date', locationkey='location').set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>...</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rwanda</th>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>RWA</td>\n",
       "      <td>273</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.077</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>41385.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974</td>\n",
       "      <td>1.642</td>\n",
       "      <td>1854.211</td>\n",
       "      <td>56.0</td>\n",
       "      <td>191.375</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.617</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>El Salvador</th>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>SLV</td>\n",
       "      <td>190</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>29.293</td>\n",
       "      <td>2.004</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.273</td>\n",
       "      <td>5.417</td>\n",
       "      <td>7292.458</td>\n",
       "      <td>2.2</td>\n",
       "      <td>167.295</td>\n",
       "      <td>8.87</td>\n",
       "      <td>2.5</td>\n",
       "      <td>18.8</td>\n",
       "      <td>90.650</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bosnia And Herzegovina</th>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>BIH</td>\n",
       "      <td>2416</td>\n",
       "      <td>10</td>\n",
       "      <td>149</td>\n",
       "      <td>3</td>\n",
       "      <td>736.402</td>\n",
       "      <td>3.048</td>\n",
       "      <td>45.416</td>\n",
       "      <td>0.914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.569</td>\n",
       "      <td>10.711</td>\n",
       "      <td>11713.895</td>\n",
       "      <td>0.2</td>\n",
       "      <td>329.635</td>\n",
       "      <td>10.08</td>\n",
       "      <td>30.2</td>\n",
       "      <td>47.7</td>\n",
       "      <td>97.164</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>PHL</td>\n",
       "      <td>230</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2.099</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.803</td>\n",
       "      <td>2.661</td>\n",
       "      <td>7599.188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>370.437</td>\n",
       "      <td>7.07</td>\n",
       "      <td>7.8</td>\n",
       "      <td>40.8</td>\n",
       "      <td>78.463</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brazil</th>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>BRA</td>\n",
       "      <td>2433</td>\n",
       "      <td>232</td>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>11.446</td>\n",
       "      <td>1.091</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.552</td>\n",
       "      <td>5.060</td>\n",
       "      <td>14103.452</td>\n",
       "      <td>3.4</td>\n",
       "      <td>177.961</td>\n",
       "      <td>8.11</td>\n",
       "      <td>10.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  iso_code  total_cases  new_cases  \\\n",
       "location               date                                          \n",
       "Rwanda                 2020-05-09      RWA          273          2   \n",
       "El Salvador            2020-04-19      SLV          190         13   \n",
       "Bosnia And Herzegovina 2020-05-27      BIH         2416         10   \n",
       "Philippines            2020-03-20      PHL          230         28   \n",
       "Brazil                 2020-03-26      BRA         2433        232   \n",
       "\n",
       "                                   total_deaths  new_deaths  \\\n",
       "location               date                                   \n",
       "Rwanda                 2020-05-09             0           0   \n",
       "El Salvador            2020-04-19             7           0   \n",
       "Bosnia And Herzegovina 2020-05-27           149           3   \n",
       "Philippines            2020-03-20            18           1   \n",
       "Brazil                 2020-03-26            57          11   \n",
       "\n",
       "                                   total_cases_per_million  \\\n",
       "location               date                                  \n",
       "Rwanda                 2020-05-09                   21.077   \n",
       "El Salvador            2020-04-19                   29.293   \n",
       "Bosnia And Herzegovina 2020-05-27                  736.402   \n",
       "Philippines            2020-03-20                    2.099   \n",
       "Brazil                 2020-03-26                   11.446   \n",
       "\n",
       "                                   new_cases_per_million  \\\n",
       "location               date                                \n",
       "Rwanda                 2020-05-09                  0.154   \n",
       "El Salvador            2020-04-19                  2.004   \n",
       "Bosnia And Herzegovina 2020-05-27                  3.048   \n",
       "Philippines            2020-03-20                  0.256   \n",
       "Brazil                 2020-03-26                  1.091   \n",
       "\n",
       "                                   total_deaths_per_million  \\\n",
       "location               date                                   \n",
       "Rwanda                 2020-05-09                     0.000   \n",
       "El Salvador            2020-04-19                     1.079   \n",
       "Bosnia And Herzegovina 2020-05-27                    45.416   \n",
       "Philippines            2020-03-20                     0.164   \n",
       "Brazil                 2020-03-26                     0.268   \n",
       "\n",
       "                                   new_deaths_per_million  total_tests  ...  \\\n",
       "location               date                                             ...   \n",
       "Rwanda                 2020-05-09                   0.000      41385.0  ...   \n",
       "El Salvador            2020-04-19                   0.000      12210.0  ...   \n",
       "Bosnia And Herzegovina 2020-05-27                   0.914          NaN  ...   \n",
       "Philippines            2020-03-20                   0.009          NaN  ...   \n",
       "Brazil                 2020-03-26                   0.052          NaN  ...   \n",
       "\n",
       "                                   aged_65_older  aged_70_older  \\\n",
       "location               date                                       \n",
       "Rwanda                 2020-05-09          2.974          1.642   \n",
       "El Salvador            2020-04-19          8.273          5.417   \n",
       "Bosnia And Herzegovina 2020-05-27         16.569         10.711   \n",
       "Philippines            2020-03-20          4.803          2.661   \n",
       "Brazil                 2020-03-26          8.552          5.060   \n",
       "\n",
       "                                   gdp_per_capita  extreme_poverty  \\\n",
       "location               date                                          \n",
       "Rwanda                 2020-05-09        1854.211             56.0   \n",
       "El Salvador            2020-04-19        7292.458              2.2   \n",
       "Bosnia And Herzegovina 2020-05-27       11713.895              0.2   \n",
       "Philippines            2020-03-20        7599.188              NaN   \n",
       "Brazil                 2020-03-26       14103.452              3.4   \n",
       "\n",
       "                                   cvd_death_rate diabetes_prevalence  \\\n",
       "location               date                                             \n",
       "Rwanda                 2020-05-09         191.375                4.28   \n",
       "El Salvador            2020-04-19         167.295                8.87   \n",
       "Bosnia And Herzegovina 2020-05-27         329.635               10.08   \n",
       "Philippines            2020-03-20         370.437                7.07   \n",
       "Brazil                 2020-03-26         177.961                8.11   \n",
       "\n",
       "                                   female_smokers  male_smokers  \\\n",
       "location               date                                       \n",
       "Rwanda                 2020-05-09             4.7          21.0   \n",
       "El Salvador            2020-04-19             2.5          18.8   \n",
       "Bosnia And Herzegovina 2020-05-27            30.2          47.7   \n",
       "Philippines            2020-03-20             7.8          40.8   \n",
       "Brazil                 2020-03-26            10.1          17.9   \n",
       "\n",
       "                                   handwashing_facilities  \\\n",
       "location               date                                 \n",
       "Rwanda                 2020-05-09                   4.617   \n",
       "El Salvador            2020-04-19                  90.650   \n",
       "Bosnia And Herzegovina 2020-05-27                  97.164   \n",
       "Philippines            2020-03-20                  78.463   \n",
       "Brazil                 2020-03-26                     NaN   \n",
       "\n",
       "                                   hospital_beds_per_100k  \n",
       "location               date                                \n",
       "Rwanda                 2020-05-09                     NaN  \n",
       "El Salvador            2020-04-19                     1.3  \n",
       "Bosnia And Herzegovina 2020-05-27                     3.5  \n",
       "Philippines            2020-03-20                     1.0  \n",
       "Brazil                 2020-03-26                     2.2  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "oxcgrt_df.loc[:, 'date'] = pd.to_datetime(oxcgrt_df.loc[:, 'date'], format='%Y%m%d')\n",
    "oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>c1_flag</th>\n",
       "      <th>c2_workplace_closing</th>\n",
       "      <th>c2_flag</th>\n",
       "      <th>c3_cancel_public_events</th>\n",
       "      <th>c3_flag</th>\n",
       "      <th>c4_restrictions_on_gatherings</th>\n",
       "      <th>c4_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>stringency_index_for_display</th>\n",
       "      <th>stringency_legacy_index</th>\n",
       "      <th>stringency_legacy_index_for_display</th>\n",
       "      <th>government_response_index</th>\n",
       "      <th>government_response_index_for_display</th>\n",
       "      <th>containment_health_index</th>\n",
       "      <th>containment_health_index_for_display</th>\n",
       "      <th>economic_support_index</th>\n",
       "      <th>economic_support_index_for_display</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Guinea Bissau</th>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>GIN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>73.15</td>\n",
       "      <td>73.15</td>\n",
       "      <td>79.29</td>\n",
       "      <td>79.29</td>\n",
       "      <td>59.62</td>\n",
       "      <td>59.62</td>\n",
       "      <td>70.45</td>\n",
       "      <td>70.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seychelles</th>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>Seychelles</td>\n",
       "      <td>SYC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.11</td>\n",
       "      <td>11.11</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.29</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.69</td>\n",
       "      <td>9.09</td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thailand</th>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>THA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.56</td>\n",
       "      <td>80.56</td>\n",
       "      <td>84.05</td>\n",
       "      <td>84.05</td>\n",
       "      <td>80.13</td>\n",
       "      <td>80.13</td>\n",
       "      <td>76.52</td>\n",
       "      <td>76.52</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kyrgyz Republic</th>\n",
       "      <th>2020-01-13</th>\n",
       "      <td>Kyrgyz Republic</td>\n",
       "      <td>KGZ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bulgaria</th>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>BGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               country_name country_code  c1_school_closing  \\\n",
       "location        date                                                          \n",
       "Guinea Bissau   2020-05-01           Guinea          GIN                3.0   \n",
       "Seychelles      2020-02-01       Seychelles          SYC                0.0   \n",
       "Thailand        2020-05-03         Thailand          THA                3.0   \n",
       "Kyrgyz Republic 2020-01-13  Kyrgyz Republic          KGZ                0.0   \n",
       "Bulgaria        2020-02-02         Bulgaria          BGR                0.0   \n",
       "\n",
       "                            c1_flag  c2_workplace_closing  c2_flag  \\\n",
       "location        date                                                 \n",
       "Guinea Bissau   2020-05-01      1.0                   3.0      0.0   \n",
       "Seychelles      2020-02-01      NaN                   0.0      NaN   \n",
       "Thailand        2020-05-03      1.0                   2.0      1.0   \n",
       "Kyrgyz Republic 2020-01-13      NaN                   0.0      NaN   \n",
       "Bulgaria        2020-02-02      NaN                   0.0      NaN   \n",
       "\n",
       "                            c3_cancel_public_events  c3_flag  \\\n",
       "location        date                                           \n",
       "Guinea Bissau   2020-05-01                      2.0      1.0   \n",
       "Seychelles      2020-02-01                      0.0      NaN   \n",
       "Thailand        2020-05-03                      2.0      1.0   \n",
       "Kyrgyz Republic 2020-01-13                      0.0      NaN   \n",
       "Bulgaria        2020-02-02                      0.0      NaN   \n",
       "\n",
       "                            c4_restrictions_on_gatherings  c4_flag  ...  \\\n",
       "location        date                                                ...   \n",
       "Guinea Bissau   2020-05-01                            3.0      1.0  ...   \n",
       "Seychelles      2020-02-01                            0.0      NaN  ...   \n",
       "Thailand        2020-05-03                            3.0      1.0  ...   \n",
       "Kyrgyz Republic 2020-01-13                            0.0      NaN  ...   \n",
       "Bulgaria        2020-02-02                            0.0      NaN  ...   \n",
       "\n",
       "                            stringency_index  stringency_index_for_display  \\\n",
       "location        date                                                         \n",
       "Guinea Bissau   2020-05-01             73.15                         73.15   \n",
       "Seychelles      2020-02-01             11.11                         11.11   \n",
       "Thailand        2020-05-03             80.56                         80.56   \n",
       "Kyrgyz Republic 2020-01-13              0.00                          0.00   \n",
       "Bulgaria        2020-02-02              0.00                          0.00   \n",
       "\n",
       "                            stringency_legacy_index  \\\n",
       "location        date                                  \n",
       "Guinea Bissau   2020-05-01                    79.29   \n",
       "Seychelles      2020-02-01                    14.29   \n",
       "Thailand        2020-05-03                    84.05   \n",
       "Kyrgyz Republic 2020-01-13                     0.00   \n",
       "Bulgaria        2020-02-02                     0.00   \n",
       "\n",
       "                            stringency_legacy_index_for_display  \\\n",
       "location        date                                              \n",
       "Guinea Bissau   2020-05-01                                79.29   \n",
       "Seychelles      2020-02-01                                14.29   \n",
       "Thailand        2020-05-03                                84.05   \n",
       "Kyrgyz Republic 2020-01-13                                 0.00   \n",
       "Bulgaria        2020-02-02                                 0.00   \n",
       "\n",
       "                            government_response_index  \\\n",
       "location        date                                    \n",
       "Guinea Bissau   2020-05-01                      59.62   \n",
       "Seychelles      2020-02-01                       7.69   \n",
       "Thailand        2020-05-03                      80.13   \n",
       "Kyrgyz Republic 2020-01-13                       0.00   \n",
       "Bulgaria        2020-02-02                       0.00   \n",
       "\n",
       "                            government_response_index_for_display  \\\n",
       "location        date                                                \n",
       "Guinea Bissau   2020-05-01                                  59.62   \n",
       "Seychelles      2020-02-01                                   7.69   \n",
       "Thailand        2020-05-03                                  80.13   \n",
       "Kyrgyz Republic 2020-01-13                                   0.00   \n",
       "Bulgaria        2020-02-02                                   0.00   \n",
       "\n",
       "                            containment_health_index  \\\n",
       "location        date                                   \n",
       "Guinea Bissau   2020-05-01                     70.45   \n",
       "Seychelles      2020-02-01                      9.09   \n",
       "Thailand        2020-05-03                     76.52   \n",
       "Kyrgyz Republic 2020-01-13                      0.00   \n",
       "Bulgaria        2020-02-02                      0.00   \n",
       "\n",
       "                            containment_health_index_for_display  \\\n",
       "location        date                                               \n",
       "Guinea Bissau   2020-05-01                                 70.45   \n",
       "Seychelles      2020-02-01                                  9.09   \n",
       "Thailand        2020-05-03                                 76.52   \n",
       "Kyrgyz Republic 2020-01-13                                  0.00   \n",
       "Bulgaria        2020-02-02                                  0.00   \n",
       "\n",
       "                            economic_support_index  \\\n",
       "location        date                                 \n",
       "Guinea Bissau   2020-05-01                     0.0   \n",
       "Seychelles      2020-02-01                     0.0   \n",
       "Thailand        2020-05-03                   100.0   \n",
       "Kyrgyz Republic 2020-01-13                     0.0   \n",
       "Bulgaria        2020-02-02                     0.0   \n",
       "\n",
       "                            economic_support_index_for_display  \n",
       "location        date                                            \n",
       "Guinea Bissau   2020-05-01                                 0.0  \n",
       "Seychelles      2020-02-01                                 0.0  \n",
       "Thailand        2020-05-03                               100.0  \n",
       "Kyrgyz Republic 2020-01-13                                 0.0  \n",
       "Bulgaria        2020-02-02                                 0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_cases = regularize_names(pd.read_csv('test_tracker_cases.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(\n",
    "                            ['location', 'date']).drop(\n",
    "                                    columns=['population','country']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>cases_per100k</th>\n",
       "      <th>deaths_per100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Azerbaijan</th>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>2060</td>\n",
       "      <td>76</td>\n",
       "      <td>26</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somalia</th>\n",
       "      <th>2020-04-26</th>\n",
       "      <td>436</td>\n",
       "      <td>46</td>\n",
       "      <td>23</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Democratic Republic Of The Congo</th>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>98</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cases  new_cases  deaths  \\\n",
       "location                         date                                   \n",
       "Azerbaijan                       2020-05-05   2060         76      26   \n",
       "Somalia                          2020-04-26    436         46      23   \n",
       "Democratic Republic Of The Congo 2020-03-31     98         17       8   \n",
       "\n",
       "                                             cases_per100k  deaths_per100k  \n",
       "location                         date                                       \n",
       "Azerbaijan                       2020-05-05           20.3             0.3  \n",
       "Somalia                          2020-04-26            2.7             0.1  \n",
       "Democratic Republic Of The Congo 2020-03-31            0.1             0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtracker_cases.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_tests = regularize_names(pd.read_csv('test_tracker_tests.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(['location', 'date']).drop(\n",
    "                                    columns=['population','country','source']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>new_tests</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>tests_per100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Faroe Islands</th>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>88</td>\n",
       "      <td>5765</td>\n",
       "      <td>11765.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ukraine</th>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>6971</td>\n",
       "      <td>129723</td>\n",
       "      <td>296.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lithuania</th>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          new_tests  tests_cumulative  tests_per100k\n",
       "location      date                                                  \n",
       "Faroe Islands 2020-04-16         88              5765        11765.3\n",
       "Ukraine       2020-05-03       6971            129723          296.6\n",
       "Lithuania     2020-03-15          0               442           16.2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtracker_tests.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used to model country-wide case numbers exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = [csse_global_time_series_df,\n",
    "                 testtracker_cases, testtracker_tests, oxcgrt_df, owid_df]\n",
    "global_data_export = [csse_global_time_series_df, daily_reports_df, \n",
    "                owid_df, oxcgrt_df, testtracker_cases, testtracker_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2020-01-22 00:00:00 to 2020-05-26 00:00:00\n",
      "The final number of countries included is 136\n"
     ]
    }
   ],
   "source": [
    "names = ['jhucsse', 'jhucsse_reports', 'owid', 'oxcgrt', 'ttc', 'ttt']\n",
    "export_list = []\n",
    "\n",
    "export_list_tmp, dates_intersection, country_intersection = regularize_time_series(global_data_export)\n",
    "print('The range of all dates is from {} to {}'.format(dates_intersection.min(), dates_intersection.max()))\n",
    "print('The final number of countries included is {}'.format(len(country_intersection)))\n",
    "\n",
    "for i, x in enumerate(export_list_tmp):\n",
    "    gd_export_copy = x.copy()\n",
    "    gd_export_copy.columns += '_' + names[i]\n",
    "    export_list.append(gd_export_copy)\n",
    "\n",
    "export_df = multiindex_to_table(pd.concat(export_list, axis=1))\n",
    "export_df = add_time_indices(export_df, index_column='cases_jhucsse')\n",
    "export_df.to_csv('eda_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense, because of the intersections between data; to us the u.s. time series and ihme data together but not with\n",
    "the global data. The hospital data is very useful and so it may be important to look specifically at the small number of countries it contains. Regardless; by using only the global data we can keep 110 countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Want to have all time dependent data defined on the same time ranges for convenience;\n",
    "this involves two steps. 1. Initialize the new dates, 2. deal with the missing values. \n",
    "These missing values references the ones introduced by resampling or redefining the range of \n",
    "each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2020-01-22 00:00:00 to 2020-05-26 00:00:00\n",
      "The number of countries included is 136\n"
     ]
    }
   ],
   "source": [
    "# names = ['jhucsse', 'owid', 'oxcgrt', 'ttc', 'ttt']\n",
    "# dataframe_list = []\n",
    "\n",
    "dataframe_list, modeling_dates, modeling_countries = regularize_time_series(global_data)\n",
    "print('The range of all dates is from {} to {}'.format(modeling_dates.min(), modeling_dates.max()))\n",
    "print('The number of countries included is {}'.format(len(modeling_countries)))\n",
    "\n",
    "# for i, x in enumerate(global_modeling_data_tmp):\n",
    "#     gd_export_copy = x.copy()\n",
    "#     gd_export_copy.columns += '_' + names[i]\n",
    "#     dataframe_list.append(gd_export_copy)\n",
    "\n",
    "df = multiindex_to_table(pd.concat(dataframe_list, axis=1))\n",
    "data_table = add_time_indices(df, index_column='cases')\n",
    "# df.to_csv('eda_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17136, 86)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of redundant data. going to predict the number of new cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groupby_indices = [data_table[data_table.location==country].index for country in data_table.location.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went through the features manually and selected the ones which were not redundant and actually seemed useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['location', 'date', 'cases', 'deaths', 'recovered', 'cases',\n",
       "       'new_cases', 'deaths', 'cases_per100k', 'deaths_per100k', 'new_tests',\n",
       "       'tests_cumulative', 'tests_per100k', 'country_name', 'country_code',\n",
       "       'c1_school_closing', 'c1_flag', 'c2_workplace_closing', 'c2_flag',\n",
       "       'c3_cancel_public_events', 'c3_flag', 'c4_restrictions_on_gatherings',\n",
       "       'c4_flag', 'c5_close_public_transport', 'c5_flag',\n",
       "       'c6_stay_at_home_requirements', 'c6_flag',\n",
       "       'c7_restrictions_on_internal_movement', 'c7_flag',\n",
       "       'c8_international_travel_controls', 'e1_income_support', 'e1_flag',\n",
       "       'e2_debt_contract_relief', 'e3_fiscal_measures',\n",
       "       'e4_international_support', 'h1_public_information_campaigns',\n",
       "       'h1_flag', 'h2_testing_policy', 'h3_contact_tracing',\n",
       "       'h4_emergency_investment_in_healthcare', 'h5_investment_in_vaccines',\n",
       "       'm1_wildcard', 'confirmed_cases', 'confirmed_deaths',\n",
       "       'stringency_index', 'stringency_index_for_display',\n",
       "       'stringency_legacy_index', 'stringency_legacy_index_for_display',\n",
       "       'government_response_index', 'government_response_index_for_display',\n",
       "       'containment_health_index', 'containment_health_index_for_display',\n",
       "       'economic_support_index', 'economic_support_index_for_display',\n",
       "       'iso_code', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths',\n",
       "       'total_cases_per_million', 'new_cases_per_million',\n",
       "       'total_deaths_per_million', 'new_deaths_per_million', 'total_tests',\n",
       "       'new_tests', 'total_tests_per_thousand', 'new_tests_per_thousand',\n",
       "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'tests_units',\n",
       "       'stringency_index', 'population', 'population_density', 'median_age',\n",
       "       'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
       "       'cvd_death_rate', 'diabetes_prevalence', 'female_smokers',\n",
       "       'male_smokers', 'handwashing_facilities', 'hospital_beds_per_100k',\n",
       "       'time_index', 'date_proxy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_indices in country_groupby_indices_list(data_table):\n",
    "    data_table.loc[country_indices, 'population'] = data_table.loc[country_indices, 'population'].fillna(method='ffill').fillna(method='bfill').values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_population = data_table.population / 1000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundant death stats\n",
    "redundant_death_columns = column_search(data_table, 'death').difference(['new_deaths_per_million', 'cvd_death_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_test_columns = column_search(data_table, 'test').difference(['new_tests_per_million','tests_units','h2_testing_policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_case_columns= column_search(data_table, 'cases').difference(['new_cases_per_million'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_new_tests_index = data_table.loc[:, 'new_tests'].isna().sum().argmin()\n",
    "new_tests = data_table.loc[:, 'new_tests'].iloc[:, better_new_tests_index].fillna(0)\n",
    "new_tests_per_million = new_tests / per_million_population\n",
    "data_table.loc[:, 'new_tests_per_million'] = new_tests_per_million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases = data_table.loc[country_groupby_indices_list(data_table)[0],'cases'].iloc[:,0].diff(1).fillna(0)\n",
    "for c_indices in country_groupby_indices_list(data_table)[1:]:\n",
    "    new_cases = pd.concat((new_cases, data_table.loc[c_indices,'cases'].iloc[:,0].diff(1).fillna(0)),axis=0)\n",
    "\n",
    "new_cases_per_million = new_cases / per_million_population\n",
    "data_table.loc[:, 'new_cases_per_million'] = (new_cases / per_million_population).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all but the \"good\" cases data\n",
    "data_table.loc[:, 'new_deaths_per_million'] = data_table.loc[:, 'new_deaths_per_million'].fillna(value=0.)\n",
    "# new_deaths_per_million = (new_deaths / per_million_population)#to_frame(name='new_deaths_per_million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_recovered = data_table.loc[country_groupby_indices[0],'recovered'].diff(1).fillna(0)\n",
    "for c_indices in country_groupby_indices[1:]:\n",
    "    new_recovered = pd.concat((new_recovered, data_table.loc[c_indices,'recovered'].diff(1).fillna(0)),axis=0)\n",
    "\n",
    "data_table.loc[:, 'new_recovered_per_million'] = (new_recovered / per_million_population).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actual new_cases columns aren't really close to actually # of cases.\n",
    "data_table.loc[:, 'new_cases'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_pruned = data_table.drop(columns=(redundant_death_columns.tolist() \n",
    "                         + redundant_test_columns.tolist() \n",
    "                         + redundant_case_columns.tolist()+['recovered']))\n",
    "\n",
    "better_quality_stringency_index = data_table_pruned.loc[:, 'stringency_index'].isna().sum().argmin()\n",
    "stringency = data_table_pruned.loc[:, 'stringency_index'].iloc[:, better_quality_stringency_index]\n",
    "\n",
    "data_table_pruned = data_table_pruned.drop(columns=['country_name', 'country_code',\n",
    "                                                    'm1_wildcard','stringency_index_for_display',\n",
    "                                                   'stringency_legacy_index', 'stringency_legacy_index_for_display',\n",
    "                                                    'government_response_index_for_display',\n",
    "                                                    'containment_health_index_for_display',\n",
    "                                                    'economic_support_index_for_display',\n",
    "                                                    'iso_code','stringency_index'])\n",
    "data_table_pruned.loc[:, 'stringency_index'] = stringency.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_pruned = data_table_pruned.loc[:, ['location','date','date_proxy','time_index'] \n",
    "                                          + data_table_pruned.drop(columns=['location','date','date_proxy','time_index']).columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = ['location','date','date_proxy','time_index']\n",
    "per_mill = ['new_cases_per_million',\n",
    "            'new_tests_per_million', \n",
    "            'new_recovered_per_million', \n",
    "            'new_deaths_per_million']\n",
    "flags = column_search(data_table_pruned,'flag')\n",
    "time_independent = data_table_pruned.loc[:, 'tests_units':'hospital_beds_per_100k'].columns.tolist()\n",
    "time_dependent = data_table_pruned.loc[:, 'c1_school_closing':'economic_support_index'].columns.difference(flags).tolist()\n",
    "flags = flags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_reorder = data_table_pruned.loc[:, indexers+per_mill+time_dependent+time_independent+flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_indices in country_groupby_indices_list(data_table_reorder):\n",
    "    fill_tmp = data_table_reorder.loc[country_indices, time_independent].fillna(method='ffill').fillna(method='bfill')\n",
    "    data_table_reorder.loc[country_indices, time_independent] = fill_tmp.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location                                     0\n",
       "date                                         0\n",
       "date_proxy                                   0\n",
       "time_index                                   0\n",
       "new_cases_per_million                        0\n",
       "new_tests_per_million                        0\n",
       "new_recovered_per_million                    0\n",
       "new_deaths_per_million                       0\n",
       "c1_school_closing                          625\n",
       "c2_workplace_closing                       629\n",
       "c3_cancel_public_events                    627\n",
       "c4_restrictions_on_gatherings              629\n",
       "c5_close_public_transport                  628\n",
       "c6_stay_at_home_requirements               604\n",
       "c7_restrictions_on_internal_movement       625\n",
       "c8_international_travel_controls           651\n",
       "containment_health_index                   741\n",
       "e1_income_support                          676\n",
       "e2_debt_contract_relief                    662\n",
       "e3_fiscal_measures                         794\n",
       "e4_international_support                   811\n",
       "economic_support_index                     716\n",
       "government_response_index                  745\n",
       "h1_public_information_campaigns            662\n",
       "h2_testing_policy                          757\n",
       "h3_contact_tracing                         734\n",
       "h4_emergency_investment_in_healthcare     1051\n",
       "h5_investment_in_vaccines                  896\n",
       "tests_units                               7434\n",
       "population                                   0\n",
       "population_density                         252\n",
       "median_age                                 378\n",
       "aged_65_older                              378\n",
       "aged_70_older                              504\n",
       "gdp_per_capita                             378\n",
       "extreme_poverty                           5670\n",
       "cvd_death_rate                             252\n",
       "diabetes_prevalence                        126\n",
       "female_smokers                            3276\n",
       "male_smokers                              3528\n",
       "handwashing_facilities                    8946\n",
       "hospital_beds_per_100k                    1638\n",
       "c1_flag                                   7851\n",
       "c2_flag                                   8961\n",
       "c3_flag                                   7743\n",
       "c4_flag                                   8473\n",
       "c5_flag                                  10918\n",
       "c6_flag                                   9616\n",
       "c7_flag                                   9126\n",
       "e1_flag                                  12213\n",
       "h1_flag                                   4344\n",
       "dtype: int64"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table_reorder.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: COVID death rate is obviously time dependent, but the form it takes in the reporting is piece-wise constant function, so \n",
    "#### I am going to treat it as \"independent' by simply forward filling values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_independent_features = ['population', 'population_density', 'median_age',\n",
    "       'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
    "       'diabetes_prevalence', 'female_smokers', 'male_smokers',\n",
    "       'handwashing_facilities', 'hospital_beds_per_100k', 'cvd_death_rate']\n",
    "\n",
    "misc_features = ['date', 'location', 'tests_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tests_units</th>\n",
       "      <th>population</th>\n",
       "      <th>population_density</th>\n",
       "      <th>median_age</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_100k</th>\n",
       "      <th>time_index</th>\n",
       "      <th>date_proxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17131</th>\n",
       "      <td>tests performed</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>63</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17132</th>\n",
       "      <td>tests performed</td>\n",
       "      <td>14862927.0</td>\n",
       "      <td>42.729</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.822</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1899.775</td>\n",
       "      <td>21.4</td>\n",
       "      <td>307.846</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>36.791</td>\n",
       "      <td>1.7</td>\n",
       "      <td>64</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17133</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17134</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17136 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tests_units  population  population_density  median_age  \\\n",
       "0                  NaN         NaN                 NaN         NaN   \n",
       "1                  NaN         NaN                 NaN         NaN   \n",
       "2                  NaN         NaN                 NaN         NaN   \n",
       "3                  NaN         NaN                 NaN         NaN   \n",
       "4                  NaN         NaN                 NaN         NaN   \n",
       "...                ...         ...                 ...         ...   \n",
       "17131  tests performed  14862927.0              42.729        19.6   \n",
       "17132  tests performed  14862927.0              42.729        19.6   \n",
       "17133              NaN         NaN                 NaN         NaN   \n",
       "17134              NaN         NaN                 NaN         NaN   \n",
       "17135              NaN         NaN                 NaN         NaN   \n",
       "\n",
       "       aged_65_older  aged_70_older  gdp_per_capita  extreme_poverty  \\\n",
       "0                NaN            NaN             NaN              NaN   \n",
       "1                NaN            NaN             NaN              NaN   \n",
       "2                NaN            NaN             NaN              NaN   \n",
       "3                NaN            NaN             NaN              NaN   \n",
       "4                NaN            NaN             NaN              NaN   \n",
       "...              ...            ...             ...              ...   \n",
       "17131          2.822          1.882        1899.775             21.4   \n",
       "17132          2.822          1.882        1899.775             21.4   \n",
       "17133            NaN            NaN             NaN              NaN   \n",
       "17134            NaN            NaN             NaN              NaN   \n",
       "17135            NaN            NaN             NaN              NaN   \n",
       "\n",
       "       cvd_death_rate  diabetes_prevalence  female_smokers  male_smokers  \\\n",
       "0                 NaN                  NaN             NaN           NaN   \n",
       "1                 NaN                  NaN             NaN           NaN   \n",
       "2                 NaN                  NaN             NaN           NaN   \n",
       "3                 NaN                  NaN             NaN           NaN   \n",
       "4                 NaN                  NaN             NaN           NaN   \n",
       "...               ...                  ...             ...           ...   \n",
       "17131         307.846                 1.82             1.6          30.7   \n",
       "17132         307.846                 1.82             1.6          30.7   \n",
       "17133             NaN                  NaN             NaN           NaN   \n",
       "17134             NaN                  NaN             NaN           NaN   \n",
       "17135             NaN                  NaN             NaN           NaN   \n",
       "\n",
       "       handwashing_facilities  hospital_beds_per_100k  time_index  date_proxy  \n",
       "0                         NaN                     NaN           0           0  \n",
       "1                         NaN                     NaN           0           1  \n",
       "2                         NaN                     NaN           0           2  \n",
       "3                         NaN                     NaN           0           3  \n",
       "4                         NaN                     NaN           0           4  \n",
       "...                       ...                     ...         ...         ...  \n",
       "17131                  36.791                     1.7          63         121  \n",
       "17132                  36.791                     1.7          64         122  \n",
       "17133                     NaN                     NaN          65         123  \n",
       "17134                     NaN                     NaN          66         124  \n",
       "17135                     NaN                     NaN          67         125  \n",
       "\n",
       "[17136 rows x 16 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.loc[:, 'tests_units':].drop(columns=['stringency_index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['location', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['c1_school_closing', 'c1_flag', 'c2_workplace_closing', 'c2_flag',\n",
    "       'c3_cancel_public_events', 'c3_flag', 'c4_restrictions_on_gatherings',\n",
    "       'c4_flag', 'c5_close_public_transport', 'c5_flag',\n",
    "       'c6_stay_at_home_requirements', 'c6_flag',\n",
    "       'c7_restrictions_on_internal_movement', 'c7_flag',\n",
    "       'c8_international_travel_controls', 'e1_income_support', 'e1_flag',\n",
    "       'e2_debt_contract_relief', 'e3_fiscal_measures',\n",
    "       'e4_international_support', 'h1_public_information_campaigns',\n",
    "       'h1_flag', 'h2_testing_policy', 'h3_contact_tracing',\n",
    "       'h4_emergency_investment_in_healthcare', 'h5_investment_in_vaccines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'stringency_index','government_response_index','containment_health_index', 'economic_support_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['location', 'date', 'cases', 'deaths', 'recovered', 'cases',\n",
       "       'new_cases', 'deaths', 'cases_per100k', 'deaths_per100k', 'new_tests',\n",
       "       'tests_cumulative', 'tests_per100k', 'country_name', 'country_code',\n",
       "       'c1_school_closing', 'c1_flag', 'c2_workplace_closing', 'c2_flag',\n",
       "       'c3_cancel_public_events', 'c3_flag', 'c4_restrictions_on_gatherings',\n",
       "       'c4_flag', 'c5_close_public_transport', 'c5_flag',\n",
       "       'c6_stay_at_home_requirements', 'c6_flag',\n",
       "       'c7_restrictions_on_internal_movement', 'c7_flag',\n",
       "       'c8_international_travel_controls', 'e1_income_support', 'e1_flag',\n",
       "       'e2_debt_contract_relief', 'e3_fiscal_measures',\n",
       "       'e4_international_support', 'h1_public_information_campaigns',\n",
       "       'h1_flag', 'h2_testing_policy', 'h3_contact_tracing',\n",
       "       'h4_emergency_investment_in_healthcare', 'h5_investment_in_vaccines',\n",
       "       'm1_wildcard', 'confirmed_cases', 'confirmed_deaths',\n",
       "       'stringency_index', 'stringency_index_for_display',\n",
       "       'stringency_legacy_index', 'stringency_legacy_index_for_display',\n",
       "       'government_response_index', 'government_response_index_for_display',\n",
       "       'containment_health_index', 'containment_health_index_for_display',\n",
       "       'economic_support_index', 'economic_support_index_for_display',\n",
       "       'iso_code', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths',\n",
       "       'total_cases_per_million', 'new_cases_per_million',\n",
       "       'total_deaths_per_million', 'new_deaths_per_million', 'total_tests',\n",
       "       'new_tests', 'total_tests_per_thousand', 'new_tests_per_thousand',\n",
       "       'new_tests_smoothed', 'new_tests_smoothed_per_thousand', 'tests_units',\n",
       "       'stringency_index', 'population', 'population_density', 'median_age',\n",
       "       'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
       "       'cvd_death_rate', 'diabetes_prevalence', 'female_smokers',\n",
       "       'male_smokers', 'handwashing_facilities', 'hospital_beds_per_100k',\n",
       "       'time_index', 'date_proxy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_independent_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature corresponding to new tests per million, to maintain consistency with cases per million and deaths\n",
    "per million.\n",
    "\n",
    "For whatever reason, the population values for Kosovo are missing; I am inserting approximates take from Google searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_indices in country_groupby_indices:\n",
    "    df.loc[country_indices, time_independent_features] = df.loc[country_indices, time_independent_features].fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "per_million = df.population / 1000000\n",
    "df.loc[:, 'new_tests_per_million'] = df.loc[:, 'new_tests'] / per_million\n",
    "df = df.drop(columns='new_tests')\n",
    "time_dependent_features.pop()\n",
    "time_dependent_features.append('new_tests_per_million')\n",
    "df.loc[df.population[df.population.isna()].index,'population_density'] = 154\n",
    "df.loc[df.population[df.population.isna()].index,'population'] = 1845000\n",
    "for country_indices in country_groupby_indices:\n",
    "    df.loc[country_indices, time_dependent_features] = df.loc[country_indices, time_dependent_features].fillna(method='ffill').fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_cases_per_million = data_table.loc[:, data_table.columns[data_table.columns.str.contains('new_cases')].unique()].iloc[:,1]#.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usaind = df[df.location=='United States'].index \n",
    "df[df.location=='United States'].new_cases_per_million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['location','new_cases_per_million']].set_index('location').idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.population < 100000].location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['location','new_cases_per_million']].set_index('location').stack().groupby(level=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(((my_new_cases_per_million / (df.population/1000000.)).fillna(method='ffill').fillna(value=0), df.location),axis=1).set_index('location').stack().groupby(level=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_cases_per_million / (df.population/1000000.).fillna(method='ffill').fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_cases_per_million# / (df.population/1000000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate new tests per million people to match case and death data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was planning on using a \"days since first case\" variable, which would equal zero until the date of the\n",
    "first case, but I believe this would correlate too strongly with the target variable. To test this assumption I'll compute it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I misinterpreted the fact that days since first case is linear growth by defininition (really has the shape of a ReLU) and number of cases is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have aggregated and dropped the respective features, the missing values of the remaining data can be flagged and\n",
    "created into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases_pos = data.iloc[:,0].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "country_groupby_indices_dropped_nan = [n_cases_pos[n_cases_pos.location==country].index for country in n_cases_pos.location.unique()]\n",
    "\n",
    "days_since = []\n",
    "for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "    nonzero_list = list(range(len(c)))\n",
    "    zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "    days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "df.loc[:, 'time_index'] = days_since\n",
    "df.loc[:, 'date_proxy'] = len(df.location.unique())*list(range(len(df.date.unique())))\n",
    "time_dependent_features += ['date_proxy', 'time_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'new_cases_per_million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_ts = ['new_cases_per_million','new_deaths_per_million','new_tests_per_million']\n",
    "df.loc[:, per_million_ts] = df.loc[:,per_million_ts].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.loc[:, flag_features].fillna('Missing').astype('category')\n",
    "for col in tmp.columns:\n",
    "    tmp.loc[:, col] = tmp.loc[:, col].cat.rename_categories({1.0 : '1', 0. : '0'})\n",
    "    \n",
    "dummy_tmp = pd.get_dummies(tmp)\n",
    "flag_data = dummy_tmp[dummy_tmp.columns[~dummy_tmp.columns.str.contains('Missing')]]\n",
    "df = df.drop(columns=flag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dependent_data = df.loc[:, time_dependent_features]\n",
    "time_independent_data= df.loc[:, time_independent_features]\n",
    "misc_data = df.select_dtypes(include=['object', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data = pd.concat((time_dependent_data, time_independent_data,\n",
    "                          flag_data, misc_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_data.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook uses a variety of different COVID-19 related datasets to explore the behavior\n",
    "of the multiple time series'. This notebook also creates new features that attempt to encapsulate the\n",
    "time dependent (and time delayed) nature of the problem; these will be used during the model creation\n",
    "project which makes time dependent forecasting models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "## [Function definitions](#generalfunctions)\n",
    "\n",
    "## [Data](#imports)\n",
    "\n",
    "## [Exploratory Data Analysis](#EDA)\n",
    "\n",
    "## [Feature production](#newfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions <a id='generalfunctions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_means(df, features, roll_widths):\n",
    "    new_feature_df_list = []\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values \n",
    "        rollmean = pd.DataFrame(df.groupby(by='location').rolling(window).mean().fillna(value=0.))\n",
    "#         rollstd = pd.DataFrame(df.groupby(by='location').rolling(window).std().fillna(value=0.))    \n",
    "#         new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        new_features = rollmean\n",
    "        new_cols = features +'_rolling_mean_' + str(window)\n",
    "#         rsind = features +'_rolling_std_' + str(window)\n",
    "#         new_cols = rmind.append(rsind)\n",
    "        new_features.columns = new_cols\n",
    "        new_feature_df_list.append(new_features)\n",
    "    return new_feature_df_list\n",
    "\n",
    "def tsplot(data, roll_width, **kw):\n",
    "    rollmean = datatmp.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "    rollstd  = datatmp.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "    cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(range(len(datatmp)), cis[0], cis[1], alpha=0.5)\n",
    "    ax.plot(range(len(datatmp)), rollmean, color='k', **kw)\n",
    "    return ax\n",
    "\n",
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "\n",
    "def column_search(df, name):\n",
    "    return df.columns[df.columns.str.contains(name)]\n",
    "\n",
    "def country_groupby_indices_list(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n",
    "\n",
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool|\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a id='imports'></a>\n",
    "\n",
    "Differences in reporting\n",
    "Differences in government responses\n",
    "differences in time series.\n",
    "\n",
    "testing vs cases vs deaths\n",
    "log-log plot for current growth trends\n",
    "bollinger bands\n",
    "histogram of trending upwards, flat, downwards\n",
    "tools, different plots, correlation plots scatter matrix plots. \n",
    "\n",
    "Going to remove microstates like San Marino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('eda_data.csv', index_col=0)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index(['location', 'date']).isna().groupby(level=1).sum().loc[:, column_name_search(data, 'new_test')].iloc[:,[0,-1]].plot()\n",
    "\n",
    "Estimation of actual death rates, assuming 50% of people are asymptomatic and do not actually get tested.\n",
    "\n",
    "today_data = data[data.date_proxy == data.date_proxy.max()]\n",
    "\n",
    "today_data.loc[:, 'estimation_death_rate'] = 100 * today_data.loc[:, 'deaths_jhucsse'].values / (2. * today_data.loc[:, 'cases_jhucsse'].values)\n",
    "\n",
    "(weighted_dr).plot.hist(bins=50)\n",
    "\n",
    "data.loc[:,column_name_search(data, 'long').tolist() + column_name_search(data, 'lat').tolist()].iloc[:, [1,3]].plot.hist()\n",
    "\n",
    "data.loc[:, 'population_owid'] = data.loc[:, 'population_owid'].fillna(method='bfill').fillna(method='ffill')\n",
    "data.loc[:, 'population_density_owid'] = data.loc[:, 'population_density_owid'].fillna(method='bfill').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 'new_cases_per_million_owid'] = data.loc[:, 'new_cases_owid'] / (data.loc[:, 'population_owid'] / 1000000.)\n",
    "data.loc[:, 'new_cases_per_million_ttc'] = data.loc[:, 'new_cases_ttc'] / (data.loc[:, 'population_owid'] / 1000000.)\n",
    "\n",
    "ax = (data.groupby('location').sum().loc[:, ['new_cases_per_million_ttc','new_cases_per_million_owid']].diff(axis=1)).plot(label='tt')\n",
    "# data.groupby('location').sum().loc[:, 'new_cases_per_million_owid'].plot(ax=ax, label='owid')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature production <a id='newfeatures'></a>\n",
    "\n",
    "function ```append_rolling_values``` is not working. Need to compute rolling averages for each\n",
    "countries time series' individually but want to store them in the multi index DataFrame. \n",
    "\n",
    "\n",
    "\n",
    "Before interpolation and backfilling, I used to prune countries which did not have cases prior\n",
    "to responses (i.e. \"early responders\" were not included)\n",
    "To make my life easier, I'm only taking data which had cases before all government mandates so the rates before and after are well defined. We can think of these as being \"late responders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((data.loc[:, 'date_proxy':'time_index'], time_dependent.drop(columns='location'), \n",
    "                pd.concat(rolling_predictors, axis=1).reset_index(drop=True), time_independent,\n",
    "                   location_one_hot, tests_units_one_hot, flag_and_misc),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.location=='United States'].loc[:, df.columns[df.columns.str.contains('new_cases')]].plot(legend=False, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has the following partitions:\n",
    "\n",
    "columns until and not including 'population' are time series variables.\n",
    "        'population':'Albania' : time independent, continuous variables\n",
    "        'Albania':'location' : one-hot encoded variables\n",
    "        'location:' location and date, not encoded.\n",
    "        \n",
    "Time series variables with drift as baseline model : any columns with 'new' (cases, deaths, tests)\n",
    "Time series variables with naive as baseline model : The complement to the drift baseline variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis<a id='EDA'></a>\n",
    "Ideas for the inclusion or creation of new columns.\n",
    "\n",
    "Moving averages\n",
    "fourier\n",
    "signal\n",
    "flags for lots of different things\n",
    "\n",
    "hardest hit countries\n",
    "\n",
    "days since\n",
    "\n",
    "extrapolated, actual, interpolated\n",
    "\n",
    "which dataset it came from\n",
    "\n",
    "humans view, interpret and forecast things in a way which are not available to robots. \n",
    "data driven, time dependent manner of modeling. Really trying to encapsulate the time dependence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government_responses = pd.concat((data.loc[:, ['location','date']],\n",
    "                                  data.loc[:, column_search(data, 'oxcgrt')].drop(\n",
    "                                      columns=column_search(data, 'flag')).iloc[:,2:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government_responses = pd.concat((data.loc[:, ['location','date']],\n",
    "                                  data.loc[:, column_search(data, 'oxcgrt')].drop(\n",
    "                                      columns=column_search(data, 'flag')).iloc[:,2:10]), axis=1)\n",
    "\n",
    "government_responses = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "government_responses.loc[:, 'date'] = pd.to_datetime(government_responses.loc[:, 'date'], format='%Y%m%d')\n",
    "# oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')\n",
    "\n",
    "government_responses = pd.concat((government_responses.loc[:, ['location', 'date']], government_responses.iloc[:,3:18:2]), axis=1)\n",
    "\n",
    "government_responses.loc[:, 'date'] = government_responses.date.dt.normalize().values\n",
    "\n",
    "\n",
    "\n",
    "countries_with_all_responses = None\n",
    "for i, country_indices in enumerate(country_groupby_indices_list(government_responses)):\n",
    "    government_responses.loc[country_indices,:] = government_responses.loc[country_indices,:].fillna(method='ffill')\n",
    "    has_all_flags = (government_responses.loc[country_indices,:].max() == 0).sum()\n",
    "    if has_all_flags != 0:\n",
    "        pass\n",
    "    else:\n",
    "        if countries_with_all_responses is None:\n",
    "            countries_with_all_responses = government_responses.loc[country_indices,:]\n",
    "        else:\n",
    "            countries_with_all_responses = pd.concat((countries_with_all_responses, government_responses.loc[country_indices,:]),axis=0)\n",
    "\n",
    "# government_responses = government_responses.dropna(axis=0).iloc[:,:10]\n",
    "\n",
    "countries_with_all_responses.loc[:, 'date'] = pd.to_datetime(countries_with_all_responses.date).dt.normalize()\n",
    "\n",
    "see_if_country_samples =countries_with_all_responses.groupby('location').count().sort_values(by='date') \n",
    "drop_these_countries = np.unique(np.where(~(see_if_country_samples == 148))[0])\n",
    "countries_with_all_responses = countries_with_all_responses[countries_with_all_responses.location != 'Kosovo']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government_responses = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "government_responses.loc[:, 'date'] = pd.to_datetime(government_responses.loc[:, 'date'], format='%Y%m%d')\n",
    "# oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ranges = []\n",
    "for country_indices in country_groupby_indices_list(countries_with_all_responses):\n",
    "    tmp = countries_with_all_responses.loc[country_indices,:].replace(to_replace=[0,0.], value=np.nan)\n",
    "    for c in tmp.columns[2:]:\n",
    "        active_range = tmp.set_index('date').loc[:,c].dropna()\n",
    "        response_ranges.append(pd.IndexSlice[active_range.index.min():active_range.index.max()])\n",
    "\n",
    "response_slices_df = pd.DataFrame(np.array(response_ranges).reshape(countries_with_all_responses.location.nunique(), -1))\n",
    "\n",
    "country_list = []\n",
    "slice_list = []\n",
    "\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    before_list = []\n",
    "    after_list = []\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "#         after = effective_range.reset_index().Date.max()\n",
    "        slice_list += [before]   \n",
    "        \n",
    "enacted_ended_df = pd.DataFrame(np.array(slice_list).reshape(len(country_list), -1), index=country_list, columns=all_responses.columns)\n",
    "\n",
    "all_responses = response_df.iloc[:, [0, 1, 2, 3, 5, 6]]\n",
    "country_list = []\n",
    "minmax_list = []\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "        after = effective_range.reset_index().Date.max()\n",
    "        minmax_list += [before, after]   \n",
    "\n",
    "start_end_columns = np.array([[x+'_start', x+'_end'] for x in all_responses.columns.tolist()]).ravel()\n",
    "start_end_df = pd.DataFrame(np.array(minmax_list).reshape(len(country_list), -1), index=country_list, columns=start_end_columns)\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]\n",
    "\n",
    "first_response_dates = start_end_df.min(axis=1).sort_index()\n",
    "first_response_dates.head(10)\n",
    "\n",
    "first_case_dates = test_multiindex_df.reset_index(level=1).groupby(level=0).Date.min().sort_index()\n",
    "first_case_dates.head(10)\n",
    "\n",
    "dates_with_test_data = test_multiindex_df.tests_cumulative.dropna()\n",
    "dates_with_test_data.head()\n",
    "\n",
    "min_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "\n",
    "first_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "last_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.max()\n",
    "\n",
    "first_testing_dates.reset_index()\n",
    "\n",
    "# convert entire dataframe to index so it can be used to slice testing data, dataframe\n",
    "first_tmp =  first_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "last_tmp =  last_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "first_tmp.head()\n",
    "\n",
    "test_min = test_multiindex_df.loc[first_tmp.index, :]\n",
    "test_max = test_multiindex_df.loc[last_tmp.index, :]\n",
    "\n",
    "# reset index so we can subtract datetime variables.\n",
    "test_max_reset = test_max.reset_index(level=1)\n",
    "test_min_reset = test_min.reset_index(level=1)\n",
    "time_differential = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "testing_rates = np.log(test_max_reset.tests_cumulative / test_min_reset.tests_cumulative)# / time_intervals\n",
    "\n",
    "\n",
    "test_final_test_initial_time_intervals = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "\n",
    "case_response_differential = (first_case_dates-first_response_dates).dt.days\n",
    "\n",
    "late_response = case_response_differential < 0\n",
    "late_response\n",
    "\n",
    "states_to_inspect = ['Michigan', 'Georgia', 'New York', 'Texas']\n",
    "\n",
    "dead=us_deaths[us_deaths['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed=us_cases[us_cases['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed.head()\n",
    "\n",
    "\n",
    "since_first_case_normalized_u = u.replace(to_replace=[0,0.], value=np.nan)\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n",
    "death_rate_df = since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].copy()\n",
    "death_rate_normalized = 100 * since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].values / since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "death_rate_df.loc[:, :] = death_rate_normalized\n",
    "\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n",
    "\n",
    "\n",
    "first_case_dates.astype('category').cat.codes.plot.hist(bins=50)\n",
    "\n",
    "pd.concat(new_feature_df_list,ignore_index=False).sort_index(axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "death_rate_df.transpose().plot().legend(bbox_to_anchor=(1, 1))\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "plt.grid(True, axis='both')\n",
    "plt.title('Death rate by state')\n",
    "plt.savefig('death_rate_NY_MI_GA.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax,ax2) = plt.subplots(1, 2, sharey=True,  figsize=(20,5), dpi=200)\n",
    "confirmed.loc[:, '2/21/20':].transpose().plot(ax=ax).legend(bbox_to_anchor=(0.2, 1))\n",
    "dead.loc[:, '2/21/20':].transpose().plot(ax=ax2).legend(bbox_to_anchor=(0.2, 1))\n",
    "ax.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax.set_title('Number of confirmed cases vs. time')\n",
    "ax2.set_title('Number of diseased vs. time')\n",
    "ax.grid(True, axis='both')\n",
    "ax2.grid(True, axis='both')\n",
    "plt.savefig('cases_vs_dead_comparison_GA_NY_MI.png', bbox_inches='tight')\n",
    "\n",
    "def top_5_counties(state_df, state_name):\n",
    "    state = state_df[(state_df.Province_State==state_name)]\n",
    "    state = state.drop(columns=['UID','iso2','iso3','code3','FIPS','Country_Region','Lat','Long_','Combined_Key','Province_State'])\n",
    "    top5_counties = state.groupby(by='Admin2').sum().sum(axis=1).sort_values(ascending=False)[:5].index.tolist()\n",
    "    state_info = state[state.Admin2.isin(top5_counties)].set_index('Admin2').transpose()\n",
    "    state_info.columns.name = 'County'\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_recovered_dates_only = global_recovered.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_confirmed_dates_only = global_confirmed.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_dead_dates_only = global_dead.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "\n",
    "global_dead['type']='Dead'\n",
    "global_confirmed['type']='Confirmed'\n",
    "global_recovered['type']='Recovered'\n",
    "\n",
    "dead=global_dead[global_dead['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "confirmed=global_confirmed[global_confirmed['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "\n",
    "global_dead = global_dead.sort_index(axis=1)\n",
    "global_confirmed = global_confirmed.sort_index(axis=1)\n",
    "global_recovered = global_recovered.sort_index(axis=1)\n",
    "\n",
    "skr = global_confirmed.groupby('Country/Region').sum().iloc[143, :].loc['1/22/20':'4/28/20']\n",
    "skr.head()\n",
    "\n",
    "top10 = global_confirmed.groupby('Country/Region').sum().loc[:, '1/22/20':'4/28/20'].sort_values(by='4/28/20').iloc[-10:, :]\n",
    "skr = global_confirmed.groupby('Country/Region').sum().loc['Korea, South', '1/22/20':'4/28/20']\n",
    "\n",
    "top10_and_south_korea = pd.concat((top10, skr.to_frame(name='South Korea').transpose()),axis=0).sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "for i, country_time_series in enumerate(top10_and_south_korea.replace(to_replace=[0,0.], value=np.nan).values):\n",
    "    nan_count = np.sum(np.isnan(country_time_series))\n",
    "    days_since_first = np.roll(country_time_series, -nan_count)\n",
    "    plt.plot(days_since_first, label=top10_and_south_korea.index[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "global_dead_dates_only\n",
    "\n",
    "dsum = global_dead_dates_only.sum()\n",
    "csum = global_confirmed_dates_only.sum()\n",
    "drsum = 100*dsum/csum\n",
    "drsum.plot()\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "_ = plt.title('Average global death rate vs. time')\n",
    "plt.grid(True, axis='both')\n",
    "plt.savefig('death_rate_global.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_case_dates = case_df.reset_index().set_index(['Country','date']).total_cases.replace(\n",
    "                           to_replace=0,value=np.nan).dropna().reset_index(level=1).groupby(level=0).date.min()\n",
    "\n",
    "first_response_dates = response_df.min(axis=1)\n",
    "tmp = response_df.copy()\n",
    "dt = pd.DataFrame(np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1])))\n",
    "diff_df = tmp - np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1]))\n",
    "num_miss=diff_df.where(diff_df > pd.Timedelta(days=0)).isna().sum(1).sort_values(ascending=False)\n",
    "countries_with_cases_before_responses = num_miss.where(num_miss==0).dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = case_multiindex_df.join(test_multiindex_df, lsuffix='_x', rsuffix='_y').sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the time series, fill in with missing values with nan. \n",
    "data = data.reindex(pd.MultiIndex.from_product([data.index.levels[0], \n",
    "                    data.index.get_level_values(1).unique().sort_values()], names=['Country', 'Date']), fill_value=np.nan)\n",
    "\n",
    "# Don't use zeros this messes things up.\n",
    "data.loc[:, 'total_cases'] = data.loc[:, 'total_cases'].replace(to_replace=[0,0.], value=np.nan)\n",
    "# instantiate with copy so that we can iterate over DataFrame groupby\n",
    "data.loc[:, 'total_cases_interpolated'] = data.loc[:, 'total_cases'].copy()\n",
    "data.loc[:, 'tests_cumulative_interpolated'] = data.loc[:, 'tests_cumulative'].copy()\n",
    "\n",
    "for country, country_df in data.groupby(level=0):\n",
    "    data.loc[country, 'total_cases_interpolated'] = country_df.loc[:, 'total_cases'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'tests_cumulative_interpolated'] = country_df.loc[:, 'tests_cumulative'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'population'] = country_df.loc[:, 'population'].fillna(method='backfill')\n",
    "\n",
    "data.loc[:, 'cases_per_1M_people_per_100k_tests'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).values\n",
    "data.loc[:, 'cases_per_1M_people'] = (data.total_cases_interpolated / ((data.population/1000000.))).values\n",
    "\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).cumsum().apply(np.log)\n",
    "\n",
    "before_minus_after = response_multiindex_df.applymap(multiindex_response_date_to_average_rates).replace(to_replace=0., value=np.nan).sort_index()\n",
    "\n",
    "before_minus_after_residual_values =  before_minus_after.values - np.tile(before_minus_after.mean(1).values.reshape(-1,1), (1, 5))\n",
    "before_minus_after_residual_df = pd.DataFrame(before_minus_after_residual_values.reshape(-1, 5), columns=before_minus_after.columns, index=before_minus_after.index)\n",
    "before_minus_after_residual_df.head()\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / data.tests_cumulative_interpolated).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
