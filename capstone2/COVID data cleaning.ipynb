{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. This process is kept track of by encoding the dates which have interpolated values.\n",
    "There are two measures to do so. Essentially its one hot encoding for the categories ['extrapolated', 'interpolated', 'actual']. The other measure is to track the \"days since infection\" where 0 represents the first day with a recorded\n",
    "case of COVID within that country. I leave the more complex feature creation to the exploratory data analysis portion\n",
    "of this project.\n",
    "\n",
    "Some of the data is currently not used but may be incorporated later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "<!-- ## [The COVID tracking project testing data.](#source1)\n",
    "[https://covidtracking.com/api](https://covidtracking.com/api)\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "\n",
    "**Data available at:**\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "\n",
    "**Data available via github**\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "\n",
    "**Data available at:**\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "\n",
    "\n",
    "**If API used to pull data (I elect not to because the datasets are different)**\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "<!-- **Website which lead me to dataset**\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/) -->\n",
    "\n",
    "**Data available at:**\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)\n",
    "\n",
    "\n",
    "# Datasets currently not used\n",
    "\n",
    "## [Delphi-epidata ](#delphi) which contains \n",
    "       Facebook surveys, google surveys, doctor visits, google health trends, quidel test data\n",
    "[https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)\n",
    "\n",
    "I have not dove into this dataset too thoroughly but it contains information from facebook and google\n",
    "surveys regarding COVID as well as doctor visits; the doctor visit data attempts to make distinctions between\n",
    "those sick with the annual influenza and those with COVID.\n",
    "\n",
    "\n",
    "## [IHME hospital data](#ihme)\n",
    "\n",
    "**Data available at:**\n",
    "[http://www.healthdata.org/covid/data-downloads](http://www.healthdata.org/covid/data-downloads)\n",
    "\n",
    "The IHME hospital data is one of the more unique datasets I've discovered with \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "\n",
    "def column_or_index_string_reformat(df, columns=True, index=False, dt_formats=('%m/%d/%y', '%Y-%m-%d')):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    if columns:\n",
    "        reformatted_column_names = []\n",
    "        for c in df.columns:\n",
    "            # handle labels which can be cast to datetime objects\n",
    "            try:\n",
    "                reformatted_column_names.append(datetime.strftime(\n",
    "                    datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "            except ValueError:\n",
    "                reformatted_column_names.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c)\n",
    "                                                                .lower()).split()))\n",
    "        df.columns = reformatted_column_names        \n",
    "        \n",
    "    if index:\n",
    "        # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "        \n",
    "        \n",
    "        reformatted_country_names = []\n",
    "        for c in df.index.get_level_values(0):\n",
    "            reformatted_country_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "                                                        .split()).title())\n",
    "        \n",
    "        reformatted_dates = pd.to_datetime(df.index.get_level_values(1)).normalize()\n",
    "        restored_columns = df.index.names\n",
    "        df = df.reset_index()\n",
    "        df.loc[:, restored_columns[0]] = reformatted_country_names\n",
    "        df.loc[:, restored_columns[1]] = reformatted_dates\n",
    "        df = df.set_index(restored_columns).sort_index()\n",
    "        \n",
    "#     if index:\n",
    "#         # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "#         reformatted_index_names = []\n",
    "#         for c in df.index.get_level_values(0):\n",
    "#             # handle labels which can be cast to datetime objects\n",
    "#             try:\n",
    "#                 reformatted_index_names.append(datetime.strftime(\n",
    "#                     datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "#             except ValueError:\n",
    "#                 reformatted_index_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "#                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "#                                                         .split()).title())\n",
    "#         restored_column = df.index.names[0]\n",
    "#         df = df.reset_index(level=0)\n",
    "#         df.loc[:, restored_column] = reformatted_index_names\n",
    "#         df = df.set_index([restored_column, df.index]).sort_index()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def csse_daily_reports_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE daily report data from local machine. \n",
    "    \"\"\"\n",
    "    csv_different_formats_list = []\n",
    "    \n",
    "    #the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_daily_reports/*'):\n",
    "        if os.path.isdir(x):\n",
    "            df_list = []\n",
    "            for days in glob.glob(x+'/*'):\n",
    "                df = pd.read_csv(days)\n",
    "                df_list.append(df)\n",
    "            csv_different_formats_list.append(column_or_index_string_reformat(pd.concat(df_list, axis=0).reset_index(drop=True)))\n",
    "    \n",
    "#     df_list = []\n",
    "#     for daily_report in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "#         # if os.path.isdir(x):\n",
    "#         # for days in glob.glob(x+'/*'):\n",
    "#         df = pd.read_csv(daily_report)\n",
    "#         df_list.append(df)\n",
    "#     daily_reports_df = column_or_index_string_reformat(pd.concat(df_list, axis=0).reset_index(drop=True))\n",
    "    \n",
    "    \n",
    "    # concatenate the data\n",
    "    daily_reports_df = pd.concat(csv_different_formats_list).reset_index(drop=True)\n",
    "    # convert the date-like variable to datetime\n",
    "    daily_reports_df.loc[:, 'last_update'] = pd.to_datetime(daily_reports_df.last_update).dt.normalize()\n",
    "    # In the reporting there are duplicate values. Also, I'm aggregating by country because the other datasets\n",
    "    # are not nearly as detailed. Probably should flag this somehow. \n",
    "    daily_reports_df = daily_reports_df.drop_duplicates().groupby(['country_region','last_update']).sum()\n",
    "    # Reformat the location names and datetime index. Look at documentation above for details. \n",
    "    daily_reports_df = column_or_index_string_reformat(daily_reports_df, index=True, columns=True)\n",
    "    # name the indices and columns for later concatenation\n",
    "    daily_reports_df.index.names = ['location','date']\n",
    "    daily_reports_df.columns.names = ['csse_global_daily_reports']\n",
    "    return daily_reports_df\n",
    "    \n",
    "def csse_timeseries_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE time series data from local machine. \n",
    "    \"\"\"\n",
    "    global_df_list = []\n",
    "\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "        global_tmp = column_or_index_string_reformat(pd.read_csv(x))\n",
    "        # only include the actual time series info; this removes latitude and \n",
    "        # longitude as well as other useless data.\n",
    "        global_specific_indice_list = [1] + list(range(4, global_tmp.shape[1]))\n",
    "        global_tmp = global_tmp.iloc[:,global_specific_indice_list].groupby(by='country_region').sum()\n",
    "        # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "        time_series_name = '_'.join(x.split('.')[0].split('_')[-2:][::-1])\n",
    "        global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    # concatenate the data and name it to abide by my convention. \n",
    "    global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "    global_time_series_df.index.names = ['location','date']\n",
    "    global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "    global_time_series_df = column_or_index_string_reformat(global_time_series_df, index=True, columns=False)\n",
    "\n",
    "    # Repeat the steps above but for United States statewide data. \n",
    "    usa_df_list = []\n",
    "    for y in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "        usa_tmp = column_or_index_string_reformat(pd.read_csv(y))\n",
    "        try:\n",
    "            usa_tmp = usa_tmp.drop(columns='population')\n",
    "        except: \n",
    "            pass\n",
    "        usa_specific_indice_list = [6] + list(range(10, usa_tmp.shape[1]))\n",
    "        usa_tmp = usa_tmp.iloc[:,usa_specific_indice_list].groupby(\n",
    "            by='province_state').sum()\n",
    "        time_series_name = '_'.join(y.split('.')[0].split('_')[-2:][::-1])\n",
    "        usa_tmp.index.name = 'state'\n",
    "        usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "    usa_time_series_df.index.names = ['location','date']\n",
    "    usa_time_series_df.columns.names = ['csse_us_timeseries']\n",
    "    usa_time_series_df = column_or_index_string_reformat(usa_time_series_df, index=True, columns=False)\n",
    "    \n",
    "    return global_time_series_df, usa_time_series_df\n",
    "\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    df = df.reset_index(level=0)\n",
    "    df.loc[:,'location'] = df.loc[:,'location'].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    df = df.set_index(['location', df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    try:\n",
    "        df_table.columns = df_table.columns.droplevel()\n",
    "        df_table.columns.names = ['']\n",
    "    except:\n",
    "        pass\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "def flag_nan_differences(df, df_altered, suffix):\n",
    "    # Use bitwise XOR to flag the values which have been changed from NaN to something else.\n",
    "    # values which get mapped true -> false are those that are changed. \n",
    "    flag_df = df.isna() ^ df_altered.isna()\n",
    "    z1 = tuple(flag_df.columns.get_level_values(0).tolist())\n",
    "    z2 = tuple((flag_df.columns.get_level_values(1) + suffix).tolist())\n",
    "    flag_df.columns = pd.MultiIndex.from_tuples(list(zip(z1,z2)),names=['dataset', 'features'])\n",
    "    return flag_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function declared for this purpose, import and reform JHU CSSE data. Likewise, for\n",
    "the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_global_daily_reports_df = csse_daily_reports_reformat().loc[:, ['confirmed','active','deaths','recovered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csse_global_daily_reports</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>active</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spain</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>222.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saint Lucia</th>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tunisia</th>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>1032.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colombia</th>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>6507.0</td>\n",
       "      <td>4775.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>1439.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "csse_global_daily_reports  confirmed  active  deaths  recovered\n",
       "location    date                                               \n",
       "Spain       2020-03-04         222.0     0.0     2.0        2.0\n",
       "Saint Lucia 2020-05-11          18.0     1.0     0.0       17.0\n",
       "Tunisia     2020-05-11        1032.0   287.0    45.0      700.0\n",
       "Malaysia    2020-03-04          50.0     0.0     0.0       22.0\n",
       "Colombia    2020-05-01        6507.0  4775.0   293.0     1439.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csse_global_daily_reports_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csse_global_timeseries_df, csse_us_timeseries_df = csse_timeseries_reformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csse_global_timeseries</th>\n",
       "      <th>global_confirmed</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Haiti</th>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taiwan*</th>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kosovo</th>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>856</td>\n",
       "      <td>26</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uruguay</th>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Africa</th>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "csse_global_timeseries   global_confirmed  global_deaths  global_recovered\n",
       "location     date                                                         \n",
       "Haiti        2020-04-01                16              0                 1\n",
       "Taiwan*      2020-02-20                24              1                 2\n",
       "Kosovo       2020-05-06               856             26               490\n",
       "Uruguay      2020-02-26                 0              0                 0\n",
       "South Africa 2020-03-03                 0              0                 0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csse_global_timeseries_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df = column_or_index_string_reformat(pd.read_csv('./OWID_git_and_manual_case_and_test_data/owid-covid-data.csv'))\n",
    "owid_df.loc[:, 'date'] = pd.to_datetime(owid_df.loc[:, 'date']).dt.normalize()\n",
    "owid_df = owid_df.set_index(['location','date']).sort_index()\n",
    "owid_df = regularize_country_names(owid_df)\n",
    "owid_df.columns.names = ['owid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owid</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>tests_units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Belarus</th>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>BLR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Africa</th>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>ZAF</td>\n",
       "      <td>709</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.954</td>\n",
       "      <td>2.563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20471.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>units unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>AGO</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aruba</th>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>ABW</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177.959</td>\n",
       "      <td>18.733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bangladesh</th>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>BGD</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>samples tested</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "owid                    iso_code  total_cases  new_cases  total_deaths  \\\n",
       "location     date                                                        \n",
       "Belarus      2020-02-19      BLR            0          0             0   \n",
       "South Africa 2020-03-26      ZAF          709        152             0   \n",
       "Angola       2020-03-25      AGO            2          0             0   \n",
       "Aruba        2020-03-26      ABW           19          2             0   \n",
       "Bangladesh   2020-03-09      BGD            3          3             0   \n",
       "\n",
       "owid                     new_deaths  total_cases_per_million  \\\n",
       "location     date                                              \n",
       "Belarus      2020-02-19           0                    0.000   \n",
       "South Africa 2020-03-26           0                   11.954   \n",
       "Angola       2020-03-25           0                    0.061   \n",
       "Aruba        2020-03-26           0                  177.959   \n",
       "Bangladesh   2020-03-09           0                    0.018   \n",
       "\n",
       "owid                     new_cases_per_million  total_deaths_per_million  \\\n",
       "location     date                                                          \n",
       "Belarus      2020-02-19                  0.000                       0.0   \n",
       "South Africa 2020-03-26                  2.563                       0.0   \n",
       "Angola       2020-03-25                  0.000                       0.0   \n",
       "Aruba        2020-03-26                 18.733                       0.0   \n",
       "Bangladesh   2020-03-09                  0.018                       0.0   \n",
       "\n",
       "owid                     new_deaths_per_million  total_tests  new_tests  \\\n",
       "location     date                                                         \n",
       "Belarus      2020-02-19                     0.0          NaN        NaN   \n",
       "South Africa 2020-03-26                     0.0      20471.0        NaN   \n",
       "Angola       2020-03-25                     0.0          NaN        NaN   \n",
       "Aruba        2020-03-26                     0.0          NaN        NaN   \n",
       "Bangladesh   2020-03-09                     0.0        137.0       10.0   \n",
       "\n",
       "owid                     total_tests_per_thousand  new_tests_per_thousand  \\\n",
       "location     date                                                           \n",
       "Belarus      2020-02-19                       NaN                     NaN   \n",
       "South Africa 2020-03-26                     0.349                     NaN   \n",
       "Angola       2020-03-25                       NaN                     NaN   \n",
       "Aruba        2020-03-26                       NaN                     NaN   \n",
       "Bangladesh   2020-03-09                     0.001                     0.0   \n",
       "\n",
       "owid                        tests_units  \n",
       "location     date                        \n",
       "Belarus      2020-02-19             NaN  \n",
       "South Africa 2020-03-26   units unclear  \n",
       "Angola       2020-03-25             NaN  \n",
       "Aruba        2020-03-26             NaN  \n",
       "Bangladesh   2020-03-09  samples tested  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = column_or_index_string_reformat(pd.read_csv('./OxCGRT_response_data/OxCGRT_latest.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df.loc[:,'date'] = pd.to_datetime(oxcgrt_df.date,format='%Y%m%d').dt.normalize()\n",
    "oxcgrt_df = oxcgrt_df.set_index(['country_name', 'date']).sort_index()\n",
    "oxcgrt_df.index.names = ['location','date']\n",
    "oxcgrt_df.columns.names = ['oxcgrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oxcgrt</th>\n",
       "      <th>country_code</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>c1_flag</th>\n",
       "      <th>c2_workplace_closing</th>\n",
       "      <th>c2_flag</th>\n",
       "      <th>c3_cancel_public_events</th>\n",
       "      <th>c3_flag</th>\n",
       "      <th>c4_restrictions_on_gatherings</th>\n",
       "      <th>c4_flag</th>\n",
       "      <th>c5_close_public_transport</th>\n",
       "      <th>...</th>\n",
       "      <th>h3_contact_tracing</th>\n",
       "      <th>h4_emergency_investment_in_healthcare</th>\n",
       "      <th>h5_investment_in_vaccines</th>\n",
       "      <th>m1_wildcard</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>confirmed_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>stringency_index_for_display</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trinidad and Tobago</th>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>TTO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>93.38</td>\n",
       "      <td>93.38</td>\n",
       "      <td>92.38</td>\n",
       "      <td>92.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>GBR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.11</td>\n",
       "      <td>11.11</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>IRQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>458.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>91.40</td>\n",
       "      <td>91.40</td>\n",
       "      <td>89.52</td>\n",
       "      <td>89.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peru</th>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>PER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65015.0</td>\n",
       "      <td>1814.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luxembourg</th>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>LUX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3695.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>75.66</td>\n",
       "      <td>75.66</td>\n",
       "      <td>76.19</td>\n",
       "      <td>76.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "oxcgrt                         country_code  c1_school_closing  c1_flag  \\\n",
       "location            date                                                  \n",
       "Trinidad and Tobago 2020-04-30          TTO                3.0      1.0   \n",
       "United Kingdom      2020-02-10          GBR                0.0      NaN   \n",
       "Iraq                2020-03-28          IRQ                3.0      1.0   \n",
       "Peru                2020-05-10          PER                NaN      NaN   \n",
       "Luxembourg          2020-04-25          LUX                3.0      1.0   \n",
       "\n",
       "oxcgrt                          c2_workplace_closing  c2_flag  \\\n",
       "location            date                                        \n",
       "Trinidad and Tobago 2020-04-30                   3.0      1.0   \n",
       "United Kingdom      2020-02-10                   0.0      NaN   \n",
       "Iraq                2020-03-28                   3.0      1.0   \n",
       "Peru                2020-05-10                   NaN      NaN   \n",
       "Luxembourg          2020-04-25                   3.0      1.0   \n",
       "\n",
       "oxcgrt                          c3_cancel_public_events  c3_flag  \\\n",
       "location            date                                           \n",
       "Trinidad and Tobago 2020-04-30                      2.0      1.0   \n",
       "United Kingdom      2020-02-10                      0.0      NaN   \n",
       "Iraq                2020-03-28                      2.0      1.0   \n",
       "Peru                2020-05-10                      NaN      NaN   \n",
       "Luxembourg          2020-04-25                      2.0      1.0   \n",
       "\n",
       "oxcgrt                          c4_restrictions_on_gatherings  c4_flag  \\\n",
       "location            date                                                 \n",
       "Trinidad and Tobago 2020-04-30                            4.0      1.0   \n",
       "United Kingdom      2020-02-10                            0.0      NaN   \n",
       "Iraq                2020-03-28                            4.0      1.0   \n",
       "Peru                2020-05-10                            NaN      NaN   \n",
       "Luxembourg          2020-04-25                            4.0      1.0   \n",
       "\n",
       "oxcgrt                          c5_close_public_transport  ...  \\\n",
       "location            date                                   ...   \n",
       "Trinidad and Tobago 2020-04-30                        1.0  ...   \n",
       "United Kingdom      2020-02-10                        0.0  ...   \n",
       "Iraq                2020-03-28                        2.0  ...   \n",
       "Peru                2020-05-10                        NaN  ...   \n",
       "Luxembourg          2020-04-25                        1.0  ...   \n",
       "\n",
       "oxcgrt                          h3_contact_tracing  \\\n",
       "location            date                             \n",
       "Trinidad and Tobago 2020-04-30                 2.0   \n",
       "United Kingdom      2020-02-10                 2.0   \n",
       "Iraq                2020-03-28                 0.0   \n",
       "Peru                2020-05-10                 NaN   \n",
       "Luxembourg          2020-04-25                 2.0   \n",
       "\n",
       "oxcgrt                          h4_emergency_investment_in_healthcare  \\\n",
       "location            date                                                \n",
       "Trinidad and Tobago 2020-04-30                                    0.0   \n",
       "United Kingdom      2020-02-10                                    0.0   \n",
       "Iraq                2020-03-28                                    0.0   \n",
       "Peru                2020-05-10                                    NaN   \n",
       "Luxembourg          2020-04-25                                    0.0   \n",
       "\n",
       "oxcgrt                          h5_investment_in_vaccines  m1_wildcard  \\\n",
       "location            date                                                 \n",
       "Trinidad and Tobago 2020-04-30                        0.0          NaN   \n",
       "United Kingdom      2020-02-10                        0.0          NaN   \n",
       "Iraq                2020-03-28                        0.0          NaN   \n",
       "Peru                2020-05-10                        NaN          NaN   \n",
       "Luxembourg          2020-04-25                        0.0          NaN   \n",
       "\n",
       "oxcgrt                          confirmed_cases  confirmed_deaths  \\\n",
       "location            date                                            \n",
       "Trinidad and Tobago 2020-04-30            116.0               8.0   \n",
       "United Kingdom      2020-02-10              4.0               0.0   \n",
       "Iraq                2020-03-28            458.0              40.0   \n",
       "Peru                2020-05-10          65015.0            1814.0   \n",
       "Luxembourg          2020-04-25           3695.0              85.0   \n",
       "\n",
       "oxcgrt                          stringency_index  \\\n",
       "location            date                           \n",
       "Trinidad and Tobago 2020-04-30             93.38   \n",
       "United Kingdom      2020-02-10             11.11   \n",
       "Iraq                2020-03-28             91.40   \n",
       "Peru                2020-05-10               NaN   \n",
       "Luxembourg          2020-04-25             75.66   \n",
       "\n",
       "oxcgrt                          stringency_index_for_display  \\\n",
       "location            date                                       \n",
       "Trinidad and Tobago 2020-04-30                         93.38   \n",
       "United Kingdom      2020-02-10                         11.11   \n",
       "Iraq                2020-03-28                         91.40   \n",
       "Peru                2020-05-10                         96.03   \n",
       "Luxembourg          2020-04-25                         75.66   \n",
       "\n",
       "oxcgrt                          legacy_stringency_index  \\\n",
       "location            date                                  \n",
       "Trinidad and Tobago 2020-04-30                    92.38   \n",
       "United Kingdom      2020-02-10                    14.29   \n",
       "Iraq                2020-03-28                    89.52   \n",
       "Peru                2020-05-10                      NaN   \n",
       "Luxembourg          2020-04-25                    76.19   \n",
       "\n",
       "oxcgrt                          legacy_stringency_index_for_display  \n",
       "location            date                                             \n",
       "Trinidad and Tobago 2020-04-30                                92.38  \n",
       "United Kingdom      2020-02-10                                14.29  \n",
       "Iraq                2020-03-28                                89.52  \n",
       "Peru                2020-05-10                                92.38  \n",
       "Luxembourg          2020-04-25                                76.19  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "#Pull the data using their API (for whatever reason this data set is different from the manual download).\n",
    "# url_to_present_date = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-01-02/' \\\n",
    "#                         + str(datetime.now().date())\n",
    "# response = requests.get(url_to_present_date)\n",
    "# response_json = response.json()\n",
    "# response_json_nested_dict = response_json['data']\n",
    "\n",
    "# response_api_df = pd.DataFrame.from_dict({(i,j): response_json_nested_dict[i][j] \n",
    "#                            for i in response_json_nested_dict.keys() \n",
    "#                            for j in response_json_nested_dict[i].keys()},\n",
    "#                        orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_tracker</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>penalty</th>\n",
       "      <th>population</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brunei</th>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>229</td>\n",
       "      <td>10579</td>\n",
       "      <td>1.3</td>\n",
       "      <td>437000</td>\n",
       "      <td>2420.8</td>\n",
       "      <td>2420.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nigeria</th>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>206140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Botswana</th>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>0</td>\n",
       "      <td>3115</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2352000</td>\n",
       "      <td>132.4</td>\n",
       "      <td>132.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slovakia</th>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>95</td>\n",
       "      <td>306</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5460000</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>0</td>\n",
       "      <td>4058</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1380004000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_tracker         new_tests  tests_cumulative  penalty  population  \\\n",
       "location date                                                           \n",
       "Brunei   2020-04-15        229             10579      1.3      437000   \n",
       "Nigeria  2020-02-29          1                 1      1.3   206140000   \n",
       "Botswana 2020-04-16          0              3115      1.3     2352000   \n",
       "Slovakia 2020-03-05         95               306      0.9     5460000   \n",
       "India    2020-03-07          0              4058      1.3  1380004000   \n",
       "\n",
       "test_tracker         per100k  testsPer100k  \n",
       "location date                               \n",
       "Brunei   2020-04-15   2420.8        2420.8  \n",
       "Nigeria  2020-02-29      0.0           0.0  \n",
       "Botswana 2020-04-16    132.4         132.4  \n",
       "Slovakia 2020-03-05      5.6           5.6  \n",
       "India    2020-03-07      0.3           0.3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtrack_df = pd.read_csv('./TestTracker_data/Tests_20200504.csv')\n",
    "testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date']).dt.normalize()\n",
    "# testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date'], format='%Y-%m-%d', errors='coerce')\n",
    "testtrack_df = testtrack_df.set_index(['country','date']).sort_index()\n",
    "testtrack_df.index.names = ['location','date']\n",
    "testtrack_df.columns.names = ['test_tracker']\n",
    "unused_columns = ['ind', 'jhu_ID.x', 'source', 'X.x', 'X.y', 'alpha2', 'alpha3',\n",
    "                  'numeric', 'latitude', 'longitude', 'jhu_ID.y', 'notes']\n",
    "\n",
    "testtrack_df = testtrack_df.drop(columns=unused_columns)\n",
    "testtrack_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used to model country-wide case numbers exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "#     csse_us_timeseries_df, ihme_df, owid_df, oxcgrt_df, testtrack_df]\n",
    "global_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "                owid_df, oxcgrt_df, testtrack_df]\n",
    "\n",
    "# for i, df in enumerate(all_data):\n",
    "#     all_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))\n",
    "\n",
    "for i, df in enumerate(global_data):\n",
    "    global_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_intersection = global_data[0].index.levels[0]\n",
    "dates_union =  global_data[0].index.levels[1].unique()\n",
    "for i in range(len(global_data)-1):\n",
    "    country_intersection = country_intersection.intersection(global_data[i+1].index.levels[0])\n",
    "    dates_union = dates_union.union(global_data[i+1].index.levels[1].unique())\n",
    "\n",
    "global_data_intersected = [intersect_country_index(df, country_intersection) for df in global_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2019-12-31 00:00:00 to 2020-05-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('The range of all dates is from {} to {}'.format(dates_union.min(), dates_union.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of countries included is 112\n"
     ]
    }
   ],
   "source": [
    "print('The final number of countries included is {}'.format(len(country_intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense, because of the intersections between data; to us the u.s. time series and ihme data together but not with\n",
    "the global data. The hospital data is very useful and so it may be important to look specifically at the small number of countries it contains. Regardless; by using only the global data we can keep 110 countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Want to have all time dependent data defined on the same time ranges for convenience;\n",
    "this involves two steps. 1. Initialize the new dates, 2. deal with the missing values. \n",
    "These missing values references the ones introduced by resampling or redefining the range of \n",
    "each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "normalized_global_data = [resample_dates(df, dates_union) for df in global_data_intersected]\n",
    "# To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "data = pd.concat([make_multilevel_columns(df) for df in normalized_global_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = multiindex_to_table(data)\n",
    "case_features = data_table.columns[data_table.columns.str.contains('confirmed') | data_table.columns.str.contains('cases')].tolist()\n",
    "case_features_to_drop = case_features[4:6]\n",
    "case_features_to_avg = case_features[:3] + [case_features[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These features are dropped because of how similar they are to the target ['total_cases_per_million', 'new_cases_per_million']\n",
      "These features are being averaged and constitute the target variable ['confirmed', 'global_confirmed', 'total_cases', 'confirmed_cases']\n"
     ]
    }
   ],
   "source": [
    "print('These features are dropped because of how similar they are to the target', case_features_to_drop)\n",
    "print('These features are being averaged and constitute the target variable', case_features_to_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groupby_indices = [data_table[data_table.location==country].index for country in data_table.location.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_averages = data_table.loc[country_groupby_indices[0], case_features_to_avg].mean(axis=1)\n",
    "for indices in country_groupby_indices[1:]:\n",
    "    case_averages = pd.concat((case_averages, data_table.loc[indices, case_features_to_avg].mean(axis=1)),axis=0)\n",
    "    \n",
    "data_table.loc[:, 'n_cases'] = case_averages\n",
    "data_table = data_table.drop(columns=case_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was planning on using a \"days since first case\" variable, which would equal zero until the date of the\n",
    "first case, but I believe this would correlate too strongly with the target variable. To test this assumption I'll compute it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_number_of_cases = data_table.n_cases.replace(to_replace=0., value=np.nan).dropna().index\n",
    "no_cases_dropped = data_table.loc[positive_number_of_cases,:]\n",
    "country_groupby_indices_dropped_nan = [no_cases_dropped[no_cases_dropped.location==country].index for country in no_cases_dropped.location.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I actually need this so that I can make predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days_since = []\n",
    "# for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "#     nonzero_list = list(range(len(c)))\n",
    "#     zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "#     days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "# data_table.loc[:, 'days_since'] = days_since\n",
    "# data_table.loc[:, 'time_index'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I misinterpreted the fact that days since first case is linear growth by defininition (really has the shape of a ReLU) and number of cases is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another peculiarity is the existence of two different features both called 'new_tests'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = data_table.drop(columns='population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.loc[:, 'new_tests_average'] = data_table.loc[:, 'new_tests'].mean(1)\n",
    "data_table = data_table.drop(columns=['new_tests', 'country_code','iso_code','m1_wildcard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_columns = ['deaths', 'global_deaths','total_deaths']\n",
    "data_table.loc[:, 'n_deaths'] = data_table.loc[:,death_columns].mean(axis=1)\n",
    "data_table = data_table.drop(columns=death_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_columns = ['recovered', 'global_recovered']\n",
    "data_table.loc[:, 'n_recovered'] = data_table.loc[:,recovered_columns].mean(axis=1)\n",
    "data_table = data_table.drop(columns=recovered_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_columns = ['total_tests', 'tests_cumulative']\n",
    "data_table.loc[:, 'n_tests'] = data_table.loc[:,tests_columns].mean(axis=1)\n",
    "data_table = data_table.drop(columns=tests_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have aggregated and dropped the respective features, the missing values of the remaining data can be flagged and\n",
    "created into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_since = []\n",
    "for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "    nonzero_list = list(range(len(c)))\n",
    "    zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "    days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "data_table.loc[:, 'days_since'] = days_since\n",
    "data_table.loc[:, 'time_index'] = len(data_table.location.unique())*list(range(len(data_table.date.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = data_table.columns.str.contains('flag')\n",
    "\n",
    "# 'tests_units' have string like values\n",
    "data_table.loc[:, 'tests_units'] = data_table.loc[:, 'tests_units'].fillna('Missing')\n",
    "# the 'flag' columns from oxcgrt data set already use 0 as a value, so fill them separately with -1\n",
    "data_table.loc[:, data_table.columns[flag_columns]] = data_table.loc[:, data_table.columns[flag_columns]].fillna(value=-1)\n",
    "# Population is a static number but some entries are missing; its ok to backfill this.\n",
    "# data_table.loc[:, 'population'] = data_table.loc[:, ['location', 'population']].replace(\n",
    "#                                     to_replace=0., value=np.nan).groupby('location').fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nancount = data_table.isna().sum()\n",
    "columns_to_flag_for_missing_values = nancount[nancount>0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which values are missing\n",
    "missing_flags = data_table.isna()\n",
    "# Add a suffix to label these flag variables\n",
    "missing_flags.columns = missing_flags.columns + '_missing_flag'\n",
    "# The first two features consist of location and date; they do not miss any values and so the flag columns would be all 0's. \n",
    "# therefore they are sliced out. \n",
    "missing_flags = missing_flags.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_interp = data_table.loc[:, data_table.columns[~flag_columns]]\n",
    "\n",
    "# Cannot fill with interpolation, because it will \"look\" into the future. \n",
    "# interpolated = data_interp.groupby(by='location', as_index=False).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "forwardfill = data_interp.groupby(by='location', as_index=False).fillna(method='ffill')\n",
    "# forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "remaining_nan = forwardfill.fillna(value=0)\n",
    "# remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "data_table.loc[remaining_nan.index, remaining_nan.columns] = remaining_nan.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "date\n",
      "active\n",
      "new_deaths\n",
      "total_deaths_per_million\n",
      "new_deaths_per_million\n",
      "total_tests_per_thousand\n",
      "new_tests_per_thousand\n",
      "tests_units\n",
      "c1_school_closing\n",
      "c1_flag\n",
      "c2_workplace_closing\n",
      "c2_flag\n",
      "c3_cancel_public_events\n",
      "c3_flag\n",
      "c4_restrictions_on_gatherings\n",
      "c4_flag\n",
      "c5_close_public_transport\n",
      "c5_flag\n",
      "c6_stay_at_home_requirements\n",
      "c6_flag\n",
      "c7_restrictions_on_internal_movement\n",
      "c7_flag\n",
      "c8_international_travel_controls\n",
      "e1_income_support\n",
      "e1_flag\n",
      "e2_debt_contract_relief\n",
      "e3_fiscal_measures\n",
      "e4_international_support\n",
      "h1_public_information_campaigns\n",
      "h1_flag\n",
      "h2_testing_policy\n",
      "h3_contact_tracing\n",
      "h4_emergency_investment_in_healthcare\n",
      "h5_investment_in_vaccines\n",
      "stringency_index\n",
      "stringency_index_for_display\n",
      "legacy_stringency_index\n",
      "legacy_stringency_index_for_display\n",
      "penalty\n",
      "per100k\n",
      "testsPer100k\n",
      "n_cases\n",
      "new_tests_average\n",
      "n_deaths\n",
      "n_recovered\n",
      "n_tests\n",
      "days_since\n",
      "time_index\n"
     ]
    }
   ],
   "source": [
    "for c in data_table.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OxCGRT's \"flag\" columns (which indicate a target or general response) are numerical but I will cast them as categorical\n",
    "#so that they are not affected by the upcoming numerical feature manipulations. \n",
    "# flag_columns =  data.columns.levels[1][data.columns.levels[1].str.contains('flag')]\n",
    "# multiindex_for_flag_columns = pd.MultiIndex.from_product([['oxcgrt'], flag_columns], names=['dataset', 'features'])\n",
    "# data.loc[:, multiindex_for_flag_columns] = data.loc[:, multiindex_for_flag_columns].fillna(value=-1.).astype('category')\n",
    "# data_numerical = data.copy().select_dtypes(include='number')\n",
    "\n",
    "# # Flagging every step is probably overkill\n",
    "# interpolated = data_numerical.groupby(level=0).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# # interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "# forwardfill = interpolated.groupby(level=0).fillna(method='ffill')\n",
    "# # forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "# remaining_nan = forwardfill.fillna(value=0)\n",
    "# # remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "# backfill with interpolation, forward fill the remainder; NaNs may remain if there are only missing values\n",
    "# in their group. Therefore, still need to replace the remainder with something. Because so many of the features\n",
    "# utilize 0, I'm going to fill the remainder of missing values with -1 because nowhere do negative values appear. \n",
    "# data.loc[data_numerical.index, data_numerical.columns] = remaining_nan\n",
    "# data.loc[:, ('owid', 'tests_units')] = data.loc[:, ('owid', 'tests_units')].fillna('Missing')\n",
    "\n",
    "# still_missing_values = data.loc[:, pd.IndexSlice['test_tracker',:]].isna().sum()#.loc[pd.IndexSlice[:, #.index.levels[1]\n",
    "# throw_out_these = still_missing_values.index[still_missing_values > 0]\n",
    "\n",
    "# data = data.drop(columns=[('owid','iso_code'),\n",
    "#                          ('oxcgrt','m1_wildcard'), ('oxcgrt','country_code')]\n",
    "#                           + throw_out_these.tolist())\n",
    "# only remaining missing values are not numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>active</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>tests_units</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>...</th>\n",
       "      <th>penalty</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "      <th>n_cases</th>\n",
       "      <th>new_tests_average</th>\n",
       "      <th>n_deaths</th>\n",
       "      <th>n_recovered</th>\n",
       "      <th>n_tests</th>\n",
       "      <th>days_since</th>\n",
       "      <th>time_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>2020-03-29 00:00:00</td>\n",
       "      <td>187.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>201.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11444</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>2020-02-23 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6540</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>2020-04-17 00:00:00</td>\n",
       "      <td>13373.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>98.424</td>\n",
       "      <td>8.506</td>\n",
       "      <td>18.545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>860.3</td>\n",
       "      <td>860.3</td>\n",
       "      <td>13625.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>515.333333</td>\n",
       "      <td>77.0</td>\n",
       "      <td>42484.0</td>\n",
       "      <td>48</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>2020-02-13 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>2020-04-10 00:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           location                 date   active  new_deaths  \\\n",
       "2501   Burkina Faso  2020-03-29 00:00:00    187.0         6.0   \n",
       "11444  Saudi Arabia  2020-02-23 00:00:00      0.0         0.0   \n",
       "6540        Ireland  2020-04-17 00:00:00  13373.0        42.0   \n",
       "7012        Jamaica  2020-02-13 00:00:00      0.0         0.0   \n",
       "2647        Burundi  2020-04-10 00:00:00      3.0         0.0   \n",
       "\n",
       "       total_deaths_per_million  new_deaths_per_million  \\\n",
       "2501                      0.431                   0.287   \n",
       "11444                     0.000                   0.000   \n",
       "6540                     98.424                   8.506   \n",
       "7012                      0.000                   0.000   \n",
       "2647                      0.000                   0.000   \n",
       "\n",
       "       total_tests_per_thousand  new_tests_per_thousand tests_units  \\\n",
       "2501                      0.000                     0.0     Missing   \n",
       "11444                     0.000                     0.0     Missing   \n",
       "6540                     18.545                     0.0     Missing   \n",
       "7012                      0.000                     0.0     Missing   \n",
       "2647                      0.000                     0.0     Missing   \n",
       "\n",
       "       c1_school_closing  ...  penalty  per100k  testsPer100k  n_cases  \\\n",
       "2501                 3.0  ...      1.3      0.5           0.5    201.0   \n",
       "11444                0.0  ...      0.0      0.0           0.0      0.0   \n",
       "6540                 3.0  ...      0.8    860.3         860.3  13625.5   \n",
       "7012                 0.0  ...      0.0      0.0           0.0      0.0   \n",
       "2647                 0.0  ...      0.0      0.0           0.0      3.0   \n",
       "\n",
       "       new_tests_average    n_deaths  n_recovered  n_tests  days_since  \\\n",
       "2501                19.0   11.000000         23.0     99.0          19   \n",
       "11444                0.0    0.000000          0.0      0.0           0   \n",
       "6540                 0.0  515.333333         77.0  42484.0          48   \n",
       "7012                 0.0    0.000000          0.0      0.0           0   \n",
       "2647                 0.0    0.000000          0.0      0.0          10   \n",
       "\n",
       "       time_index  \n",
       "2501           89  \n",
       "11444          54  \n",
       "6540          108  \n",
       "7012           44  \n",
       "2647          101  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_names in data.columns.levels[0]:\n",
    "#     dataset_datatable = multiindex_to_table(data.loc[:, pd.IndexSlice[dataset_names, :]])\n",
    "#     dataset_datatable.to_csv(dataset_names+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_table = pd.concat((data_table, missing_flags), axis=1)\n",
    "data_table.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Unused as of now</font>\n",
    "\n",
    "\n",
    "## Repeat of the above calculations for United States only data.\n",
    "\n",
    "\n",
    "\n",
    "The United States' data merits separate investigation 1. because of the case number 2. because the IHME dataset is only really\n",
    "properly defined for the statewide description of the U.S. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME hospital data\n",
    "<a id='ihme'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "[JHU CSSE](#csse) \n",
    "<font color='red'>\n",
    "### Has all USA states but only 32 countries which overlap with other data; stash this dataset for now. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_df = column_or_index_string_reformat(pd.read_csv(\n",
    "    './IHME_hospital_data/2020_04_12.02/Hospitalization_all_locs.csv').rename(columns={'location_name':'location'}))\n",
    "ihme_df.loc[:, 'date'] = pd.to_datetime(ihme_df.loc[:,'date']).dt.normalize()\n",
    "ihme_df = ihme_df.set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>allbed_mean</th>\n",
       "      <th>allbed_lower</th>\n",
       "      <th>allbed_upper</th>\n",
       "      <th>icubed_mean</th>\n",
       "      <th>icubed_lower</th>\n",
       "      <th>icubed_upper</th>\n",
       "      <th>inv_ven_mean</th>\n",
       "      <th>inv_ven_lower</th>\n",
       "      <th>inv_ven_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>new_icu_upper</th>\n",
       "      <th>totdea_mean</th>\n",
       "      <th>totdea_lower</th>\n",
       "      <th>totdea_upper</th>\n",
       "      <th>bedover_mean</th>\n",
       "      <th>bedover_lower</th>\n",
       "      <th>bedover_upper</th>\n",
       "      <th>icuover_mean</th>\n",
       "      <th>icuover_lower</th>\n",
       "      <th>icuover_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nebraska</th>\n",
       "      <th>2020-06-28</th>\n",
       "      <td>178</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.714484</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.989</td>\n",
       "      <td>57.950</td>\n",
       "      <td>1049.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <th>2020-07-03</th>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21129.580</td>\n",
       "      <td>20487.975</td>\n",
       "      <td>22310.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slovakia</th>\n",
       "      <th>2020-02-17</th>\n",
       "      <td>46</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indiana</th>\n",
       "      <th>2020-05-16</th>\n",
       "      <td>135</td>\n",
       "      <td>3.545224</td>\n",
       "      <td>1.05</td>\n",
       "      <td>8.091761</td>\n",
       "      <td>0.597010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.684962</td>\n",
       "      <td>0.385795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.117831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>859.724</td>\n",
       "      <td>418.000</td>\n",
       "      <td>2220.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      v1  allbed_mean  allbed_lower  allbed_upper  \\\n",
       "location date                                                       \n",
       "Nebraska 2020-06-28  178     0.048338          0.00      0.714484   \n",
       "Italy    2020-07-03  183     0.000000          0.00      0.000000   \n",
       "Slovakia 2020-02-17   46     0.000000          0.00      0.000000   \n",
       "Indiana  2020-05-16  135     3.545224          1.05      8.091761   \n",
       "Alaska   2020-03-06   64     0.000000          0.00      0.000000   \n",
       "\n",
       "                     icubed_mean  icubed_lower  icubed_upper  inv_ven_mean  \\\n",
       "location date                                                                \n",
       "Nebraska 2020-06-28     0.005907           0.0      0.000000      0.003585   \n",
       "Italy    2020-07-03     0.000000           0.0      0.000000      0.000000   \n",
       "Slovakia 2020-02-17     0.000000           0.0      0.000000      0.000000   \n",
       "Indiana  2020-05-16     0.597010           0.0      1.684962      0.385795   \n",
       "Alaska   2020-03-06     0.000000           0.0      0.000000      0.000000   \n",
       "\n",
       "                     inv_ven_lower  inv_ven_upper  ...  new_icu_upper  \\\n",
       "location date                                      ...                  \n",
       "Nebraska 2020-06-28            0.0       0.000000  ...            0.0   \n",
       "Italy    2020-07-03            0.0       0.000000  ...            0.0   \n",
       "Slovakia 2020-02-17            0.0       0.000000  ...            0.0   \n",
       "Indiana  2020-05-16            0.0       1.117831  ...            0.0   \n",
       "Alaska   2020-03-06            0.0       0.000000  ...            0.0   \n",
       "\n",
       "                     totdea_mean  totdea_lower  totdea_upper  bedover_mean  \\\n",
       "location date                                                                \n",
       "Nebraska 2020-06-28      280.989        57.950       1049.20           0.0   \n",
       "Italy    2020-07-03    21129.580     20487.975      22310.85           0.0   \n",
       "Slovakia 2020-02-17        0.000         0.000          0.00           0.0   \n",
       "Indiana  2020-05-16      859.724       418.000       2220.60           0.0   \n",
       "Alaska   2020-03-06        0.000         0.000          0.00           0.0   \n",
       "\n",
       "                     bedover_lower  bedover_upper  icuover_mean  \\\n",
       "location date                                                     \n",
       "Nebraska 2020-06-28            0.0            0.0           0.0   \n",
       "Italy    2020-07-03            0.0            0.0           0.0   \n",
       "Slovakia 2020-02-17            0.0            0.0           0.0   \n",
       "Indiana  2020-05-16            0.0            0.0           0.0   \n",
       "Alaska   2020-03-06            0.0            0.0           0.0   \n",
       "\n",
       "                     icuover_lower  icuover_upper  \n",
       "location date                                      \n",
       "Nebraska 2020-06-28            0.0            0.0  \n",
       "Italy    2020-07-03            0.0            0.0  \n",
       "Slovakia 2020-02-17            0.0            0.0  \n",
       "Indiana  2020-05-16            0.0            0.0  \n",
       "Alaska   2020-03-06            0.0            0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihme_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delphi-epidata\n",
    "<a id='delphi'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_source\tname of upstream data source \n",
    "(e.g., fb-survey, google-survey, ght, quidel, doctor-visits)\tstring\n",
    "\n",
    "signal\tname of signal derived from upstream data (see notes below)\tstring\n",
    "\n",
    "time_type\ttemporal resolution of the signal (e.g., day, week)\tstring\n",
    "\n",
    "geo_type\tspatial resolution of the signal (e.g., county, hrr, msa, dma, state)\tstring\n",
    "\n",
    "time_values\ttime unit (e.g., date) over which underlying events happened\tlist of time values (e.g., 20200401)\n",
    "\n",
    "geo_value\tunique code for each location, depending on geo_type (county -> FIPS 6-4 code, HRR -> HRR number, MSA -> CBSA code,\n",
    "DMA -> DMA code, state -> two-letter state code), or * for all\tstring\n",
    "\n",
    "As of this writing, data sources have the following signals:\n",
    "\n",
    "fb-survey signal values include raw_cli, raw_ili, raw_wcli, raw_wili, and also four additional named with raw_* replaced by smoothed_* (e.g. smoothed_cli, etc).\n",
    "google-survey signal values include raw_cli and smoothed_cli.\n",
    "ght signal values include raw_search and smoothed_search.\n",
    "quidel signal values include smoothed_pct_negative and smoothed_tests_per_device.\n",
    "doctor-visits signal values include smoothed_cli.\n",
    "\n",
    "Delphi API data :\n",
    "doctor visits : 20200201-20200429 (as of 20200503)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Currently Unused ----------------------#\n",
    "\n",
    "def pull_delphi_data(data_source=['fb-survey', 'google-survey', 'ght', 'quidel', 'quidelneg', 'doctor-visits'], \n",
    "                     daterange=pd.date_range(start=\"20200101\",\n",
    "                                             end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d'),\n",
    "                     **kwargs):\n",
    "    \"\"\" Pull data from https://cmu-delphi.github.io/delphi-epidata/api/\n",
    "        https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for data in data_source:\n",
    "        signal_dict = {'fb-survey':'smoothed_cli',\n",
    "                       'google-survey':'smoothed_cli',\n",
    "                       'ght':'smoothed_search',\n",
    "                       'quidel':'smoothed_tests_per_device',\n",
    "                       'quidelneg':'smoothed_pct_negative',\n",
    "                       'doctor-visits':'smoothed_cli'}\n",
    "        \n",
    "        signal = signal_dict[data]\n",
    "        if data=='quidelneg':\n",
    "            #change the proxy for the quidel signal\n",
    "            data = 'quidel'\n",
    "        for days in daterange:\n",
    "            resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "            day_data = resp.json().get('epidata', None)\n",
    "            if day_data is None:\n",
    "                pass\n",
    "            else:\n",
    "                var_number += pd.json_normalize(day_data).size\n",
    "                print(pd.json_normalize(day_data).shape)    \n",
    "                \n",
    "                \n",
    "# date_range_2020 = pd.date_range(start=\"20200101\",end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d')\n",
    "# var_number = 0 \n",
    "# for days in date_range_2020:\n",
    "# #     days='20200302'\n",
    "#     resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "#     day_data = resp.json().get('epidata', None)\n",
    "#     if day_data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         var_number += pd.json_normalize(day_data).size\n",
    "#         print(pd.json_normalize(day_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_us_timeseries_df.groupby(level=1).sum().to_csv('csse_united_states.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = [\n",
    "    csse_us_timeseries_df,\n",
    "    ihme_df,\n",
    "    owid_df,\n",
    "    oxcgrt_df,\n",
    "    testtrack_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_table = multiindex_to_table(csse_us_timeseries_df)\n",
    "today = us_table[us_table.date==us_table.date.max()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
