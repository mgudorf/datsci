{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, model_data.time_index.max() - n_days_into_future + 1):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(model_data.time_index <= max_date_in_window) & (model_data.time_index > max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_and_normalize_Xy(X, y, n_time_steps, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "\n",
    "    X_means = X_train.mean(axis=(1,2))\n",
    "    X_stds = X_train.std(axis=(1,2))\n",
    "\n",
    "    # To avoid division by zero. This is a big assumption but this typically occurs when the frame's feature\n",
    "    # value is identically zero, which would result in x-x_mean / x_std = 0 / 1 = 0. So it doesn't matter what \n",
    "    # the x_std value is changed to as they are always divided into 0.\n",
    "    X_stds[np.where(X_stds==0.)] = 1\n",
    "\n",
    "#     # First two features are time_index and time_index (days_since_first_case)\n",
    "    if date_normalization==False:\n",
    "        X_means[:,:2] = 0\n",
    "        X_stds[:, :2] = 1\n",
    "\n",
    "    # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "    # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "    # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "    # be repeated for each validation, test frame for each country for each timestep.\n",
    "    latest_training_mean = X_means[-1,:][np.newaxis, np.newaxis, np.newaxis, :]\n",
    "    latest_training_std = X_stds[-1,:][np.newaxis, np.newaxis, np.newaxis, :]\n",
    "    latest_training_std[np.where(latest_training_std==0)] = 1\n",
    "    \n",
    "    if train_test_only:\n",
    "    # Normalize the training data by each frame's specific mean and std deviation. \n",
    "        X_train_means = np.tile(X_means[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))\n",
    "        X_train_stds =  np.tile(X_stds[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))                \n",
    "        X_test_means = np.tile(latest_training_mean, \n",
    "                               (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "        X_test_stds = np.tile(latest_training_std, \n",
    "                              (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))   \n",
    "        \n",
    "        X_train = ((X_train - X_train_means) /  X_train_stds)\n",
    "        X_test = ((X_test - X_test_means) /  X_test_stds)\n",
    "        \n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "        normalizing_values = (X_train_means, X_train_stds, X_test_means, X_test_stds)\n",
    "    else:\n",
    "        X_train_means = np.tile(X_means[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))\n",
    "        X_train_stds =  np.tile(X_stds[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))                \n",
    "        X_validate_means = np.tile(latest_training_mean, \n",
    "                                   (X_validate.shape[0],X_validate.shape[1],X_validate.shape[2],1))\n",
    "        X_validate_stds = np.tile(latest_training_std, \n",
    "                                  (X_validate.shape[0],X_validate.shape[1],X_validate.shape[2],1))\n",
    "        X_test_means = np.tile(latest_training_mean, \n",
    "                               (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "        X_test_stds = np.tile(latest_training_std, \n",
    "                              (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))    \n",
    "\n",
    "        X_train = ((X_train - X_train_means) /  X_train_stds)\n",
    "        X_validate = ((X_validate - X_validate_means) / X_validate_stds)\n",
    "        X_test = ((X_test - X_test_means) /  X_test_stds)\n",
    "\n",
    "\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "        normalizing_values = (X_train_means, X_train_stds, X_validate_means, X_validate_stds,\n",
    "                              X_test_means, X_test_stds)\n",
    "                          \n",
    "    return splits, normalizing_values\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_test, y_naive, y_predict, title=''):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20,5))\n",
    "    ymax = np.max([y_test.max(), y_predict.max()])\n",
    "    axes[0].scatter(y_test, y_naive, s=5)\n",
    "    axes[0].plot([0, ymax], [0, ymax])\n",
    "\n",
    "    axes[1].scatter(y_test, y_predict, s=5)\n",
    "    axes[1].plot([0, ymax], [0, ymax])\n",
    "\n",
    "    axes[0].set_xlabel('True value')\n",
    "    axes[0].set_ylabel('Predicted value')\n",
    "    axes[0].set_title('Naive model')\n",
    "\n",
    "    axes[1].set_xlabel('True value')\n",
    "    axes[1].set_ylabel('Predicted value')\n",
    "    axes[1].set_title('CNN model')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test,y_predict,title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "#     plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_naive, y_predict, y_true, n_test_frames,n_days_into_future, n_countries):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    for i in range(n_test_frames):\n",
    "        xrange = range(n_countries*i, n_countries*(i+1))\n",
    "        ax1.plot(xrange, y_true.reshape(-1,n_countries)[i,:]-y_naive.reshape(-1,n_countries)[i,:])\n",
    "        ax2.plot(xrange, y_true.reshape(-1,n_countries)[i,:]-y_predict.reshape(-1,n_countries)[i,:])\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('True minus Naive baseline')\n",
    "    ax2.set_title('True minus CNN')\n",
    "    residual_plot(y_true,y_naive,title='Naive residual',ax=ax3)\n",
    "    residual_plot(y_true,y_predict,title='CNN residual',ax=ax4)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def n_step_model_predictions(model_data, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, batch_size,\n",
    "                             train_test_only=False, Xy_truncation=None):\n",
    "    \n",
    "    \"\"\" wrapper for iteration loop \n",
    "    \n",
    "    data : DataFrame of very specific make\n",
    "    \n",
    "    model : one of my custom models, sequential_Conv1D_model, SeparableConv2D_model, parallel_Conv1D_model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    new_cases_weighted_index = column_search(model_data,'new_cases_weighted', return_style='iloc')[0]#-n_pruned\n",
    "    prediction = []\n",
    "    naive = []\n",
    "    test = []\n",
    "    mae_naive_list = []\n",
    "    mae_predict_list = []\n",
    "    model_list = []\n",
    "\n",
    "    for n_days_into_future in predict_steps:\n",
    "        X, y = create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries)\n",
    "        if Xy_truncation is not None:\n",
    "            X = X[:Xy_truncation,:,:,:]\n",
    "            y = y[:Xy_truncation,:]\n",
    "        splits, normalizing = split_and_normalize_Xy(X, y,frame_size, n_validation_frames,\n",
    "                                                     n_test_frames,train_test_only=train_test_only)\n",
    "        A_splits = concatenate_4d_into_3d(splits, train_test_only=train_test_only) \n",
    "        \n",
    "        if model_generator == SeparableConv2D_model:\n",
    "            B_splits = splits\n",
    "            if train_test_only:\n",
    "                X_train_A, y_train_A, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_test_B, y_test_B = B_splits\n",
    "                \n",
    "                # model building\n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A\n",
    "                X_validate = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A\n",
    "            else:\n",
    "                X_train_A, y_train_A, X_validate_A, y_validate, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_validate_B, y_validate, X_test_B, y_test_B = B_splits  \n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A.ravel()\n",
    "                X_validate = [X_validate_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A.ravel()\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A.ravel()\n",
    "        else: \n",
    "            if train_test_only:\n",
    "                X_train, y_train, X_test, y_test = A_splits\n",
    "                X_validate, y_validate = X_test, y_test\n",
    "            else:\n",
    "                X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "                \n",
    "\n",
    "        model = model_generator(X_train, f, k)\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        # fit network\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "                  batch_size=batch_size)\n",
    "        \n",
    "        ### analysis\n",
    "        y_test = y_test.ravel()\n",
    "        y_naive = X[-n_test_frames:, :, -1, new_cases_weighted_index].ravel()\n",
    "        y_predict = model.predict(X_test).ravel()\n",
    "        # evaluate model\n",
    "\n",
    "        mae_naive = mean_absolute_error(y_test, y_naive)\n",
    "        mae_predict = mean_absolute_error(y_test, y_predict)\n",
    "        r2_naive = explained_variance_score(y_test, y_naive)\n",
    "        r2_predict = explained_variance_score(y_test, y_predict)\n",
    "        mae_naive_list.append(mae_naive)\n",
    "        mae_predict_list.append(mae_predict)\n",
    "        model_list.append(model)\n",
    "        print('{}-step MAE [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, mae_naive, mae_predict))\n",
    "        print('{}-step R^2 [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, r2_naive, r2_predict))\n",
    "        \n",
    "        true_predict_plot(y_test, y_naive, y_predict, title='')\n",
    "        residual_diff_plots(y_naive, y_predict, y_test, n_test_frames, n_days_into_future, n_countries)\n",
    "        \n",
    "    return test, naive, prediction, mae_naive_list, mae_predict_list, model_list\n",
    "\n",
    "def SeparableConv2D_model(X_train, f, k):\n",
    "    (X_train_A, X_train_B) = X_train\n",
    "    fA, fB, fAB = f\n",
    "    kA, kB, kAB = k\n",
    "    n_countries = X_train_B.shape[1]\n",
    "    # define two sets of inputs\n",
    "    # define two sets of inputs\n",
    "    inputA = Input(shape=X_train_A.shape[1:])\n",
    "    inputB = Input(shape=X_train_B.shape[1:])\n",
    "\n",
    "    # the first branch operates on the first input\n",
    "    A = Conv1D(filters=int(fA),\n",
    "               kernel_size=int(kA),        \n",
    "               padding='valid',\n",
    "               #kernel_regularizer=l2(0.001),\n",
    "               activation='relu'\n",
    "                )(inputA)\n",
    "    # for compatibility with Separable2D\n",
    "    A = Reshape((1,A.shape[1],  A.shape[2]))(A)\n",
    "    A = Model(inputs=inputA, outputs=A)\n",
    "\n",
    "    # If the convolution over the countries isn't n_counties then it will depend on how the countries are\n",
    "    # themselves ordered.  \n",
    "    # convolve countries first then time second\n",
    "    # the second branch opreates on the second input\n",
    "    B = SeparableConv2D(filters=int(fB),\n",
    "                        kernel_size=[n_countries, int(kB)],\n",
    "                        activation='relu',\n",
    "                        padding='valid'\n",
    "                       )(inputB)\n",
    "\n",
    "\n",
    "    B = Model(inputs=inputB, outputs=B)\n",
    "    # combine the output of the two branches\n",
    "    combined = concatenate([A.output, B.output], axis=1)\n",
    "\n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "    AB = SeparableConv2D(filters=int(fAB),\n",
    "                        kernel_size=[2, int(kAB)],\n",
    "                        activation='relu',\n",
    "                        padding='valid'\n",
    "                       )(combined)\n",
    "\n",
    "    AB = Flatten()(AB)\n",
    "    AB = Dense(AB.shape[-1]//2, activation='relu')(AB)\n",
    "    AB = Dense(1, activation='relu')(AB)\n",
    "\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=[A.input, B.input], outputs=AB)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel (1+2) D model\n",
    "\n",
    "Thoughts: the more I'm training to more that the performance simply approaches the baseline model, as it's overtraining on the \"present\" values\n",
    "\n",
    "switching the order of countries and timesteps affects the following:\n",
    "\n",
    "    reshape A\n",
    "    kernel size B\n",
    "    kernal size AB\n",
    "    concatenate axis 1->2\n",
    "    \n",
    "    \n",
    "For SeperableConv2D needs to be organized in ```(batch_size, channels, rows, cols)``` is ``` data_format='channels_first'``` (not default)\n",
    "\n",
    "The main worry is that the number of samples is very low....  maybe change input_shape = (None, 103, 10) and generate more windows and frames? the problem; would have to pad or truncate such that they're all 32 in length. Truncation would introduce redudant data and \n",
    "\n",
    "Original code:\n",
    "\n",
    "Indeed I was correct about having pairwise samples. There for I need a copy of each frame for the number of countries. but these still need to be time ordered correctly. The input A is formatted as such\n",
    "\n",
    "    [Country 0, Frame 0, :]\n",
    "    [Country 0, Frame 1, :]\n",
    "    ...\n",
    "    [Country 0, Frame N, :]\n",
    "    [Country 1, Frame 0, :]\n",
    "    [Country 1, Frame 1, :]\n",
    "    ...\n",
    "    [Country 1, Frame N, :]\n",
    "    \n",
    "Therefore the correct time ordering is made by simply tiling input B along its first  axis (axis=0)\n",
    "\n",
    "The idea is to create a predictive model with two components, the first just takes input wherein each sample is a single\n",
    "frame from a single country. The second input is a 2D convolution which takes the same frame from all countries. \n",
    "I need to make sure that the inputs are synchronized but I don't know how to check this. Maybe the output\n",
    "\n",
    "https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "\n",
    "\"The branches work independently until they are concatenated. This seems to imply that the batch sizes need to be the same,\n",
    "which would require a repetitition of the Conv2D tensor so that the batches equal. i.e. I would need to create a copy of the 2D data for each country so that each single-country-single-frame in inputA would be paired. Also, what am I predicting for the output. I believe that need to set it up for a single number. I.e. input A takes a single-country-single-frame, imbuing the model with knowledge of a single time series while the 2D input \"shares\" knowledge between countries at the same moment in time. Perhaps I should make it simply a 1-D convolution over the countries over a single day, namely, the latest data?\n",
    "\n",
    "maybe I should actually be convolving countries first, and then time. i.e. do not need transposition. \n",
    "\n",
    "filters have to be the same, so that output dimensions are the same?\n",
    "\n",
    "Original method just convolved individually and then flattened; this doesn't combine the information, instead,\n",
    "concatenate and then do ANOTHER SeparableConv2D. This convolves the specific (convolved) country frame with the 2D countries+frame convolved data.\n",
    "\n",
    "The convolution has not been doing what I thought it was doing all along. The \"features\" are the channels. Not the time steps or countries. I.e. for EACH FEATURE, the country and time axes are convolved. This constitutes an \"output\". I.e. for each filter there are 10 (n_features) outputs. These 10 outputs are then convolved with a pointwis convolution. \n",
    "\n",
    "I.e. I was right about the n_countries being the \"channels\" but for the wrong reason. In fact, I don't even know if I actually want a 2D convolution. This mixes the feature data together. \n",
    "\n",
    "2 ideas.\n",
    "I know I want to convolve along the time axis and I do not want to the features to communicate (this will \n",
    "\n",
    "How to capture time trends: have a time index. Do not mix the feature data; i.e. the trend is captured by the temporal\n",
    "convolution of the time index; but the filter \n",
    "\n",
    "So, each feature is convolving along the time axis and then the country axis. This means that for the time_index feature, \n",
    "this convolves the sequence [1,2,3,4,5,6,...] and then uses that result and convolves it with copies. creating a rectangle\n",
    "of identical rows. (if time is columns). then, this is pointwise convolved with the other features. \n",
    "\n",
    "If the channels are the features, then the ordering of the countries matters. therefore the channels have to be the countries,\n",
    "but this will make the order of the features matter. The way of getting around this is to make the kernel size == the span of\n",
    "the dimension, then only 1 convolution is computed which includes either all countries or all features. ...\n",
    "\n",
    "I believe that I do care about mixing feature information but I do not care about mixing time or country information, actually the country mixing is the entire points. s\n",
    "\n",
    "Axes that do not have a well defined order (i.e. countries and features) have to be convolved with kernel == n_features (or countries, respectively). Otherwise, the order of the features and countries will matter. Because this reduces the dim from n_features (or n_countries) -> 1 it's important to take a large number of filters. \n",
    "\n",
    "I believe I was mistaken with how Conv1D actually works. i.e. if I give Conv1D a time series of n features, then how does data of dimension ```(n_steps, n_features)``` reduce to ```(n_steps, n_filters)```??? In other words, I didn't think the feature data was being mixed but it seems I am wrong.\n",
    "\n",
    "Each channel (feature) has its own weight. I believe at least. Therefore, I should indeed be doing channels last. Now, whether to do time or countries first still needs to be determined.\n",
    "\n",
    "This is determined by how it interacts with the other input. Namely, there is a single country time series (with multiple features) being input. Now, \n",
    "\n",
    "What I ***know*** is that the convolution kernel for countries has to be n_countries large otherwise the order matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.iloc[:,1:]\n",
    "\n",
    "fA = 16\n",
    "fB = 16\n",
    "kA = 4\n",
    "kB = 4\n",
    "fAB = 32\n",
    "kAB = 4\n",
    "\n",
    "f = (fA, fB, fAB)\n",
    "k = (kA, kB, kAB)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "frame_size = 32\n",
    "start_date = frame_size + model_data.time_index.min()\n",
    "n_validation_frames = 1\n",
    "n_test_frames = 3\n",
    "predict_steps = [1,7,14]\n",
    "\n",
    "model_generator = SeparableConv2D_model\n",
    "results = n_step_model_predictions(model_data, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, batch_size,\n",
    "                             train_test_only=True)\n",
    "\n",
    "test, naive, prediction, mae_naive_list, mae_predict_list, model = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_naive_list,label='Naive')\n",
    "plt.plot(mae_predict_list,label='CNN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
