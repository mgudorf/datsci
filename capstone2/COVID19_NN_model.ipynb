{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        ax1.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_naive+1))\n",
    "        ax2.plot(xrange, np.log(y_true+1)\n",
    "                 -np.log(y_predict+1))\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        ax1.plot(xrange, y_true-y_naive)\n",
    "        ax2.plot(xrange, y_true-y_predict)\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def flatten_Xy(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict,n_countries, title='',suptitle=''):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    mse_train_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_train_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_train_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_train_naive, r2_predict))\n",
    "\n",
    "    true_predict_plot(y_true.ravel(), y_naive.ravel(), y_predict, title=title, suptitle=suptitle)\n",
    "    residual_diff_plots(y_true.ravel(), y_naive.ravel(), y_predict , n_days_into_future, n_countries)\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    Normalize with respect to some absolute max, just choose 2*absolute max of training set. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "data = pd.read_csv('cnn_data.csv',index_col=0)\n",
    "data = data.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 161)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_countries = data.location.nunique()\n",
    "n_dates = data.time_index.nunique()\n",
    "n_countries, n_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.time_index>=40]\n",
    "model_data = data.copy().iloc[:, 1:]\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 28\n",
    "start_date = frame_size\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 132 159\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_nn_train, y_nn_train, X_nn_validate, y_nn_validate, X_nn_test, y_nn_test) = scaled_splits\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "# ax.scatter(range(len(X)), X[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=30,color='k')\n",
    "ax.scatter(range(len(X_nn_train)), X_nn_train[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='train')\n",
    "ax.scatter(range(len(X_nn_train), len(X_nn_train)+len(X_nn_validate)), X_nn_validate[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='validate')\n",
    "ax.scatter(range(len(X_nn_train)+len(X_nn_validate), len(X)), X_nn_test[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='test')\n",
    "plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy(scaled_splits)\n",
    "(X_nn_train_model,y_nn_train_model,X_nn_validate_model,\n",
    " y_nn_validate_model,X_nn_test_model,y_nn_test_model) = flat_splits\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [50, 100, 200]\n",
    "batch_size_list = [64, 256, 1024]\n",
    "fc_quotient_list = [1,2,4,8,16]\n",
    "parameter_combinations = list(itertools.product(epochs_list, batch_size_list, fc_quotient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('nn_score_logging.csv'):\n",
    "    score_logging_df = pd.read_csv('nn_score_logging.csv', index_col=0)\n",
    "else:\n",
    "    score_logging_df = pd.DataFrame(np.array(parameter_combinations), \n",
    "                                    columns=['epochs','batch_size','filter_1','filter_2','kernel_1','kernel_2'])\n",
    "    score_logging_df.loc[:, 'mean_squared_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_squared_error'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel0 = RandomNormal(seed=0)\n",
    "kernel1 = RandomNormal(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "for i, hyper_parameters in enumerate(parameter_combinations):\n",
    "    \n",
    "    (epochs, batch_size, filter1, \n",
    "     filter2, kernel1_size, kernel2_size) = hyper_parameters\n",
    "    \n",
    "    if score_logging_df.isna().loc[i,'mean_squared_error']:\n",
    "        nn_input = Input(shape=X_nn_train.shape[2:])\n",
    "        flat = Flatten()(nn_input)\n",
    "        dense0 = Dense(int(frame_size//8), \n",
    "                        use_bias=False,\n",
    "                       kernel_initializer=kernel0,\n",
    "                       )(flat)\n",
    "        dense1 = Dense(1, \n",
    "                        activation='relu',\n",
    "                        use_bias=False,\n",
    "                       kernel_initializer=kernel1,\n",
    "                       )(dense0)\n",
    "\n",
    "        nn = Model(inputs=nn_input, outputs=dense1)\n",
    "        nn.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "        history = nn.fit(X_nn_train_model, y_nn_train_model, epochs=epochs, validation_data=(X_nn_validate_model, y_nn_validate_model), \n",
    "                  batch_size=batch_size, verbose=0)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('#',end='')\n",
    "    if i % 50 == 0:\n",
    "          print('\\n')\n",
    "    \n",
    "    y_true = y_nn_validate_model.ravel()\n",
    "    y_predict = nn.predict(X_nn_validate_model).ravel()\n",
    "    y_naive = y_validate_naive.ravel()\n",
    "\n",
    "    score_logging_df.loc[i,'naive_mean_squared_error'] = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    score_logging_df.loc[i,'mean_squared_error']  = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    score_logging_df.loc[i,'naive_explained_variance_score']  = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    score_logging_df.loc[i,'explained_variance_score']  = explained_variance_score(y_true.ravel(), y_predict)\n",
    "    score_logging_df.loc[i,'naive_mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_naive.ravel())\n",
    "    score_logging_df.loc[i,'mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_predict)\n",
    "    # every time a new score is calculated, overwrite the original file, to save space but also save progress scoring.\n",
    "    score_logging_df.to_csv('nn_score_logging.csv')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (16500, 868)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f61209af64cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = nn.fit(X_nn_train_model, y_nn_train_model, epochs=epochs, validation_data=(X_nn_validate_model, y_nn_validate_model), \n\u001b[1;32m----> 2\u001b[1;33m           batch_size=batch_size, verbose=2)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    374\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (16500, 868)"
     ]
    }
   ],
   "source": [
    "history = nn.fit(X_nn_train_model, y_nn_train_model, epochs=epochs, validation_data=(X_nn_validate_model, y_nn_validate_model), \n",
    "          batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_nn_train_model.ravel()\n",
    "y_predict = nn.predict(X_nn_train_model).ravel()\n",
    "model_analysis(y_true, y_train_naive, y_predict, n_countries, title='nn model', suptitle='Performance on training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_nn_validate_model.ravel()\n",
    "y_predict = nn.predict(X_nn_validate_model).ravel()\n",
    "model_analysis(y_true, y_validate_naive, y_predict, n_countries, title='nn model', suptitle='Performance on validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,5),sharex=True)\n",
    "(ax1,ax2) = axes.flatten()\n",
    "_ = ax1.hist(nn.get_weights()[0].ravel())\n",
    "_ = ax2.hist(nn.get_weights()[1].ravel())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
