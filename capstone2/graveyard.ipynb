{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 1)\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "    if normalization_method=='minmax':\n",
    "        # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "        # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "        # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "        # be repeated for each validation, test frame for each country for each timestep.\n",
    "        X_min = X_train.min(axis=(2))\n",
    "        X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "        X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "                        X_max[:, :, np.newaxis, :])\n",
    "        X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "                               X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "                                       X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "    else:\n",
    "        \n",
    "        X_mean = X_train.mean(axis=(1,2))\n",
    "        X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "        # To avoid division by zero. This is a big assumption but this typically occurs when the frame's feature\n",
    "        # value is identically zero, which would result in x-x_mean / x_std = 0 / 1 = 0. So it doesn't matter what \n",
    "        # the x_std value is changed to as they are always divided into 0.\n",
    "        #     X_std[np.where(X_std==0.)] = 1\n",
    "\n",
    "        #     # First two features are time_index and time_index (days_since_first_case)\n",
    "        #         if date_normalization==False:\n",
    "        #             X_mean[:,:2] = 0\n",
    "        #             X_std[:, :2] = 1\n",
    "        X_train_scaled = normal(X_train, \n",
    "                                X_mean[:, np.newaxis, np.newaxis, :],\n",
    "                                X_std[:, np.newaxis, np.newaxis, :])\n",
    "        X_test_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 1)\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "    if normalization_method=='minmax':\n",
    "        # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "        # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "        # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "        # be repeated for each validation, test frame for each country for each timestep.\n",
    "        X_min = X_train.min(axis=(2))\n",
    "        X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "        X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "                        X_max[:, :, np.newaxis, :])\n",
    "        X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "                               X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "                                       X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "    else:\n",
    "        \n",
    "        X_mean = X_train.mean(axis=(1,2))\n",
    "        X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "        X_train_scaled = normal(X_train, \n",
    "                                X_mean[:, np.newaxis, np.newaxis, :],\n",
    "                                X_std[:, np.newaxis, np.newaxis, :])\n",
    "        X_test_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index < max_date_in_window) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    y_time_index = time_index.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    # y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, (y,y_time_index)\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 1):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window) & \n",
    "                                (time_index > max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    # y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "                   \n",
    "    return splits\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "                   \n",
    "    return splits\n",
    "\n",
    "\n",
    "def simple_model_forecast(df, drift_features, naive_features, forecast_interval=7):\n",
    "    \"\"\" Simple model forecasting of predictors. \n",
    "    \n",
    "    df : DataFrame\n",
    "        Dataframe which contains training set only, for\n",
    "        every country.\n",
    "    \n",
    "    drift_features : list-like or pd.Index\n",
    "        Features which will be forecasted using a drift model.\n",
    "        \n",
    "    naive_features :\n",
    "        Features which will be forecasted using a naive (constant) model.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    \"\"\"\n",
    "    start, end = df.date_proxy.min(), df.date_proxy.max()\n",
    "    df_start = df[df.date_proxy == start].set_index('location')\n",
    "    df_end = df[df.date_proxy == end].set_index('location')\n",
    "    span = end - start\n",
    "    \n",
    "    # This computes the secant line for all \"drift features\", for all countries, separately.\n",
    "    drift_slopes = (1.0/span) * (df_end.loc[:, drift_features]\n",
    "                                 - df_start.loc[:, drift_features])\n",
    "\n",
    "    # Given a matrix of drifts, the correct way of using them to extrapolate (i.e. every\n",
    "    # row turns into a number of rows equal to forecast_interval) is to do the following\n",
    "    delta_t = np.arange(1, forecast_interval+1).reshape(-1, 1)\n",
    "    # the result is of course of the form, y = mx + b \n",
    "    drift_forecasts = pd.DataFrame((np.kron(drift_slopes.values, delta_t.reshape(-1, 1)) \n",
    "                       + np.kron(df_end.loc[:, drift_features].values, \n",
    "                                 np.ones(forecast_interval).reshape(-1,1))))\n",
    "    drift_forecasts.columns = drift_features\n",
    "    \n",
    "    \n",
    "    naive_df = df_end.loc[:, naive_features]\n",
    "    naive_forecasts = pd.concat(forecast_interval * [naive_df], axis=0).sort_index() \n",
    "    \n",
    "    drift_forecasts.index = naive_forecasts.index\n",
    "\n",
    "    forecast_df = pd.concat((drift_forecasts, naive_forecasts), axis=1)\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "\n",
    "def shift_for_forecasting(data, forecast_interval=1, target_name='new_cases_per_million'):\n",
    "    \n",
    "    country_groupby_indices = country_groupby_indices_list(data)\n",
    "    for i, country_indices in enumerate(country_groupby_indices):\n",
    "        # fundamentally different operation for rate of change\n",
    "#         if target_name == 'new_cases_change_per_million':\n",
    "#             y =  data.loc[country_indices, 'new_cases_per_million'].diff(1).fillna(0)\n",
    "#         else:\n",
    "        y = data.loc[country_indices, target_name]\n",
    "        y.index = country_indices \n",
    "        \n",
    "        if i == 0 :\n",
    "            shifted_y = y.shift(-forecast_interval)\n",
    "        else:\n",
    "            shifted_y = pd.concat((shifted_y, y.shift(-forecast_interval)),axis=0)\n",
    "\n",
    "    return shifted_y\n",
    "\n",
    "# def box\n",
    "\n",
    "def minmax(X, X_min, X_max):\n",
    "    # X_min and X_max need to have already been made into 4-d tensors with np.newaxis\n",
    "    tile_shape = np.array(np.array(X.shape) / np.array(X_min.shape), dtype=int)\n",
    "    denominator = np.tile(X_max, tile_shape) - np.tile(X_min, tile_shape)\n",
    "    denominator[denominator==0] = 1\n",
    "    X_scaled = (X - np.tile(X_min, tile_shape)) / denominator\n",
    "    return X_scaled\n",
    "\n",
    "def rolling_features(df, features, roll_widths):\n",
    "    new_feature_df_list = []\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values\n",
    "        rollmean = pd.DataFrame(df.set_index('location').loc[:, features].groupby(level=0).rolling(window).mean().fillna(value=0.))\n",
    "#         rollstd = pd.DataFrame(df.set_index('location').loc[:, features].groupby(level=0).rolling(window).std().fillna(value=0.))\n",
    "        new_features = rollmean.reset_index(drop=True)\n",
    "#         new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        rolling_mean_names = features + '_rolling_mean_' + str(window)\n",
    "#         rolling_std_names = features +'_rolling_std_' + str(window)\n",
    "        new_cols = rolling_mean_names\n",
    "#         new_cols = rolling_mean_names.append(rolling_std_names)\n",
    "        new_features.columns = new_cols\n",
    "        new_feature_df_list.append(new_features)\n",
    "        new_df = pd.concat(new_feature_df_list,axis=1)\n",
    "        new_df.index = df.index\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub(r'([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub(r'([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub(r'([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub(r'([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "\n",
    "\n",
    "def normal(X, X_mean, X_std):\n",
    "    tile_shape = np.array(np.array(X.shape) / np.array(X_mean.shape), dtype=int)\n",
    "    mean_ = np.tile(X_mean, tile_shape)\n",
    "    std_ =  np.tile(X_std, tile_shape)   \n",
    "    std_[np.where(std_==0.)] = 1\n",
    "    X_scaled = ((X - mean_) /  std_)\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "\n",
    "\n",
    "def normal(X, X_mean, X_std):\n",
    "    tile_shape = np.array(np.array(X.shape) / np.array(X_mean.shape), dtype=int)\n",
    "    mean_ = np.tile(X_mean, tile_shape)\n",
    "    std_ =  np.tile(X_std, tile_shape)   \n",
    "    std_[np.where(std_==0.)] = 1\n",
    "    X_scaled = ((X - mean_) /  std_)\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    if len(df.index.names) == 1:\n",
    "        placeholder = df.index.name\n",
    "        df = df.reset_index()\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index(placeholder)#.sum()\n",
    "    else:\n",
    "        placeholder = df.index.names[0]\n",
    "        df = df.reset_index(level=0)\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index([placeholder, df.index])\n",
    "    return df\n",
    "\n",
    "def clean_DataFrame(df):\n",
    "    \"\"\" Remove all NaN or single value columns. \n",
    "    \n",
    "    \"\"\"\n",
    "    # if 0 then column is all NaN, if 1 then could be mix of NaN and a\n",
    "    # single value at most. \n",
    "    df = df.loc[:, df.columns[(df.nunique() > 0)]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_all_but_least_missing(df, feature):\n",
    "    matching_columns = column_search(df, feature, return_style='iloc', threshold='match') \n",
    "    feature_index =  matching_columns[df.iloc[:, matching_columns].isna().sum().argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data.location=='Afghanistan'].set_index(['time_index','location'])#.transpose()#.droplevel(1, axis=1)#.unstack()#.drop_level(1)\n",
    "\n",
    "# test = data.set_index(['time_index','location'])\n",
    "# pd.DataFrame(test.values.reshape(-1, 22*data.time_index.nunique())[0,:].reshape(-1, 22))\n",
    "\n",
    "# model_data = data.drop(columns=data.columns[data.columns.str.contains('flag')])\n",
    "# model_data.columns\n",
    "\n",
    "# model_data.iloc[:,2:].corr().replace(to_replace=1., value=np.nan).max()\n",
    "\n",
    "# model_data.iloc[:,2:].corr().replace(to_replace=1., value=np.nan).idxmax()\n",
    "\n",
    "# null_info = (data.groupby('location').mean()==0).sum(1).sort_values()==0\n",
    "# null_info.index[np.where(null_info)[0]].sort_values()\n",
    "\n",
    "# np.where(null_info)#[0]\n",
    "\n",
    "# null_info = (data.groupby('location').mean()==0).sum(1).sort_values()==0\n",
    "# full_data = data[data.location.isin(null_info.index[np.where(null_info)[0]])]\n",
    "\n",
    "# n_test_days = 1\n",
    "# n_days_into_future = 1\n",
    "# n_prune = 4\n",
    "# # model_data = data.drop(columns=column_search(data, 'test'))\n",
    "# # model_data = data[data.time_index>40]\n",
    "# # model_data = data.drop(columns=data.columns[data.columns.str.contains('flag')])#.drop(columns=data.columns[data.columns.str.contains('mean')])\n",
    "# model_data = full_data\n",
    "# # model_data = data.iloc[:,:8]\n",
    "# Xs, ys, model =  n_day_forecasting(model_data, n_test_days, n_days_into_future,\n",
    "#                                    n_prune=n_prune, col_transformer=MinMaxScaler())\n",
    "# y_true, y_naive, y_predict = ys\n",
    "# print('There were {} negative predictions, setting these values to 0.'.format(len(y_predict[y_predict<0])))\n",
    "# # y_predict[y_predict<0]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_prune = 2\n",
    "# mae_list_naive = []\n",
    "# r2_list_naive = []\n",
    "# mae_list_predict = []\n",
    "# r2_list_predict = []\n",
    "# # data = data[data.time_index >= first_day]\n",
    "# model_data = data.iloc[:, n_prune:].copy()#.apply(lambda x : np.log(x+1))\n",
    "# new_cases_index = column_search(model_data,'new_cases_weighted',threshold='match', return_style='iloc')[0]\n",
    "# n_countries = data.location.nunique()\n",
    "# target_data = data.new_cases_weighted\n",
    "# time_index = data.time_index\n",
    "# frame_size = 14\n",
    "# start_date = frame_size + time_index.min()\n",
    "# n_validation_frames = 0\n",
    "# n_test_frames = 1\n",
    "# n_days_into_future = 14\n",
    "# train_or_test = 'train'\n",
    "\n",
    "# for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "#     # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "#     frame_data = model_data[(time_index < max_date_in_window) & \n",
    "#                             (time_index >= max_date_in_window-frame_size)]\n",
    "#     #     print(frame_data.shape)\n",
    "#     # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "#     reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#     #     print(reshaped_frame_data.shape)\n",
    "#     # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "#     # the first axis is always the default iteration axis. \n",
    "#     # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "#     resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "#     frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "#     if max_date_in_window == start_date:\n",
    "#         X = frame_data_4D.copy()\n",
    "#     else:\n",
    "#         X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "# days_in_dataset_list = []\n",
    "# earliest_date_in_dataset = []\n",
    "# # X = X[80:,:,:,:]\n",
    "# for first_day in range(0, X.shape[0]-2):\n",
    "#     X = X[1:,:,:,:]\n",
    "#     days_in_dataset_list.append(X.shape[0])\n",
    "#     earliest_date_in_dataset.append(int(X[0, 0, 0, 0]))\n",
    "#     print(X.shape[0],end=' ')\n",
    "#     y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "#     y_time_index = time_index.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "#     # y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "\n",
    "#     X_train= X[:-n_test_frames,:,:,:]\n",
    "#     y_train =  y[:-n_test_frames,:]\n",
    "#     X_test = X[-n_test_frames:, :, :, :] \n",
    "#     y_test = y[-n_test_frames:, :]\n",
    "#     splits =  (X_train, y_train, X_test, y_test)\n",
    "#     y_train_time = y_time_index[:-n_test_frames,:]\n",
    "#     y_test_time = y_time_index[-n_test_frames:, :]\n",
    "\n",
    "#     X_train_model = np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "#     X_test_model = np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)#[:,2:23]\n",
    "#     y_train_model = y_train.ravel()\n",
    "#     y_test_model = y_test.ravel()\n",
    "\n",
    "#     model = Ridge(fit_intercept=False, tol=1e-12) \n",
    "#     _ = model.fit(X_train_model, y_train_model.ravel())\n",
    "\n",
    "\n",
    "#     y_true = y_train_model\n",
    "#     _, y_predict, mae = classifier_analysis(model, X_train_model, \n",
    "#                                                  y_train_model.ravel(), \n",
    "#                                                  plot=False, metric='mae')\n",
    "\n",
    "\n",
    "# #     plt.plot(y_predict)\n",
    "# #     plt.plot(y_true.ravel())\n",
    "\n",
    "#     y_train_naive = X_train[:,:,-1,new_cases_index].ravel()\n",
    "#     y_test_naive = X_test[:,:,-1,new_cases_index].ravel()\n",
    "    \n",
    "# #     print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "#     y_predict[y_predict<0]=0\n",
    "\n",
    "\n",
    "#     mae_list_naive.append(mean_absolute_error(y_true.ravel(), y_train_naive.ravel()))\n",
    "#     mae_list_predict.append(mean_absolute_error(y_true.ravel(), y_predict))\n",
    "#     r2_list_naive.append(explained_variance_score(y_true.ravel(), y_train_naive.ravel()))\n",
    "#     r2_list_predict.append(explained_variance_score(y_true.ravel(), y_predict))\n",
    "\n",
    "# #     print('{}-step MAE [Naive, Ridge Regression] = [{},{}]'.format(\n",
    "# #     n_days_into_future, mae_train_naive, mae_predict))\n",
    "# #     print('{}-step R^2 [Naive, Ridge Regression] = [{},{}]'.format(\n",
    "# #     n_days_into_future, r2_train_naive, r2_predict))\n",
    "\n",
    "# #     true_predict_plot(y_true.ravel(), y_train_naive.ravel(), y_predict)\n",
    "# #     residual_diff_plots(y_true.ravel(), y_train_naive.ravel(), y_predict , n_days_into_future, data.location.nunique())\n",
    "\n",
    "# train_or_test = 'train'\n",
    "# if train_or_test == 'train':\n",
    "#     y_true = y_train_model\n",
    "#     _, y_predict, mae = classifier_analysis(model, X_train_model, \n",
    "#                                                  y_train_model.ravel(), \n",
    "#                                                  plot=False, metric='mae')\n",
    "# else:\n",
    "#     y_true = y_test\n",
    "#     _, y_predict, mae = classifier_analysis(model, X_test_model,\n",
    "#                                                  y_test.ravel(),\n",
    "#                                                  plot=False, metric='mae')\n",
    "    \n",
    "# y_train_naive = X_train[:, :, -1, new_cases_index].ravel()\n",
    "# y_test_naive = X_test[:, :, -1, new_cases_index].ravel()\n",
    "\n",
    "# print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "# y_predict[y_predict<0]=0\n",
    "\n",
    "\n",
    "# mae_train_naive = mean_absolute_error(y_true.ravel(), y_train_naive.ravel())\n",
    "# mae_predict = mean_absolute_error(y_true.ravel(), y_predict)\n",
    "# r2_train_naive = explained_variance_score(y_true.ravel(), y_train_naive.ravel())\n",
    "# r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "# print('{}-step MAE [Naive, Ridge Regression] = [{},{}]'.format(\n",
    "# n_days_into_future, mae_train_naive, mae_predict))\n",
    "# print('{}-step R^2 [Naive, Ridge Regression] = [{},{}]'.format(\n",
    "# n_days_into_future, r2_train_naive, r2_predict))\n",
    "\n",
    "# true_predict_plot(y_true.ravel(), y_train_naive.ravel(), y_predict)\n",
    "# residual_diff_plots(y_true.ravel(), y_train_naive.ravel(), y_predict , n_days_into_future, data.location.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(days_in_dataset_list, r2_list_naive)\n",
    "plt.xlabel('Number of days in training set')\n",
    "plt.ylabel('Naive baseline R2 value')\n",
    "\n",
    "plt.plot(days_in_dataset_list,r2_list_predict)\n",
    "plt.xlabel('Number of days in training set')\n",
    "plt.ylabel('Predictions (on training data) R2 value')\n",
    "\n",
    "plt.plot(days_in_dataset_list, mae_list_predict)\n",
    "plt.xlabel('Number of days in training set')\n",
    "plt.ylabel('Predictions (on training data) MAE')\n",
    "\n",
    "plt.plot(days_in_dataset_list,mae_list_naive)\n",
    "plt.xlabel('Number of days in training set')\n",
    "plt.ylabel('Naive MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot is a *truncation* of the *earlier* days in the data set. This is only predictions on the training data;\n",
    "the x axis corresponds to the earliest date in the dataset. In other words, the left endpoint of each curve includes all available dates (115 days), while the endpoint on the right includes only the most recent dates (2 days).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(earliest_date_in_dataset)-X[-1,0,0,0], mae_list_predict, label='MAE of predictions')\n",
    "plt.plot(np.array(earliest_date_in_dataset)-X[-1,0,0,0], mae_list_naive, label='MAE of naive baseline')\n",
    "plt.xlabel('Earliest date in dataset, relative to present date')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('MAE vs. Number of days in dataset, 14-day predictions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# framewise mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, X_train_model.shape[0]):\n",
    "#     current_frame_mean = X_train_model[i,:,:,:].mean(axis=1)\n",
    "#     current_frame_std = X_train_model[i,:,:,:].std(axis=1)\n",
    "#     latest_mean_array = np.tile(current_frame_mean[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     latest_std_array = np.tile(current_frame_std[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     if i == 0:\n",
    "#         frame_only_mean_array = latest_mean_array\n",
    "#         frame_only_std_array = latest_std_array\n",
    "#     else:\n",
    "\n",
    "#         frame_only_mean_array = np.concatenate((frame_only_mean_array, \n",
    "#                                                latest_mean_array)\n",
    "#                                               ,axis=0)\n",
    "#         frame_only_std_array = np.concatenate((frame_only_std_array, \n",
    "#                                                latest_std_array)\n",
    "#                                               ,axis=0)\n",
    "        \n",
    "\n",
    "# frame_only_std_array[np.where(frame_only_std_array==0)]=1\n",
    "# X_train_model_model = (X_train_model - frame_only_mean_array) / frame_only_std_array\n",
    "# # Use the latest min and max for test scaling.\n",
    "\n",
    "# latest_std_array[np.where(latest_std_array==0)] = 1\n",
    "# X_test_model_model = (X_test_model - latest_mean_array) / latest_std_array\n",
    "\n",
    "# X_train_model_model = np.concatenate(X_train_model_model.reshape(X_train_model_model.shape[0], \n",
    "#                                                                      X_train_model_model.shape[1], -1), axis=0)\n",
    "# X_test_model_model = np.concatenate(X_test_model_model.reshape(X_test_model_model.shape[0], \n",
    "#                                                                    X_test_model_model.shape[1], -1), axis=0)\n",
    "\n",
    "# y_train_model = y_train.ravel()\n",
    "# y_test_model = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# framewise min max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, X_train.shape[0]):\n",
    "#     # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "#     # each time step in the frame. \n",
    "#     current_frame_min = X_train[i,:,:,:].min(axis=1)\n",
    "#     current_frame_max = X_train[i,:,:,:].max(axis=1)\n",
    "#     latest_min_array = np.tile(current_frame_min[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     latest_max_array = np.tile(current_frame_max[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     if i == 0:\n",
    "#         frame_only_min_array = latest_min_array\n",
    "#         frame_only_max_array = latest_max_array\n",
    "#     else:\n",
    "\n",
    "#         frame_only_min_array = np.concatenate((frame_only_min_array, \n",
    "#                                                latest_min_array)\n",
    "#                                               ,axis=0)\n",
    "#         frame_only_max_array = np.concatenate((frame_only_max_array, \n",
    "#                                                latest_max_array)\n",
    "#                                               ,axis=0)\n",
    "        \n",
    "\n",
    "# frame_only_minmax_denominator = (frame_only_max_array-frame_only_min_array)\n",
    "# num_zeros_train = (frame_only_minmax_denominator==0).sum()\n",
    "\n",
    "# frame_only_minmax_denominator[np.where(frame_only_minmax_denominator==0)]=1\n",
    "# X_train_normalized = (X_train - frame_only_min_array) / frame_only_minmax_denominator\n",
    "# # Use the latest min and max for test scaling.\n",
    "\n",
    "# frame_only_denom_for_test = latest_max_array - latest_min_array\n",
    "# num_zeros_test = (frame_only_denom_for_test==0).sum()\n",
    "\n",
    "# frame_only_denom_for_test[np.where(frame_only_denom_for_test==0)] = 1\n",
    "# X_test_normalized = (X_test - latest_min_array) / frame_only_denom_for_test\n",
    "\n",
    "# X_train_model = np.concatenate(X_train_normalized.reshape(X_train_normalized.shape[0], \n",
    "#                                                                      X_train_normalized.shape[1], -1), axis=0)\n",
    "# X_test_model = np.concatenate(X_test_normalized.reshape(X_test_normalized.shape[0], \n",
    "#                                                                    X_test_normalized.shape[1], -1), axis=0)\n",
    "\n",
    "# y_train_model = y_train.ravel()\n",
    "# y_test_model = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, X_train.shape[0]):\n",
    "    # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "    # each time step in the frame. \n",
    "    current_frame_min = X_train[i,:,:,:].min(axis=1)\n",
    "    current_frame_max = X_train[i,:,:,:].max(axis=1)\n",
    "    latest_min_array = np.tile(current_frame_min[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "    latest_max_array = np.tile(current_frame_max[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "    if i == 0:\n",
    "        frame_only_min_array = latest_min_array\n",
    "        frame_only_max_array = latest_max_array\n",
    "    else:\n",
    "\n",
    "        frame_only_min_array = np.concatenate((frame_only_min_array, \n",
    "                                               latest_min_array)\n",
    "                                              ,axis=0)\n",
    "        frame_only_max_array = np.concatenate((frame_only_max_array, \n",
    "                                               latest_max_array)\n",
    "                                              ,axis=0)\n",
    "        \n",
    "\n",
    "frame_only_minmax_denominator = (frame_only_max_array-frame_only_min_array)\n",
    "num_zeros_train = (frame_only_minmax_denominator==0).sum()\n",
    "\n",
    "frame_only_minmax_denominator[np.where(frame_only_minmax_denominator==0)]=1\n",
    "X_train_normalized = (X_train - frame_only_min_array) / frame_only_minmax_denominator\n",
    "# Use the latest min and max for test scaling.\n",
    "\n",
    "frame_only_denom_for_test = latest_max_array - latest_min_array\n",
    "num_zeros_test = (frame_only_denom_for_test==0).sum()\n",
    "\n",
    "frame_only_denom_for_test[np.where(frame_only_denom_for_test==0)] = 1\n",
    "X_test_normalized = (X_test - latest_min_array) / frame_only_denom_for_test\n",
    "\n",
    "# for i in range(1, X_train.shape[0]+1):\n",
    "#     # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "#     # each time step in the frame. \n",
    "#     up_to_current_frame_min = X_train[:i,:,:,:].min((0,2))\n",
    "#     up_to_current_frame_max = X_train[:i,:,:,:].max((0,2))\n",
    "#     latest_min_array = np.tile(up_to_current_frame_min[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     latest_max_array = np.tile(up_to_current_frame_max[np.newaxis, :, np.newaxis, :],(1,1,frame_size,1))\n",
    "#     if i == 1:\n",
    "#         frame_wise_min_array = latest_min_array\n",
    "#         frame_wise_max_array = latest_max_array\n",
    "#     else:\n",
    "\n",
    "#         frame_wise_min_array = np.concatenate((frame_wise_min_array, \n",
    "#                                                latest_min_array)\n",
    "#                                               ,axis=0)\n",
    "#         frame_wise_max_array = np.concatenate((frame_wise_max_array, \n",
    "#                                                latest_max_array)\n",
    "#                                               ,axis=0)\n",
    "        \n",
    "# frame_wise_minmax_denominator = (frame_wise_max_array-frame_wise_min_array)\n",
    "# num_zeros_train = (frame_wise_minmax_denominator==0).sum()\n",
    "# print('num zeros train', num_zeros_train)\n",
    "# frame_wise_minmax_denominator[np.where(frame_wise_minmax_denominator==0)]=1\n",
    "# X_train_normalized = (X_train - frame_wise_min_array) / frame_wise_minmax_denominator\n",
    "# # Use the latest min and max for test scaling.\n",
    "\n",
    "# frame_wise_denom_for_test = latest_max_array - latest_min_array\n",
    "# num_zeros_test = (frame_wise_denom_for_test==0).sum()\n",
    "\n",
    "# frame_wise_denom_for_test[np.where(frame_wise_denom_for_test==0)] = 1\n",
    "# X_test_normalized = (X_test - latest_min_array) / frame_wise_denom_for_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-frame minmax, then up to present day minmaxing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prune = 4\n",
    "first_day = 40\n",
    "mae_list_naive = []\n",
    "r2_list_naive = []\n",
    "mae_list_predict = []\n",
    "r2_list_predict = []\n",
    "data = data[data.time_index >= first_day]\n",
    "model_data = data.iloc[:, n_prune:].copy()#.apply(lambda x : np.log(x+1))\n",
    "new_cases_index = column_search(model_data,'new_cases_weighted',threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_weighted\n",
    "time_index = data.time_index\n",
    "frame_size = 14\n",
    "start_date = frame_size + time_index.min()\n",
    "n_validation_frames = 0\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'\n",
    "n_features = model_data.shape[-1]\n",
    "\n",
    "\n",
    "for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "    # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "    frame_data = model_data[(time_index < max_date_in_window) & \n",
    "                            (time_index >= max_date_in_window-frame_size)]\n",
    "    #     print(frame_data.shape)\n",
    "    # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "    reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "    #     print(reshaped_frame_data.shape)\n",
    "    # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "    # the first axis is always the default iteration axis. \n",
    "    # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "    resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "    frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "    if max_date_in_window == start_date:\n",
    "        X = frame_data_4D.copy()\n",
    "    else:\n",
    "        X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "\n",
    "\n",
    "y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "y_time_index = time_index.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "# y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "\n",
    "X_train= X[:-n_test_frames,:,:,:]\n",
    "y_train =  y[:-n_test_frames,:]\n",
    "X_test = X[-n_test_frames:, :, :, :] \n",
    "y_test = y[-n_test_frames:, :]\n",
    "splits =  (X_train, y_train, X_test, y_test)\n",
    "y_train_time = y_time_index[:-n_test_frames,:]\n",
    "y_test_time = y_time_index[-n_test_frames:, :]\n",
    "X_train_model = X_train\n",
    "X_test_model = X_test\n",
    "\n",
    "y_train_naive = X_train[:, :, -1, new_cases_index]#.ravel()\n",
    "y_test_naive = X_test[:, :, -1, new_cases_index]#.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I think its likely that I need to aggregate/normalize with respect to all countries, otherwise the relative values won't be predicted accurately, as well as change the order of the data so that it includes all countries \n",
    "\n",
    "# To capture seasonality, normalize in-frame. To capture trend, normalize up to frame, AFTER in-frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "_ = ax.plot(ev_scores, marker='.', markersize=2, linewidth=0.5, color='k')\n",
    "\n",
    "_ = ax.add_patch(Rectangle((0, 0), missing_flag_epoch.min(),  max(ev_scores), \n",
    "                           angle=0.0, alpha=0.5, color='g', label='original data'))\n",
    "\n",
    "_ = ax.add_patch(Rectangle((missing_flag_epoch.min(), 0), \n",
    "                           missing_flag_epoch.max()-missing_flag_epoch.min(),  max(ev_scores), \n",
    "                           angle=0.0, alpha=0.5, color='k', label='Missing value flags'))\n",
    "\n",
    "_ = ax.add_patch(Rectangle((rolling_epoch.min(), 0), \n",
    "                           rolling_epoch.max()-rolling_epoch.min(), max(ev_scores), \n",
    "                           angle=0.0, alpha=0.25, color='red', label='Rolling averages'))\n",
    "\n",
    "_ = ax.add_patch(Rectangle((location_dummies.min(), 0),\n",
    "                           location_dummies.max()-location_dummies.min(),  max(ev_scores), \n",
    "                           angle=0.0, alpha=0.5, color='gray', label='Location one-hot'))\n",
    "\n",
    "_ = ax.add_patch(Rectangle((date_dummies.min(), 0), \n",
    "                           date_dummies.max()-date_dummies.min(), max(ev_scores), \n",
    "                           angle=0.0, alpha=0.25, color='blue', label='Date one-hot'))\n",
    "\n",
    "_ = ax.set_ylabel('Explained Variance')\n",
    "_ = ax.set_xlabel('Number of features included in model')\n",
    "_ = plt.legend(loc=(1.05,0.6), title='Feature Origin')\n",
    "\n",
    "drift_features = model_data.loc[:, :'time_index'].iloc[:, :-2].columns.tolist()\n",
    "\n",
    "naive_features = (time_independent.columns.tolist() \n",
    "                  + location_one_hot.columns.tolist()\n",
    "                  +tests_units_one_hot.columns.tolist()\n",
    "                  + flag_and_misc.columns.tolist()[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 slices date, location, 4 slices date,location,time_index, days_since_first_case\n",
    "\n",
    "X_train = X.loc[train_indices, :]#.apply(lambda x : np.log(x+1))\n",
    "y_train = y.loc[train_indices,['time_index','new_cases_weighted']]\n",
    "\n",
    "X_test = X.loc[test_indices,:]#.apply(lambda x : np.log(x+1))\n",
    "y_test =  y.loc[test_indices,['time_index','new_cases_weighted']]#.values.ravel()\n",
    "\n",
    "# if train_or_test == 'train':\n",
    "y_train_naive = X_train.loc[:, ['time_index','new_cases_weighted']]\n",
    "# else:\n",
    "y_test_naive = X_test.loc[:, ['time_index','new_cases_weighted']]\n",
    "X_train = X_train.iloc[:,n_prune:]\n",
    "X_test = X_test.iloc[:,n_prune:]\n",
    "col_transformer = MinMaxScaler()\n",
    "_ = col_transformer.fit(X_train)\n",
    "X_train_normalized =  col_transformer.transform(X_train)\n",
    "X_test_normalized =  col_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
