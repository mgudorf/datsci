{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. This process is kept track of by encoding the dates which have interpolated values.\n",
    "There are two measures to do so. Essentially its one hot encoding for the categories ['extrapolated', 'interpolated', 'actual']. The other measure is to track the \"days since infection\" where 0 represents the first day with a recorded\n",
    "case of COVID within that country. I leave the more complex feature creation to the exploratory data analysis portion\n",
    "of this project.\n",
    "\n",
    "Some of the data is currently not used but may be incorporated later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "\n",
    "**Data available at:**\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "\n",
    "**Data available via github**\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "\n",
    "**Data available at:**\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "\n",
    "\n",
    "**If API used to pull data (I elect not to because the datasets are different)**\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "<!-- **Website which lead me to dataset**\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/) -->\n",
    "\n",
    "**Data available at:**\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_values(values_to_transform, category='columns',dateformat=None):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    # three cases, column names, country names, or datetime. \n",
    "    if category == 'location':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val).lower())\n",
    "                                                        .split()).title())\n",
    "        transformed_values = pd.Series(reformatted_values).replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    \n",
    "    elif category == 'columns':\n",
    "        reformatted_values = []\n",
    "        for val in values_to_transform:\n",
    "            reformatted_values.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                     re.sub('([A-Z]+)|_|\\/|\\)|\\(', r' \\1', val)\n",
    "                                                            .lower()).split()))\n",
    "        transformed_values = pd.Series(reformatted_values)\n",
    "        \n",
    "    elif category == 'date':\n",
    "        transformed_values = pd.to_datetime(pd.Series(\n",
    "            values_to_transform), errors='coerce',format=dateformat).dt.normalize()\n",
    "\n",
    "\n",
    "    return transformed_values\n",
    "\n",
    "def clean_DataFrame(df):\n",
    "    \"\"\" Remove all NaN or single value columns. \n",
    "    \n",
    "    \"\"\"\n",
    "    # if 0 then column is all NaN, if 1 then could be mix of NaN and a\n",
    "    # single value at most. \n",
    "    df = df.loc[:, df.columns[(df.nunique() > 0)]]\n",
    "    return df\n",
    "    \n",
    "#     reformatted_country_names = []\n",
    "#     for c in df.index.get_level_values(0):\n",
    "#         reformatted_country_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "#                                                     re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "#                                                     .split()).title())\n",
    "        \n",
    "#         reformatted_dates = pd.to_datetime(df.index.get_level_values(1)).normalize()\n",
    "#         restored_columns = df.index.names\n",
    "#         df = df.reset_index()\n",
    "#         df.loc[:, restored_columns[0]] = reformatted_country_names\n",
    "#         df.loc[:, restored_columns[1]] = reformatted_dates\n",
    "# #         df = df.drop_duplicates()\n",
    "#         df = df.set_index(restored_columns).sort_index()\n",
    "        \n",
    "# #     if index:\n",
    "# #         # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "# #         reformatted_index_names = []\n",
    "# #         for c in df.index.get_level_values(0):\n",
    "# #             # handle labels which can be cast to datetime objects\n",
    "# #             try:\n",
    "# #                 reformatted_index_names.append(datetime.strftime(\n",
    "# #                     datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "# #             except ValueError:\n",
    "# #                 reformatted_index_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "# #                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "# #                                                         .split()).title())\n",
    "# #         restored_column = df.index.names[0]\n",
    "# #         df = df.reset_index(level=0)\n",
    "# #         df.loc[:, restored_column] = reformatted_index_names\n",
    "# #         df = df.set_index([restored_column, df.index]).sort_index()\n",
    "#     df = df.loc[df.index.drop_duplicates(),:]\n",
    "#     return df\n",
    "\n",
    "# def regularize_country_names(df):\n",
    "\n",
    "#     if len(df.index.names) == 1:\n",
    "#         placeholder = df.index.name\n",
    "#         df = df.reset_index()\n",
    "#         df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "#         print(len(df))\n",
    "#         df = df.drop_duplicates()\n",
    "#         print(len(df))\n",
    "#         df = df.set_index(placeholder)#.sum()\n",
    "#     else:\n",
    "#         placeholder = df.index.names[0]\n",
    "#         df = df.reset_index(level=0)\n",
    "#         df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "#         print(len(df))\n",
    "#         df = df.drop_duplicates()\n",
    "#         print(len(df))\n",
    "#         df = df.set_index([placeholder, df.index])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "def csse_daily_reports_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE daily report data from local machine. \n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "\n",
    "    #the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "    for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "        tmp_df = pd.read_csv(x)\n",
    "        tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "    #     df = column_or_index_string_reformat(pd.read_csv(x),columns=True,index=False)\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    daily_reports_df = pd.concat(df_list, axis=0)\n",
    "    daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "    daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "    daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values\n",
    "    return daily_reports_df\n",
    "    \n",
    "def csse_timeseries_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE time series data from local machine. \n",
    "    \"\"\"\n",
    "    global_df_list = []\n",
    "\n",
    "    for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "        global_tmp = column_or_index_string_reformat(pd.read_csv(x))\n",
    "        # only include the actual time series info; this removes latitude and \n",
    "        # longitude as well as other useless data.\n",
    "        global_specific_indice_list = [1] + list(range(4, global_tmp.shape[1]))\n",
    "        global_tmp = global_tmp.iloc[:,global_specific_indice_list].drop_duplicates().groupby(by='country_region').sum()\n",
    "        global_tmp = regularize_country_names(global_tmp)\n",
    "        # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "        time_series_name = x.split('.')[0].split('_')[-2]\n",
    "        global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    # concatenate the data and name it to abide by my convention. \n",
    "    global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "    global_time_series_df.index.names = ['location','date']\n",
    "    global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "    global_time_series_df = column_or_index_string_reformat(global_time_series_df, index=True, columns=False)\n",
    "    global_time_series_df = regularize_country_names(global_time_series_df.sort_index())\n",
    "    # Repeat the steps above but for United States statewide data. \n",
    "    usa_df_list = []\n",
    "    for y in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "        usa_tmp = column_or_index_string_reformat(pd.read_csv(y))\n",
    "        try:\n",
    "            usa_tmp = usa_tmp.drop(columns='population')\n",
    "        except: \n",
    "            pass\n",
    "        usa_specific_indice_list = [6] + list(range(10, usa_tmp.shape[1]))\n",
    "        usa_tmp = usa_tmp.iloc[:,usa_specific_indice_list].drop_duplicates().groupby(\n",
    "            by='province_state').sum()\n",
    "        time_series_name = '_'.join(y.split('.')[0].split('_')[-2:][::-1])\n",
    "        usa_tmp.index.name = 'state'\n",
    "        usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "    usa_time_series_df.index.names = ['location','date']\n",
    "    usa_time_series_df.columns.names = ['csse_us_timeseries']\n",
    "    usa_time_series_df = column_or_index_string_reformat(usa_time_series_df.sort_index(), index=True, columns=False)\n",
    "    \n",
    "    return global_time_series_df, usa_time_series_df\n",
    "\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    if len(df.index.names) == 1:\n",
    "        placeholder = df.index.name\n",
    "        df = df.reset_index()\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index(placeholder)#.sum()\n",
    "    else:\n",
    "        placeholder = df.index.names[0]\n",
    "        df = df.reset_index(level=0)\n",
    "        df.loc[:,placeholder] = df.loc[:,placeholder].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "        print(len(df))\n",
    "        df = df.drop_duplicates()\n",
    "        print(len(df))\n",
    "        df = df.set_index([placeholder, df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    try:\n",
    "        df_table.columns = df_table.columns.droplevel()\n",
    "        df_table.columns.names = ['']\n",
    "    except:\n",
    "        pass\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "def flag_nan_differences(df, df_altered, suffix):\n",
    "    # Use bitwise XOR to flag the values which have been changed from NaN to something else.\n",
    "    # values which get mapped true -> false are those that are changed. \n",
    "    flag_df = df.isna() ^ df_altered.isna()\n",
    "    z1 = tuple(flag_df.columns.get_level_values(0).tolist())\n",
    "    z2 = tuple((flag_df.columns.get_level_values(1) + suffix).tolist())\n",
    "    flag_df.columns = pd.MultiIndex.from_tuples(list(zip(z1,z2)),names=['dataset', 'features'])\n",
    "    return flag_df\n",
    "\n",
    "def regularize_names(df, datekey=None, locationkey=None, dateformat=None):\n",
    "    df.columns = reformat_values(df.columns, category='columns').values\n",
    "    if datekey is not None:\n",
    "        df.loc[:, 'date'] = reformat_values(df.loc[:, datekey], category='date', dateformat=None).values\n",
    "    if locationkey is not None:\n",
    "        df.loc[:, 'location'] =  reformat_values(df.loc[:, locationkey], category='location').values\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "#the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "#     df = column_or_index_string_reformat(pd.read_csv(x),columns=True,index=False)\n",
    "    df_list.append(tmp_df)\n",
    "\n",
    "daily_reports_df = pd.concat(df_list, axis=0)\n",
    "daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values\n",
    "daily_reports_df = daily_reports_df.groupby(['location','date']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function declared for this purpose, import and reform JHU CSSE data. Likewise, for\n",
    "the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "\n",
    "# #the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "# for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/*.csv'):\n",
    "#     tmp_df = pd.read_csv(x)\n",
    "#     tmp_df.columns = reformat_values(tmp_df.columns, category='columns').values\n",
    "# #     df = column_or_index_string_reformat(pd.read_csv(x),columns=True,index=False)\n",
    "#     df_list.append(tmp_df)\n",
    "\n",
    "# daily_reports_df = pd.concat(df_list, axis=0)\n",
    "# daily_reports_df.columns = reformat_values(daily_reports_df.columns, category='columns').values\n",
    "# daily_reports_df.loc[:, 'date'] = reformat_values(daily_reports_df.loc[:, 'last_update'], category='date').values\n",
    "# daily_reports_df.loc[:, 'location'] =  reformat_values(daily_reports_df.loc[:, 'country_region'], category='location').values\n",
    "# csse_global_daily_reports_df = daily_reports_df.groupby(['location','date']).sum()\n",
    "# csse_global_daily_reports_df = csse_global_daily_reports_df.drop(columns=['latitude', 'longitude', 'fips', 'lat', 'long'])\n",
    "# csse_global_daily_reports_df.columns = ['cases', 'deaths', 'recovered','active']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global daily reports is unreliable. look at united states confirmed cases time series for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = csse_global_daily_reports_df.loc['United States', :].cases.plot()\n",
    "# ax.set_ylabel('Cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_df_list = []\n",
    "\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :4]\n",
    "    datecols = tmp_df.iloc[:, 4:]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  reformat_values(catcols.loc[:, 'country_region'], category='location').values\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    global_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))\n",
    "    # only include the actual time series info; this removes latitude and \n",
    "    # longitude as well as other useless data.\n",
    "\n",
    "\n",
    "csse_global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "csse_global_time_series_df.index.names = ['location','date']\n",
    "csse_global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "csse_global_time_series_df.columns = ['cases', 'deaths', 'recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_df_list = []\n",
    "for x in glob.glob('COVID-19/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "    tmp_df = pd.read_csv(x)\n",
    "    catcols = tmp_df.iloc[:, :np.where(tmp_df.columns == '1/22/20')[0][0]]\n",
    "    catcols.columns = reformat_values(catcols.columns, category='columns').values\n",
    "    catcols.loc[:, 'location'] =  catcols.loc[:, 'province_state'].values\n",
    "    \n",
    "    datecols = tmp_df.iloc[:,np.where(tmp_df.columns == '1/22/20')[0][0]:]\n",
    "    datecols.columns = reformat_values(datecols.columns, category='date').values\n",
    "    usa_tmp = pd.concat((catcols.location,datecols),axis=1).groupby(by='location').sum().sort_index()\n",
    "    # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "    time_series_name = x.split('.')[0].split('_')[-2]\n",
    "    usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))\n",
    "    \n",
    "usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "usa_time_series_df.index.names = ['location','date']\n",
    "usa_time_series_df.columns.names = ['csse_us_timeseries']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df =pd.read_csv('./covid-19-data/public/data/owid-covid-data.csv')\n",
    "owid_df = regularize_names(owid_df, datekey='date', locationkey='location').set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>...</th>\n",
       "      <th>aged_65_older</th>\n",
       "      <th>aged_70_older</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cvd_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Guatemala</th>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>GTM</td>\n",
       "      <td>730</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>40.747</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.061</td>\n",
       "      <td>0.112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.694</td>\n",
       "      <td>3.016</td>\n",
       "      <td>7423.808</td>\n",
       "      <td>8.7</td>\n",
       "      <td>155.898</td>\n",
       "      <td>10.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.665</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Somalia</th>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>SOM</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.731</td>\n",
       "      <td>1.496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365.769</td>\n",
       "      <td>6.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.831</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominica</th>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>DMA</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>222.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9673.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.376</td>\n",
       "      <td>11.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>MYS</td>\n",
       "      <td>900</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.807</td>\n",
       "      <td>3.399</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.293</td>\n",
       "      <td>3.407</td>\n",
       "      <td>26808.164</td>\n",
       "      <td>0.1</td>\n",
       "      <td>260.942</td>\n",
       "      <td>16.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>CHN</td>\n",
       "      <td>83402</td>\n",
       "      <td>50</td>\n",
       "      <td>3346</td>\n",
       "      <td>0</td>\n",
       "      <td>57.945</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2.325</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.641</td>\n",
       "      <td>5.929</td>\n",
       "      <td>15308.712</td>\n",
       "      <td>0.7</td>\n",
       "      <td>261.899</td>\n",
       "      <td>9.74</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     iso_code  total_cases  new_cases  total_deaths  \\\n",
       "location  date                                                        \n",
       "Guatemala 2020-05-05      GTM          730         27            19   \n",
       "Somalia   2020-04-05      SOM            7          2             0   \n",
       "Dominica  2020-05-06      DMA           16          0             0   \n",
       "Malaysia  2020-03-20      MYS          900        110             2   \n",
       "China     2020-04-16      CHN        83402         50          3346   \n",
       "\n",
       "                      new_deaths  total_cases_per_million  \\\n",
       "location  date                                              \n",
       "Guatemala 2020-05-05           2                   40.747   \n",
       "Somalia   2020-04-05           0                    0.440   \n",
       "Dominica  2020-05-06           0                  222.250   \n",
       "Malaysia  2020-03-20           0                   27.807   \n",
       "China     2020-04-16           0                   57.945   \n",
       "\n",
       "                      new_cases_per_million  total_deaths_per_million  \\\n",
       "location  date                                                          \n",
       "Guatemala 2020-05-05                  1.507                     1.061   \n",
       "Somalia   2020-04-05                  0.126                     0.000   \n",
       "Dominica  2020-05-06                  0.000                     0.000   \n",
       "Malaysia  2020-03-20                  3.399                     0.062   \n",
       "China     2020-04-16                  0.035                     2.325   \n",
       "\n",
       "                      new_deaths_per_million  total_tests  ...  aged_65_older  \\\n",
       "location  date                                             ...                  \n",
       "Guatemala 2020-05-05                   0.112          NaN  ...          4.694   \n",
       "Somalia   2020-04-05                   0.000          NaN  ...          2.731   \n",
       "Dominica  2020-05-06                   0.000          NaN  ...            NaN   \n",
       "Malaysia  2020-03-20                   0.000      10143.0  ...          6.293   \n",
       "China     2020-04-16                   0.000          NaN  ...         10.641   \n",
       "\n",
       "                      aged_70_older  gdp_per_capita extreme_poverty  \\\n",
       "location  date                                                        \n",
       "Guatemala 2020-05-05          3.016        7423.808             8.7   \n",
       "Somalia   2020-04-05          1.496             NaN             NaN   \n",
       "Dominica  2020-05-06            NaN        9673.367             NaN   \n",
       "Malaysia  2020-03-20          3.407       26808.164             0.1   \n",
       "China     2020-04-16          5.929       15308.712             0.7   \n",
       "\n",
       "                      cvd_death_rate  diabetes_prevalence  female_smokers  \\\n",
       "location  date                                                              \n",
       "Guatemala 2020-05-05         155.898                10.18             NaN   \n",
       "Somalia   2020-04-05         365.769                 6.05             NaN   \n",
       "Dominica  2020-05-06         227.376                11.62             NaN   \n",
       "Malaysia  2020-03-20         260.942                16.74             1.0   \n",
       "China     2020-04-16         261.899                 9.74             1.9   \n",
       "\n",
       "                      male_smokers  handwashing_facilities  \\\n",
       "location  date                                               \n",
       "Guatemala 2020-05-05           NaN                  76.665   \n",
       "Somalia   2020-04-05           NaN                   9.831   \n",
       "Dominica  2020-05-06           NaN                     NaN   \n",
       "Malaysia  2020-03-20          42.4                     NaN   \n",
       "China     2020-04-16          48.4                     NaN   \n",
       "\n",
       "                      hospital_beds_per_100k  \n",
       "location  date                                \n",
       "Guatemala 2020-05-05                    0.60  \n",
       "Somalia   2020-04-05                    0.90  \n",
       "Dominica  2020-05-06                    3.80  \n",
       "Malaysia  2020-03-20                    1.90  \n",
       "China     2020-04-16                    4.34  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = regularize_names(pd.read_csv('OxCGRT_latest.csv'), locationkey='country_name')\n",
    "oxcgrt_df.loc[:, 'date'] = pd.to_datetime(oxcgrt_df.loc[:, 'date'], format='%Y%m%d')\n",
    "oxcgrt_df = oxcgrt_df.set_index(['location', 'date'])\n",
    "# oxcgrt_df = oxcgrt_df.drop(columns='m1_wildcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>country_code</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>c1_flag</th>\n",
       "      <th>c2_workplace_closing</th>\n",
       "      <th>c2_flag</th>\n",
       "      <th>c3_cancel_public_events</th>\n",
       "      <th>c3_flag</th>\n",
       "      <th>c4_restrictions_on_gatherings</th>\n",
       "      <th>c4_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>h3_contact_tracing</th>\n",
       "      <th>h4_emergency_investment_in_healthcare</th>\n",
       "      <th>h5_investment_in_vaccines</th>\n",
       "      <th>m1_wildcard</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>confirmed_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>stringency_index_for_display</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Costa Rica</th>\n",
       "      <th>2020-01-08</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>CRI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bermuda</th>\n",
       "      <th>2020-01-21</th>\n",
       "      <td>Bermuda</td>\n",
       "      <td>BMU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>GBR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>194990.0</td>\n",
       "      <td>29427.0</td>\n",
       "      <td>79.63</td>\n",
       "      <td>79.63</td>\n",
       "      <td>77.38</td>\n",
       "      <td>77.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kosovo</th>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>Kosovo</td>\n",
       "      <td>RKS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.71</td>\n",
       "      <td>94.71</td>\n",
       "      <td>93.57</td>\n",
       "      <td>93.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Switzerland</th>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>CHE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29898.0</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>63.09</td>\n",
       "      <td>63.09</td>\n",
       "      <td>57.38</td>\n",
       "      <td>57.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             country_name country_code  c1_school_closing  \\\n",
       "location       date                                                         \n",
       "Costa Rica     2020-01-08      Costa Rica          CRI                0.0   \n",
       "Bermuda        2020-01-21         Bermuda          BMU                0.0   \n",
       "United Kingdom 2020-05-06  United Kingdom          GBR                3.0   \n",
       "Kosovo         2020-04-02          Kosovo          RKS                3.0   \n",
       "Switzerland    2020-05-05     Switzerland          CHE                3.0   \n",
       "\n",
       "                           c1_flag  c2_workplace_closing  c2_flag  \\\n",
       "location       date                                                 \n",
       "Costa Rica     2020-01-08      NaN                   0.0      NaN   \n",
       "Bermuda        2020-01-21      NaN                   0.0      NaN   \n",
       "United Kingdom 2020-05-06      1.0                   2.0      1.0   \n",
       "Kosovo         2020-04-02      1.0                   2.0      1.0   \n",
       "Switzerland    2020-05-05      1.0                   2.0      1.0   \n",
       "\n",
       "                           c3_cancel_public_events  c3_flag  \\\n",
       "location       date                                           \n",
       "Costa Rica     2020-01-08                      0.0      NaN   \n",
       "Bermuda        2020-01-21                      0.0      NaN   \n",
       "United Kingdom 2020-05-06                      2.0      1.0   \n",
       "Kosovo         2020-04-02                      2.0      1.0   \n",
       "Switzerland    2020-05-05                      2.0      1.0   \n",
       "\n",
       "                           c4_restrictions_on_gatherings  c4_flag  ...  \\\n",
       "location       date                                                ...   \n",
       "Costa Rica     2020-01-08                            0.0      NaN  ...   \n",
       "Bermuda        2020-01-21                            0.0      NaN  ...   \n",
       "United Kingdom 2020-05-06                            4.0      1.0  ...   \n",
       "Kosovo         2020-04-02                            4.0      1.0  ...   \n",
       "Switzerland    2020-05-05                            4.0      1.0  ...   \n",
       "\n",
       "                           h3_contact_tracing  \\\n",
       "location       date                             \n",
       "Costa Rica     2020-01-08                 0.0   \n",
       "Bermuda        2020-01-21                 0.0   \n",
       "United Kingdom 2020-05-06                 0.0   \n",
       "Kosovo         2020-04-02                 1.0   \n",
       "Switzerland    2020-05-05                 NaN   \n",
       "\n",
       "                           h4_emergency_investment_in_healthcare  \\\n",
       "location       date                                                \n",
       "Costa Rica     2020-01-08                                    0.0   \n",
       "Bermuda        2020-01-21                                    0.0   \n",
       "United Kingdom 2020-05-06                                    0.0   \n",
       "Kosovo         2020-04-02                                    0.0   \n",
       "Switzerland    2020-05-05                                    NaN   \n",
       "\n",
       "                           h5_investment_in_vaccines  m1_wildcard  \\\n",
       "location       date                                                 \n",
       "Costa Rica     2020-01-08                        0.0          NaN   \n",
       "Bermuda        2020-01-21                        0.0          NaN   \n",
       "United Kingdom 2020-05-06                        0.0          NaN   \n",
       "Kosovo         2020-04-02                        0.0          NaN   \n",
       "Switzerland    2020-05-05                        NaN          NaN   \n",
       "\n",
       "                           confirmed_cases  confirmed_deaths  \\\n",
       "location       date                                            \n",
       "Costa Rica     2020-01-08              NaN               NaN   \n",
       "Bermuda        2020-01-21              NaN               NaN   \n",
       "United Kingdom 2020-05-06         194990.0           29427.0   \n",
       "Kosovo         2020-04-02            125.0               1.0   \n",
       "Switzerland    2020-05-05          29898.0            1476.0   \n",
       "\n",
       "                           stringency_index  stringency_index_for_display  \\\n",
       "location       date                                                         \n",
       "Costa Rica     2020-01-08              0.00                          0.00   \n",
       "Bermuda        2020-01-21              0.00                          0.00   \n",
       "United Kingdom 2020-05-06             79.63                         79.63   \n",
       "Kosovo         2020-04-02             94.71                         94.71   \n",
       "Switzerland    2020-05-05             63.09                         63.09   \n",
       "\n",
       "                           legacy_stringency_index  \\\n",
       "location       date                                  \n",
       "Costa Rica     2020-01-08                     0.00   \n",
       "Bermuda        2020-01-21                     0.00   \n",
       "United Kingdom 2020-05-06                    77.38   \n",
       "Kosovo         2020-04-02                    93.57   \n",
       "Switzerland    2020-05-05                    57.38   \n",
       "\n",
       "                           legacy_stringency_index_for_display  \n",
       "location       date                                             \n",
       "Costa Rica     2020-01-08                                 0.00  \n",
       "Bermuda        2020-01-21                                 0.00  \n",
       "United Kingdom 2020-05-06                                77.38  \n",
       "Kosovo         2020-04-02                                93.57  \n",
       "Switzerland    2020-05-05                                57.38  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_cases = regularize_names(pd.read_csv('test_tracker_cases.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(\n",
    "                            ['location', 'date']).drop(\n",
    "                                    columns=['population','country']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>cases_per100k</th>\n",
       "      <th>deaths_per100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Finland</th>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Bahamas</th>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myanmar</th>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cases  new_cases  deaths  cases_per100k  \\\n",
       "location    date                                                  \n",
       "Finland     2020-02-29      3          1       0            0.1   \n",
       "The Bahamas 2020-04-02     24          3       1            6.1   \n",
       "Myanmar     2020-04-16     85         11       4            0.2   \n",
       "\n",
       "                        deaths_per100k  \n",
       "location    date                        \n",
       "Finland     2020-02-29             0.0  \n",
       "The Bahamas 2020-04-02             0.3  \n",
       "Myanmar     2020-04-16             0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtracker_cases.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtracker_tests = regularize_names(pd.read_csv('test_tracker_tests.csv'),\n",
    "                          datekey='date', locationkey='country').set_index(['location', 'date']).drop(\n",
    "                                    columns=['population','country','source']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>new_tests</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>tests_per100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hungary</th>\n",
       "      <th>2020-04-17</th>\n",
       "      <td>3101</td>\n",
       "      <td>41590.0</td>\n",
       "      <td>430.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belize</th>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Zealand</th>\n",
       "      <th>2020-04-27</th>\n",
       "      <td>2939</td>\n",
       "      <td>123920.0</td>\n",
       "      <td>2569.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        new_tests  tests_cumulative  tests_per100k\n",
       "location    date                                                  \n",
       "Hungary     2020-04-17       3101           41590.0          430.5\n",
       "Belize      2020-04-01          0              13.0            3.3\n",
       "New Zealand 2020-04-27       2939          123920.0         2569.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtracker_tests.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used to model country-wide case numbers exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = [csse_global_time_series_df,\n",
    "                owid_df, oxcgrt_df, testtracker_cases, testtracker_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_intersection = global_data[0].index.levels[0].unique()\n",
    "dates_union =  global_data[0].index.levels[1].unique()\n",
    "dates_intersection =  global_data[0].index.levels[1].unique()\n",
    "\n",
    "for i in range(len(global_data)-1):\n",
    "    country_intersection = country_intersection.intersection(global_data[i+1].index.levels[0].unique())\n",
    "    dates_union = dates_union.union(global_data[i+1].index.levels[1].unique())\n",
    "    # not really intersection, this is the minimum date that at least one country has data for, in each dataset.\n",
    "    dates_intersection = dates_intersection.intersection(global_data[i+1].index.levels[1].unique())\n",
    "\n",
    "global_data_intersected = [intersect_country_index(df, country_intersection) for df in global_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2020-01-22 00:00:00 to 2020-05-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('The range of all dates is from {} to {}'.format(dates_intersection.min(), dates_intersection.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of countries included is 126\n"
     ]
    }
   ],
   "source": [
    "print('The final number of countries included is {}'.format(len(country_intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense, because of the intersections between data; to us the u.s. time series and ihme data together but not with\n",
    "the global data. The hospital data is very useful and so it may be important to look specifically at the small number of countries it contains. Regardless; by using only the global data we can keep 110 countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Want to have all time dependent data defined on the same time ranges for convenience;\n",
    "this involves two steps. 1. Initialize the new dates, 2. deal with the missing values. \n",
    "These missing values references the ones introduced by resampling or redefining the range of \n",
    "each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "time_normalized_global_data = [resample_dates(df, dates_intersection.normalize()) for df in global_data_intersected]\n",
    "# To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "data = pd.concat(time_normalized_global_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['csse', 'owid', 'oxcgrt', 'ttc', 'ttt']\n",
    "global_data_export = []\n",
    "for i, x in enumerate(time_normalized_global_data):\n",
    "    gd_export_copy = x.copy()\n",
    "    gd_export_copy.columns += '_' + names[i]\n",
    "    global_data_export.append(gd_export_copy)\n",
    "pd.concat(global_data_export,axis=1).to_csv('full_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of redundant data. going to predict the number of new cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = multiindex_to_table(data)\n",
    "# using np.where masks identically named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groupby_indices = [data_table[data_table.location==country].index for country in data_table.location.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I went through the features manually and selected the ones which were not redundant and actually seemed useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors_iloc = [0, 1, 11, 13] + list(range(18,32)) + list(range(34,60)) + [63, 72]\n",
    "# predictors_loc= data_table.columns[predictors_iloc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['location', 'date', 'new_cases_per_million', 'new_deaths_per_million',\n",
    "       'tests_units', 'population', 'population_density', 'median_age',\n",
    "       'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
    "       'cvd_death_rate', 'diabetes_prevalence', 'female_smokers',\n",
    "       'male_smokers', 'handwashing_facilities', 'hospital_beds_per_100k',\n",
    "       'c1_school_closing', 'c1_flag', 'c2_workplace_closing', 'c2_flag',\n",
    "       'c3_cancel_public_events', 'c3_flag', 'c4_restrictions_on_gatherings',\n",
    "       'c4_flag', 'c5_close_public_transport', 'c5_flag',\n",
    "       'c6_stay_at_home_requirements', 'c6_flag',\n",
    "       'c7_restrictions_on_internal_movement', 'c7_flag',\n",
    "       'c8_international_travel_controls', 'e1_income_support', 'e1_flag',\n",
    "       'e2_debt_contract_relief', 'e3_fiscal_measures',\n",
    "       'e4_international_support', 'h1_public_information_campaigns',\n",
    "       'h1_flag', 'h2_testing_policy', 'h3_contact_tracing',\n",
    "       'h4_emergency_investment_in_healthcare', 'h5_investment_in_vaccines',\n",
    "       'stringency_index', 'new_tests']\n",
    "df = data_table.loc[:, predictors].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.iloc[:, -1]\n",
    "df = df.drop(columns=['new_tests'])\n",
    "df = pd.concat((df, tmp),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dependent_features = ['new_cases_per_million', 'new_deaths_per_million',\n",
    "       'c1_school_closing', 'c2_workplace_closing', \n",
    "       'c3_cancel_public_events',  'c4_restrictions_on_gatherings',\n",
    "      'c5_close_public_transport', \n",
    "       'c6_stay_at_home_requirements',\n",
    "       'c7_restrictions_on_internal_movement',\n",
    "       'c8_international_travel_controls', 'e1_income_support',\n",
    "       'e2_debt_contract_relief', 'e3_fiscal_measures',\n",
    "       'e4_international_support', 'h1_public_information_campaigns',\n",
    "       'h2_testing_policy', 'h3_contact_tracing',\n",
    "       'h4_emergency_investment_in_healthcare', 'h5_investment_in_vaccines',\n",
    "       'stringency_index', 'new_tests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_features = df.columns[df.columns.str.contains('flag')].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: COVID death rate is obviously time dependent, but the form it takes in the reporting is piece-wise constant function, so \n",
    "#### I am going to treat it as \"independent' by simply forward filling values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_independent_features = ['population', 'population_density', 'median_age',\n",
    "       'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'extreme_poverty',\n",
    "       'diabetes_prevalence', 'female_smokers', 'male_smokers',\n",
    "       'handwashing_facilities', 'hospital_beds_per_100k', 'cvd_death_rate']\n",
    "\n",
    "misc_features = ['date', 'location', 'tests_units']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_independent_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature corresponding to new tests per million, to maintain consistency with cases per million and deaths\n",
    "per million.\n",
    "\n",
    "For whatever reason, the population values for Kosovo are missing; I am inserting approximates take from Google searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_indices in country_groupby_indices:\n",
    "    df.loc[country_indices, time_independent_features] = df.loc[country_indices, time_independent_features].fillna(method='ffill').fillna(method='bfill').values\n",
    "\n",
    "per_million = df.population / 1000000\n",
    "df.loc[:, 'new_tests_per_million'] = df.loc[:, 'new_tests'] / per_million\n",
    "df = df.drop(columns='new_tests')\n",
    "time_dependent_features.pop()\n",
    "time_dependent_features.append('new_tests_per_million')\n",
    "df.loc[df.population[df.population.isna()].index,'population_density'] = 154\n",
    "df.loc[df.population[df.population.isna()].index,'population'] = 1845000\n",
    "for country_indices in country_groupby_indices:\n",
    "    df.loc[country_indices, time_dependent_features] = df.loc[country_indices, time_dependent_features].fillna(method='ffill').fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate new tests per million people to match case and death data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was planning on using a \"days since first case\" variable, which would equal zero until the date of the\n",
    "first case, but I believe this would correlate too strongly with the target variable. To test this assumption I'll compute it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I misinterpreted the fact that days since first case is linear growth by defininition (really has the shape of a ReLU) and number of cases is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have aggregated and dropped the respective features, the missing values of the remaining data can be flagged and\n",
    "created into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cases_pos = data.iloc[:,0].replace(to_replace=0, value=np.nan).dropna().reset_index()\n",
    "country_groupby_indices_dropped_nan = [n_cases_pos[n_cases_pos.location==country].index for country in n_cases_pos.location.unique()]\n",
    "\n",
    "days_since = []\n",
    "for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "    nonzero_list = list(range(len(c)))\n",
    "    zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "    days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "df.loc[:, 'time_index'] = days_since\n",
    "df.loc[:, 'date_proxy'] = len(df.location.unique())*list(range(len(df.date.unique())))\n",
    "time_dependent_features += ['date_proxy', 'time_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_million_ts = ['new_cases_per_million','new_deaths_per_million','new_tests_per_million']\n",
    "df.loc[:, per_million_ts] = df.loc[:,per_million_ts].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.loc[:, flag_features].fillna('Missing').astype('category')\n",
    "for col in tmp.columns:\n",
    "    tmp.loc[:, col] = tmp.loc[:, col].cat.rename_categories({1.0 : '1', 0. : '0'})\n",
    "    \n",
    "dummy_tmp = pd.get_dummies(tmp)\n",
    "flag_data = dummy_tmp[dummy_tmp.columns[~dummy_tmp.columns.str.contains('Missing')]]\n",
    "df = df.drop(columns=flag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dependent_data = df.loc[:, time_dependent_features]\n",
    "time_independent_data= df.loc[:, time_independent_features]\n",
    "misc_data = df.select_dtypes(include=['object', 'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data = pd.concat((time_dependent_data, time_independent_data,\n",
    "                          flag_data, misc_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_data.to_csv('data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
