{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, ElasticNet, RidgeCV, Lasso\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.resetwarnings()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline comparisons.\n",
    "\n",
    "Because I am predicting the number of new cases per million on a daily basis, (i.e. not the rate) I am going to employ a naive (constant) baseline.\n",
    "\n",
    "\n",
    "## Metric to determine model accuracy\n",
    "\n",
    "I will be using mean absolute error as to minimize the effect of outliers.\n",
    "\n",
    "\n",
    "## Details yet to be incorporated:\n",
    "\n",
    "Growth of standard deviation over time in forecasting.\n",
    "To accomplish this, I will produce a number of \"new\" features which attempt to capture the time dependence via rolling averages and rolling standard deviations. I will also one-hot encode certain variables which represent flags for certain behaviors or discrete, time independent variables such as location (country)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the number of cases of COVID-19\n",
    "\n",
    "\n",
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [COVID-19 Case number modeling ](#model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    print(y_true.shape, y_naive.shape, y_predict.shape)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        ax1.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_naive+1))\n",
    "        ax2.plot(xrange, np.log(y_true+1)\n",
    "                 -np.log(y_predict+1))\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        ax1.plot(xrange, y_true-y_naive)\n",
    "        ax2.plot(xrange, y_true-y_predict)\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def normalize_Xy_splits(splits, feature_range=(0., 0.5), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 0.5)\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    for i in range(1, X_train.shape[0]+1):\n",
    "        # find the minima and maxima of all features for all countries, ranging up to current frame and \n",
    "        # each time step in the frame. \n",
    "        up_to_current_frame_min = X_train[:i,:,:,:].min((0,1,2))\n",
    "        up_to_current_frame_max = X_train[:i,:,:,:].max((0,1,2))\n",
    "        latest_min_array = np.tile(up_to_current_frame_min[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "        latest_max_array = np.tile(up_to_current_frame_max[np.newaxis, np.newaxis, np.newaxis, :],(1,n_countries,frame_size,1))\n",
    "        if i == 1:\n",
    "            frame_min_array = latest_min_array\n",
    "            frame_max_array = latest_max_array\n",
    "        else:\n",
    "            frame_min_array = np.concatenate((frame_min_array, \n",
    "                                                   latest_min_array)\n",
    "                                                  ,axis=0)\n",
    "            frame_max_array = np.concatenate((frame_max_array, \n",
    "                                                   latest_max_array)\n",
    "                                                  ,axis=0)\n",
    "\n",
    "    # frame_min_array = np.tile(frame_min_array, (1, n_countries, 1, 1))\n",
    "    # frame_max_array = np.tile(frame_max_array, (1, n_countries, 1, 1))\n",
    "\n",
    "    minmax_denominator = (frame_max_array-frame_min_array)\n",
    "    minmax_denominator[np.where(minmax_denominator==0)]=1\n",
    "    X_train_scaled = (max_-min_)*(X_train - frame_min_array) / minmax_denominator\n",
    "    # Use the latest min and max for test scaling.\n",
    "\n",
    "    latest_minmax_denominator = latest_max_array - latest_min_array\n",
    "    latest_minmax_denominator[np.where(latest_minmax_denominator==0)] = 1\n",
    "    X_validate_scaled = (max_- min_)*((X_validate - np.tile(latest_min_array,(n_validation_frames,1,1,1)))\n",
    "                                        / np.tile(latest_minmax_denominator,(n_validation_frames,1,1,1)))\n",
    "    X_test_scaled = (max_- min_)*((X_test - np.tile(latest_min_array,(n_test_frames,1,1,1))) \n",
    "                                        / np.tile(latest_minmax_denominator,(n_test_frames,1,1,1)))\n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "\n",
    "    train_norm_arrays =  (frame_max_array, frame_min_array, minmax_denominator)\n",
    "    validate_and_test_norm_arrays = (latest_max_array,latest_min_array,latest_minmax_denominator)\n",
    "\n",
    "    return scaled_splits, train_norm_arrays, validate_and_test_norm_arrays\n",
    "\n",
    "def flatten_Xy_splits(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict,n_countries, title='',suptitle=''):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    mse_train_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_train_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_train_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_train_naive, r2_predict))\n",
    "\n",
    "    true_predict_plot(y_true.ravel(), y_naive.ravel(), y_predict, title=title, suptitle=suptitle)\n",
    "    residual_diff_plots(y_true.ravel(), y_naive.ravel(), y_predict , n_days_into_future, n_countries)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data has the following partitions (list in order of partition as realized in data used in modelling):\n",
    "\n",
    "    1. Continuous time dependent data\n",
    "    2. Rolling averages of continuous time dependent data\n",
    "    3. Continuous time independent data\n",
    "    4. One-hot encoded data: location (country), flags for government responses, test units. \n",
    "    5. Time indexing variables (date, time-index (days since first case))\n",
    "        \n",
    "    Continuous time dependent data with drift as baseline model, Rolling averages computed afterwards\n",
    "    Time series variables with naive as baseline model : The complement to the drift baseline variables. \n",
    "\n",
    "tried scaling y.\n",
    "What do these values mean? The above values represent the maximum factor or magnitude which each country is trying to predict.\n",
    "i.e. when normalized up to frame N, the new_cases_weighted max will be, of course, 1. A y_train_model value of 20 means that the future value is 20 times larger than the current in-frame max.\n",
    "\n",
    "Weighting values up to frame n with max, min in all days up until then. Then, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing\n",
    "\n",
    "    get rid of all pre-global data, i.e. the days before every country in dataset has at least 1 case.\n",
    "    this corresponds to truncation via time_index or days_since_first_case\n",
    "\n",
    "# data format lists\n",
    "\n",
    "    single day, single country for each prediction\n",
    "    multiple day, single country for each prediction\n",
    "    single day, multiple country for each prediction. \n",
    "   \n",
    "# normalization methods\n",
    "    \n",
    "    none (wrong)\n",
    "    entire train, test (wrong)\n",
    "    up to present date minmax, standard scaler\n",
    "    in-frame minmax, standard scalar\n",
    "    up to present day and then in-frame. \n",
    "    \n",
    "\n",
    "\n",
    "# the types of models used\n",
    "     \n",
    "     Ridge(alpha=0.01, fit_intercept=False)\n",
    "     Lasso(alpha=0.01, fit_intercept=False)\n",
    "     ElasticNet(alpha=0.01, l1_ratio=0.9, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things tried so far\n",
    "\n",
    "    Lasso, Ridge, ElasticNet and their parameters\n",
    "    minmax and standard normal, normalization\n",
    "    in-frame and historical normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I'm safe as long as the training data does not include any days included in the 14 day rolling average of the day that I am trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make the regression work, need to leverage the time dependence more, I'm going to elect to use multiple day values of the same country to predict the values for each country one at a time, because the differences between countries are so large (in terms of weighted quantities). \n",
    "\n",
    "To do so, need to repeat the data such that (7, 14) days of predictors are in each row for each prediction. I.e. each row\n",
    "is a single country but their frame values. I just did this but perhaps it would be easier to create the frames from the CNN notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting the index makes it so X's index can be used to slice y. I.e. its the operation which \"realigns\" the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try or have been tried.\n",
    "\n",
    "Slicing the countries which have 0 mean for any variables as this is indicative of having no information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data_optimal.csv', index_col=0)\n",
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(columns=column_search(data, 'log'))\n",
    "data.loc[:, 'time_index'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.days_since_first_case > 0].groupby('location').time_index.min().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should likely slice countries which have not had sufficient amount of time of infection,\n",
    "do not have recorded values, and time_index < ~40 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction without any normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_date_in_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0347ea1ba46b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrain_or_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m frame_data = model_data[(time_index <= max_date_in_window-1) & \n\u001b[0m\u001b[0;32m     15\u001b[0m                         (time_index >= max_date_in_window-frame_size)]\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#     print(frame_data.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_date_in_window' is not defined"
     ]
    }
   ],
   "source": [
    "model_data = data.iloc[:,2:].copy()\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index.astype(int)\n",
    "frame_size = 28\n",
    "start_date = frame_size #+ time_index.min()\n",
    "# start_date = 50\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "train_or_test = 'train'\n",
    "\n",
    "frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                        (time_index >= max_date_in_window-frame_size)]\n",
    "#     print(frame_data.shape)\n",
    "# Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "\n",
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames,model_type='ridge')\n",
    "(train_indices, validate_indices, test_indices) = indices\n",
    "\n",
    "(X_train, y_train, X_validate,\n",
    " y_validate, X_test, y_test) = splits\n",
    "\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index]\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index]\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index]\n",
    "# y_train_naive = (np.exp(X_for_naive_slicing[train_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_validate_naive =  (np.exp(X_for_naive_slicing[validate_indices, last_day_new_cases_index])-1).ravel()\n",
    "# y_test_naive =  (np.exp(X_for_naive_slicing[test_indices, last_day_new_cases_index])-1).ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy_splits(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits\n",
    "\n",
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "regression_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = regression_model.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = regression_model.predict(X_regression_train).ravel()\n",
    "model_analysis(y_regression_train, y_train_naive, y_predict_train, n_countries, title='Ridge', suptitle='Scaled predictor training set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_model.predict(X_regression_validate).ravel()\n",
    "model_analysis(y_regression_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_predict_test = regression_model.predict(X_regression_test).ravel()\n",
    "model_analysis(y_regression_test, y_test_naive, y_predict_test, n_countries, title='Ridge', suptitle='Scaled predictor test set performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with minmax normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normalization is alot easier if I split into train, validate, test but this makes other parts more annoying\n",
    "scaled_splits, frame_minmax, latest_minmax =  normalize_Xy_splits(splits, feature_range=(0,1), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "\n",
    "(latest_max, latest_min, latest_denom) = latest_minmax\n",
    "(frame_max, frame_min, frame_denom) = frame_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_splits = flatten_Xy_splits(scaled_splits)\n",
    "(X_regression_scaled_train, y_regression_scaled_train, X_regression_scaled_validate,\n",
    " y_regression_scaled_validate, X_regression_scaled_test, y_regression_scaled_test) = flat_splits\n",
    "\n",
    "X_regression_scaled = np.concatenate((X_regression_scaled_train, X_regression_scaled_validate),axis=0)\n",
    "y_regression_scaled = np.concatenate((y_regression_scaled_train, y_regression_scaled_validate),axis=0)\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "regression_scaled_model = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = regression_scaled_model.fit(X_regression_scaled, y_regression_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = regression_scaled_model.predict(X_regression_scaled_train).ravel()\n",
    "model_analysis(y_regression_scaled_train, y_train_naive, y_predict_train, n_countries, title='Ridge', suptitle='Scaled predictor training set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_validate = regression_scaled_model.predict(X_regression_scaled_validate).ravel()\n",
    "model_analysis(y_regression_scaled_validate, y_validate_naive, y_predict_validate, n_countries, title='Ridge', suptitle='Scaled predictor validation set performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole point of this next section is to see what kind of results we would get if I *accidentally did snoop the future* to serve as a red flag if comparable results are reached with the more complicated normalization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the scaling. Does the frame-wise (up to present date) capture the overall\n",
    "## trend but the in-frame scaling captures the seasonality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with frame-wise scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when collapsing the last axis with reshape, the format I had in mind the columns would be grouped by time but instead I think they're grouped by feature. If grouped by feature then the first 14 rows should be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zeroth column is time index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If concatenate is doing what I think it is, it should be sorted by time_index then country. i.e. the -n_country row is the last row of the first country, with time_steps and features contracted. this is indeed what is happening, so in the final 2d array,\n",
    "the first n_countries rows corresponds to the first frame, and the columns are the features at the different time steps (a total of time steps equals ```frame_size```). s.t. X_train looks like the following\n",
    "\n",
    "    n_countries @ frame=frame_size, frame [n_features(t=0),n_features(t=1),...,n_features(t=frame_size)]\n",
    "    n_countries @ frame=frame_size+1, frame [n_features(t=1),n_features(t=2),...,n_features(t=frame_size+1)]\n",
    "    n_countries @ frame=frame_size+1, frame [n_features(t=1),n_features(t=2),...,n_features(t=frame_size+1)]\n",
    "    n_countries @ frame=present_date-n_days_into_future- n_test_frames  [n_features(t=1),n_features(t=2),...,n_features(t=frame_size+1)]\n",
    "    \n",
    "The normalization only occurs with respect to the dates inside the frame, i.e. each *row* of the final array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_model = (y_train- frame_wise_min_array[:, :, -1, 2]) / frame_wise_minmax_denominator[:,:,-1,2]\n",
    "y_test_model= (y_test - latest_min_array[:, :, -1, 2]) / frame_wise_denom_for_test[:,:,-1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to normalize in a consistent manner, need to find the min and max for each country and each feature up until that point in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the minima of each feature of each country up to current frame's end date. Need copies for each day in frame, but\n",
    "only one frame value i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X time index goes from 0 to max-n_days_into_future. \n",
    "\n",
    "first slice of X goes from 0:n_days_into_future (non-inclusive), i.e. 13. Therefore, the first prediction being made\n",
    "is t=27. Therefore, the first row of y_train equals X[27,:,0,2]\n",
    "\n",
    "X slices. start at 0, end at n_days_into_future - 1.\n",
    "\n",
    "so X[14] corresponds to n_days_into_future - 1 + 14 = 27\n",
    "\n",
    "X's rows correspond to the minimum time_index (starting from 0) value. so X[13] is 13,14,....26. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question for mike, each of the shifted variables is being treated as feature, should they be normalized separately...? or normalized before the shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need the naive values, which are the values n_days_into_future before y_train rows. To see that this is true, look at\n",
    "time_index + n_days_into_future rows. of X. i.e. first row of y is time==n_days_into_future for X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train[t,c] = X[t+n_days_into_future, c, 0, 2] OR \n",
    "\n",
    "y_train[t,c] = X[t+1, c, -1, 2]\n",
    "\n",
    "y_train[0, 26] = X[14,26,-1,2] = X[27,26,0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think minmaxing each frame might actually be really dumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old way of modeling, single day single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = 'train'\n",
    "n_test_days = 1\n",
    "n_days_into_future = 14\n",
    "n_prune = 4\n",
    "# X has already been shifted by n_days_into future, so the last day in X is predicting that last day + n_days_into_future. \n",
    "#     y_naive = X.new_cases_weighted[X.time_index > X.time_index.max() - n_days_into_future]\n",
    "n_countries = data.location.nunique()\n",
    "model_data = full_data\n",
    "# model_data = data.drop(columns=column_search(data, 'test'))\n",
    "\n",
    "chronological_data = model_data.sort_values(by=['time_index','location'])\n",
    "\n",
    "X = chronological_data[(chronological_data.time_index \n",
    "                        <=  chronological_data.time_index.max() - n_days_into_future)].reset_index(drop=True)\n",
    "\n",
    "y = chronological_data[(chronological_data.time_index  \n",
    "                        >= n_days_into_future)].loc[:, ['time_index','new_cases_weighted']].reset_index(drop=True)\n",
    "\n",
    "# X = X[(X.days_since_first_case > 0) & (X.time_index > 40)]\n",
    "# X = X[(X.days_since_first_case > 0)]\n",
    "# X = X[X.time_index > 40]\n",
    "\n",
    "y = y.loc[X.index,:]\n",
    "train_indices = X[X.time_index <= X.time_index.max() - n_test_days].index\n",
    "test_indices = X[X.time_index > X.time_index.max() - n_test_days].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data[data.time_index.isin(list(range(7)))].values.reshape(-1, 7*24))\n",
    "\n",
    "data.set_index('time_index')#.transpose()#[list(range(7))]#.transpose()\n",
    "\n",
    "chronological_data.set_index('time_index').transpose()[list(range(7))].transpose()\n",
    "\n",
    "chronological_data.set_index('time_index').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 slices date, location, 4 slices date,location,time_index, days_since_first_case\n",
    "\n",
    "X_train = X.loc[train_indices, :]#.apply(lambda x : np.log(x+1))\n",
    "y_train = y.loc[train_indices,['time_index','new_cases_weighted']]\n",
    "\n",
    "X_test = X.loc[test_indices,:]#.apply(lambda x : np.log(x+1))\n",
    "y_test =  y.loc[test_indices,['time_index','new_cases_weighted']]#.values.ravel()\n",
    "\n",
    "# if train_or_test == 'train':\n",
    "y_train_naive = X_train.loc[:, ['time_index','new_cases_weighted']]\n",
    "# else:\n",
    "y_test_naive = X_test.loc[:, ['time_index','new_cases_weighted']]\n",
    "X_train = X_train.iloc[:,n_prune:]\n",
    "X_test = X_test.iloc[:,n_prune:]\n",
    "col_transformer = MinMaxScaler()\n",
    "_ = col_transformer.fit(X_train)\n",
    "X_train_normalized =  col_transformer.transform(X_train)\n",
    "X_test_normalized =  col_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute new rolling means, take new drift feature values, concatenate with those in training, and then roll.\n",
    "Need to reintroduce indexes however to sort the values correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Case number modeling with no new features.\n",
    "<a id='model'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The first segment of the modeling process takes the \"original\" data, reformatted as per the\n",
    "data cleaning notebook and then performs the necessary operations to encode the categorical variables.\n",
    "The target variable is assigned to be the number of cases and the pipeline for the modeling process is as\n",
    "follows (current iteration):\n",
    "    \n",
    "    1. Encode data, split into feature and target data.\n",
    "    2. Create cross validation folds; split into train-test and holdout data sets. \n",
    "    2. Normalize the feature data via sklearn's StandardScaler. \n",
    "    3. Perform PCA on the normalized feature data\n",
    "    4. Apply Ridge regression on the normalized, PCA transformed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single step predictions\n",
    "\n",
    "Need location for groupby indices, but date and date proxy I do not think are useful currently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cross validation folds which respect the time-series dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of flailing about let's actually type out how to do this.\n",
    "\n",
    "Steps that need to be completed.\n",
    "\n",
    "    Target variable shifting.\n",
    "    CV folds\n",
    "    Feature encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the indices which correspond to each country, prior to shifting the target variable time series.\n",
    "Assign target and feature variable arrays prior to shifting the target variable.\n",
    "I don't think dropping the case data is actually necessary so long as the time series aspect is respected.\n",
    "Get the data ready for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the indices which correspond to each country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift the target variable by one so that the feature data up until day n-1 is predicting the target variable at day n.\n",
    "In other words, train on data which is up to but not including the date of prediction. \n",
    "\n",
    "What Mike said:\n",
    "\n",
    "X_train ......................................... y_train\n",
    "\n",
    "DATA UP TO AND INCLUDING 2020-05-01 ............. TARGET ON 2020-05-02\n",
    "\n",
    "This does not predict the first day and does not include the data on the last day. \n",
    "Therefore the correct operation is to group by country, shift the target by -1 and drop the last day of data (drop NaN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift the target data so that its suited to prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice and rename variables X,y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each countries' indices except minus the last day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create CV folds....unused other than train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the data in 7 folds, but use the first 6 as a training and the 7th as a testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast bool features as int, one-hot encode the categorical (object) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign target variable and remove the corresponding variables from the regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split between traintest (Cross validation, currently used simply as a 1-fold training set)\n",
    "and holdout (final testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the StandardScaler() transformer instance with the training data, only applied to the continuous, time dependent\n",
    "numerical features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- For every date, need a prediction. For every date need to scale/do pca.\n",
    "therefore everything is a for looop around the date.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictors from drift, naive strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 day step predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day 1 to predict day 8\n",
    "day 1-2 to predict day 9\n",
    "days 1-3 to predict day 10 ----> shift -7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target variable, the change in the amount of daily new cases (today new cases per million - yesterday new cases per million). Store the \"present values\" as a predictor, but then shift the values by a week, to get the target variable. Due to the usage of first order finite difference to calculate the change, it is not defined for the first day of the time series, and the target variables are not defined for dates within 7 days, as their 7 day futures do not exist yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it better to predict the smoothed quantity, or use the smoothed quantity to predict the jagged quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_size = 7\n",
    "folds = [X.date.unique()[fold_size*i:fold_size*(i+1)] for i in range(1 + len(X.date.unique())//fold_size)]\n",
    "folds_indices = [X[X.date.isin(fold)].index for fold in folds]\n",
    "folds_series = pd.Series(folds_indices).to_frame(name='folds')\n",
    "\n",
    "fold_indices = folds_series.values.flatten()[:-2]\n",
    "test_indices = folds_series.values.flatten()[-2:]\n",
    "test_indices = test_indices[0].append(test_indices[1]).sort_values()\n",
    "\n",
    "train_indices = fold_indices[0]\n",
    "for indices in fold_indices[1:]:\n",
    "    train_indices = full_index.union(indices)\n",
    "\n",
    "for i, epochs in enumerate(folds_series.values.flatten()):\n",
    "    start, end = model_data.loc[epochs, 'date'].min(), model_data.loc[epochs, 'date'].max()\n",
    "    print('Epoch {} spans the dates {} to {}'.format(i,start,end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = model_data.loc[train_indices, 'date'].min(), model_data.loc[train_indices, 'date'].max()\n",
    "print('Training data spans the dates {} to {}'.format(start,end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = model_data.loc[test_indices, 'date'].min(), model_data.loc[test_indices, 'date'].max()\n",
    "print('Testing data spans the dates {} to {}'.format(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date of predictions range from (initial date) + 7  to (final date) + 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly biased as true value increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning raised because the first prediction uses only 1 day's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, fit the PCA() transformer instance with the training data then apply it\n",
    "to both the training and holdout sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, fit Ridge regression model to the training data, and test it with the holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I'm messing up how to actually train test split; mike seems to indicate that for every prediction, the model is trained\n",
    "on all but one day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results vastly exceeded my expectations (with respect to the explained variance score). \n",
    "**I am making a large mistake of including the variables from the future like deaths, active, recovered, etc. This model is not predicting the future.**\n",
    "\n",
    "The predicted vs. the actual\n",
    "value has two key patterns which stand out; namely, the underestimation of very large values and the overestimation of intermediate values (from one-hundred and fifth to two-hundred thousand). I think this is due to the nature of the case number time series. Looking to a one dimensional sigmoid curve as a crude example. This function is motivated by simple epidemiological models as a crude approximation for expected behavior. My model seems to imply that the larger values indicate that the spread of COVID is past the inflection point present in the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the Ridge regression predictions sort of make sense as the asymptotic behavior of the sigmoid curve\n",
    "has not yet begun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Case number modeling with new features.\n",
    "<a id='modelplus'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Repeat the modeling steps above, applied to the dataset which calculated many different moving averages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I look at the effect of the number of PCA components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot of explained variance versus number of PCA components I believe it is fairly obvious that\n",
    "there is something wrong. I believe it's because I'm using different facets of the case number (deaths, active cases, number recovered) in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of explained variance score as a function of the features included; to see which features have the most effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not doing what I want. This is saying given I know the number of deaths/active cases/number recovered I can infer how many confirmed cases there were. This is not the same as predicting future numbers of cases. i.e. this is not plugging in a date (or days since first infection) and getting a number out.... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
