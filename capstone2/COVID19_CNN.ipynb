{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, make_scorer\n",
    "from tensorflow.keras.constraints import non_neg\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN COVID-19 case number forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"files/architecture2.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n",
    "\n",
    "![title](conv_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        ax1.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_naive+1))\n",
    "        ax2.plot(xrange, np.log(y_true+1)\n",
    "                 -np.log(y_predict+1))\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        ax1.plot(xrange, y_true-y_naive)\n",
    "        ax2.plot(xrange, y_true-y_predict)\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def flatten_Xy_splits(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat )\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict,n_countries, title='',suptitle=''):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    mse_train_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_train_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_train_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_train_naive, r2_predict))\n",
    "\n",
    "    true_predict_plot(y_true.ravel(), y_naive.ravel(), y_predict, title=title, suptitle=suptitle)\n",
    "    residual_diff_plots(y_true.ravel(), y_naive.ravel(), y_predict , n_days_into_future, n_countries)\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    Normalize with respect to some absolute max, just choose 2*absolute max of training set. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main format (shape) of the data and how it is being sliced and then reshaped is described in the notebook ```COVID19_model_prototypes.ipynb``` and so I will avoid a redundant discussion here. \n",
    "\n",
    "In regards to the feature data being considered, I am using the time dependent variables pertaining directly to COVID-19, such as new cases, new deaths, etc. as well as time dependent variables which quantify different governments' reactions to the pandemic; most notably is the usage of the oxford's \"stringency index\" which scores each goverments reaction with a number from 0 to 100.\n",
    "\n",
    "I am in fact not using the data on the number of tests, however, because the feature lacks a consistent set of units. In the regression this is counteracted upon by using one-hot encoding to categorize the units but instead of stratifying the new tests variable by units, I elect to simply drop the testing data, which may or may not be an unwise choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is data on 132 countries, and for 161 different days. There is not case information for all countries for all dates, this may be a source of error and one potential correction is to remove all \"frames\" which contain 0 information on the pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 132 different countries included in the data\n",
      "The time series span 161 different days\n"
     ]
    }
   ],
   "source": [
    "print('There are {} different countries included in the data'.format(data.location.nunique()))\n",
    "print('The time series span {} different days'.format(data.time_index.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full intersection of countries in fact included more information, but the countries with populations less than 1 million people were pruned from the dataset, in order to remove any chance of undue influence from very small states such as the Vatican or San Marino, for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of this notebook is to tune the hyperparameters and architecture of a simple one dimensional convolutional neural network such that it is able to make accurate predictions of the number of new cases. In pursuit of this goal, I elect to utilize the keras API for the deep learning implementation. The details or philosophy on how the model is constructed is also detailed in the model prototypes notebook, this notebook is simply devoted to tuning the model parameters; the architecture could also be played with (and has been) but I will leave any dramatically different architectures to different notebooks. The current architecture in this notebook represents the best (using the performance from the prototyping stage as a metric). Other inclusions which were previously tested were: more activation layers, pooling layers, dropout layers, non negative kernel constraints (the predicted value should be non-negative, there is a constraint for this). The architecture that is fixed in this notebook consists of two convolutional layers followed by two fully connected layers. In the first convolutional layer, the number of filters is chosen to be much larger than the filters in the second convolutional layer (but is also tuned). Likewise, the output of the first fully connected layer is much larger than the second (which has only 1 output in this setup). To ensure that all future tests are comparable, I always start with the same model coefficients by initializing the kernels of each layer of the network. This introduces even more parameters, using a normal distribution to initialize, such as the mean and standard deviation. This is not an issue however as in the final model these kernel initializers will be unincluded. The data is split such that the final holdout set contains a single day's information, the validation set contains 1 week or 7 days of information and the remainder is in the training set.\n",
    "\n",
    "As a reminder, the data format as well as the features being included are both described in the model prototyping notebook. The most obvious parameters with which to tune the model are the number of training epochs, the batch size of the training, the number of filters in each convolutional layer, the size of the convolutional kernel (i.e. convolutional window) and finally the number of outputs in the first dense layer. The number of convolutional filters are chosen such that the ratio is always the same between the first and second layer. Something to keep in mind as well is that the data I am training with contains only 20000 samples, and so the number of parameters should reflect this fact, by not being too numerous. \n",
    "\n",
    "The following values will be used for tuning purposes. \n",
    "\n",
    "    epochs = [50, 100, 200]\n",
    "    batch_size = [64, 256, 1024]\n",
    "    filter1 = [16,32,64]\n",
    "    filter2 = [2,4,8]\n",
    "    kernel1_size = [4,6,8]\n",
    "    kernel2_size = [4,6,8]\n",
    "    \n",
    "I elect to constrain the output of the first dense layer to be the same as the number of inputs to the first dense layer, which is in turn the number of outputs of the second convolutional layer. This dimension depends on the kernel size as well as filter number in said convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main difference between this and model prototyping\n",
    "\n",
    "The data being used, and the parameters being tuned; it was the architecture that was mainly tested previously in the\n",
    "prototyping notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.time_index>=40]\n",
    "model_data = data.copy().iloc[:, 3:].drop(columns=column_search(data,'new_test'))\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 28\n",
    "start_date = frame_size\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 132 159\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "scaled_splits, scaling_arrays =  normalize_Xy(splits, feature_range=(0,1.0), \n",
    "                                                                  normalization_method='minmax',\n",
    "                                                                  train_test_only=False,\n",
    "                                                                  feature_indices=None)\n",
    "# if need to supply folds for sklearn CV regression functions.\n",
    "(X_cnn_train, y_cnn_train, X_cnn_validate, y_cnn_validate, X_cnn_test, y_cnn_test) = scaled_splits\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAExCAYAAABPt7ftAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5ScVZ2v8edH05JAGC4hYC4EAkYhQEywBXK4HANGSWYkekQJ4JEBj5nx6MLj6HA5Lhnh6Bq8LGRYo8xCRUSYIIThkHWIA0F6hHGCpgORCU0wCdcmEiNIDJJA0u7zR1WHpqjurkrX2/VW1fNZK6v7vdRbu956u+qbvfe7d6SUkCRJUm3tVu8CSJIkNSNDliRJUgYMWZIkSRkwZEmSJGXAkCVJkpQBQ5YkSVIGKgpZEXF6RDweEesi4pIy20+JiIciYkdEnFmy7byIWFv8d16tCi5JkpRnMdQ4WRHRBvwamAP0ACuAs1NK3f32ORT4M+ALwJKU0uLi+v2BLqADSMBK4F0ppd/X+oVIkiTlye4V7HMcsC6l9ARARNwCzAd2hqyU0lPFbX8qeez7gWUppReL25cBpwOLBnqyAw44IB166KGVvwJJkqQ6Wbly5e9SSuPKbaskZE0Enu233AMcX+Fzl3vsxNKdImIhsBBg8uTJdHV1VXh4SZKk+omIpwfaVkmfrCizrtK5eCp6bErpupRSR0qpY9y4smFQkiSpoVQSsnqAg/stTwI2VHj84TxWkiSpYVUSslYAUyNiSkS8BVgALKnw+HcD74uI/SJiP+B9xXWSJElNbcg+WSmlHRHxGQrhqA24PqX0aERcAXSllJZExLuBO4D9gA9ExOUppaNSSi9GxP+hENQArujrBF+N7du309PTw7Zt26p9aEsaNWoUkyZNor29vd5FkSSpZQ05hMNI6+joSKUd35988kn23ntvxo4dS0S5bl7qk1LihRdeYMuWLUyZMqXexZEkqalFxMqUUke5bQ0x4vu2bdsMWBWKCMaOHWutnyRJddYQIQswYFXBcyVJUv01TMiSJElqJIasCr300kt85zvfqfpx8+bN46WXXsqgRJIkKc8MWRUaKGT19vYO+rilS5ey7777ZlUsSZKUU5VMq9OQlnVv5IG1mzh56jjmTDto2Me75JJLWL9+PTNmzKC9vZ0xY8Ywfvx4Vq1aRXd3Nx/84Ad59tln2bZtG5/97GdZuHAhAIceeihdXV28/PLLzJ07l5NOOon/+I//YOLEidx5552MHj162GWTJEn505Q1Wcu6N3Lhooe5cfnTXLjoYZZ1bxz2Ma+88koOP/xwVq1axTe+8Q1++ctf8tWvfpXu7sI82ddffz0rV66kq6uLa665hhdeeOFNx1i7di2f/vSnefTRR9l33325/fbbh10uSZKUT00Zsh5Yu4mt2wvNeFu39/LA2k01f47jjjvuDeNQXXPNNbzzne/khBNO4Nlnn2Xt2rVvesyUKVOYMWMGAO9617t46qmnal4uSZKUD00Zsk6eOo7R7W0AjG5v4+SptZ90eq+99tr5+7/9279x7733snz5cn71q18xc+bMsuNU7bHHHjt/b2trY8eOHTUvlyRJyoem7JM1Z9pBXHP2zJr2ydp7773ZsmVL2W2bN29mv/32Y88992TNmjU8+OCDw34+SZLU2JoyZEEhaNUiXPUZO3YsJ554IkcffTSjR4/moINeP/bpp5/OP/3TPzF9+nTe8Y53cMIJJ9TseSVJUmNqiLkLH3vsMY488sg6lagxec4kScpew89dKEmS1GgMWZIkSRkwZEmSJGXAkCVJkpQBQ5YkSVIGDFmSJEkZMGRlZMyYMQBs2LCBM888s+w+73nPeygdrqLU1VdfzSuvvFLz8kkqWNa9kcvuXF2TOU4lqT9DVsYmTJjA4sWLd/nxhiwpO1lMJi9JfZo3ZK1ZCnd9ofCzBi6++GK+853v7Fz+8pe/zOWXX85pp53GscceyzHHHMOdd975psc99dRTHH300QBs3bqVBQsWMH36dM466yy2bt26c79PfepTdHR0cNRRR/F3f/d3QGHS6Q0bNjB79mxmz54NwD333MOsWbM49thj+chHPsLLL79ck9cntaKRmExeUutqzpC1ZincfgGs+G7hZw2C1oIFC/jxj3+8c/nWW2/l/PPP54477uChhx6is7OTz3/+8ww2gv61117LnnvuySOPPMIXv/hFVq5cuXPbV7/6Vbq6unjkkUf42c9+xiOPPMKFF17IhAkT6OzspLOzk9/97nd85Stf4d577+Whhx6io6ODq666ativTWpVIzGZvKTW1ZxzF66/D7YXa4m2by0sHzFvWIecOXMmv/3tb9mwYQObNm1iv/32Y/z48Xzuc5/j/vvvZ7fdduO5555j48aNvPWtby17jPvvv58LL7wQgOnTpzN9+vSd22699Vauu+46duzYwW9+8xu6u7vfsB3gwQcfpLu7mxNPPBGA1157jVmzZg3rdUmtLIvJ5CWpT3OGrMNPhVU3FQJW++jCcg2ceeaZLF68mOeff54FCxZw8803s2nTJlauXEl7ezuHHnoo27ZtG/QYEfGmdU8++STf/OY3WbFiBfvttx9/+Zd/WfY4KSXmzJnDokWLavJ6JNV+MnlJ6tOczYVHzIMPXw/v/mTh5zBrsfosWLCAW265hcWLF3PmmWeyefNmDjzwQNrb2+ns7OTpp58e9PGnnHIKN998MwCrV6/mkUceAeAPf/gDe+21F/vssw8bN27kJz/5yc7H7L333mzZsgWAE044gZ///OesW7cOgFdeeYVf//rXNXltkiSptpqzJgsKwapG4arPUUcdxZYtW5g4cSLjx4/n3HPP5QMf+AAdHR3MmDGDI444YtDHf+pTn+L8889n+vTpzJgxg+OOOw6Ad77zncycOZOjjjqKww47bGdzIMDChQuZO3cu48ePp7OzkxtuuIGzzz6bV199FYCvfOUrvP3tb6/p65QkScMXg3XUroeOjo5UOnbUY489xpFHHlmnEjUmz5kkSdmLiJUppY5y25qzuVCSJKnODFmSJEkZMGRJkiRlwJAlSZKUAUOWJElqLDWeOi8rhixJktQ4Mpg6LyuGrAq99NJLb5gguhpXX301r7zySo1LJElSCyo3dV5OGbIqZMiSJCkHDj+1MGUe1HTqvCw07Yjvnc90snzDcmZNmMXsybOHfbxLLrmE9evXM2PGDObMmcOBBx7IrbfeyquvvsqHPvQhLr/8cv74xz/y0Y9+lJ6eHnp7e/nSl77Exo0b2bBhA7Nnz+aAAw6gs7OzBq9OkqQW1Td13vr7CgGrxrO71FJThqzOZzq56P6L2Na7jTvW3cHXT/n6sIPWlVdeyerVq1m1ahX33HMPixcv5pe//CUpJc444wzuv/9+Nm3axIQJE7jrrrsA2Lx5M/vssw9XXXUVnZ2dHHDAAbV4eZIktbYMps7LQlM2Fy7fsJxtvdsA2Na7jeUbltf0+Pfccw/33HMPM2fO5Nhjj2XNmjWsXbuWY445hnvvvZeLL76YBx54gH322aemzytJkhpHU9ZkzZowizvW3cG23m2MahvFrAmzanr8lBKXXnopf/VXf/WmbStXrmTp0qVceumlvO997+Oyyy6r6XNLkqTG0JQha/bk2Xz9lK/XtE/W3nvvzZYtWwB4//vfz5e+9CXOPfdcxowZw3PPPUd7ezs7duxg//3352Mf+xhjxozhhhtueMNjbS6UJKl1VBSyIuJ04B+ANuB7KaUrS7bvAdwIvAt4ATgrpfRURLQD3wOOLT7XjSmlv69h+Qc0e/LsmoSrPmPHjuXEE0/k6KOPZu7cuZxzzjnMmlWoIRszZgw33XQT69at42//9m/ZbbfdaG9v59prrwVg4cKFzJ07l/Hjx9vxXZKkFhEppcF3iGgDfg3MAXqAFcDZKaXufvv8T2B6SumvI2IB8KGU0lkRcQ5wRkppQUTsCXQD70kpPTXQ83V0dKSurq43rHvsscc48sgjd+kFtirPmSRJ2YuIlSmljnLbKun4fhywLqX0RErpNeAWYH7JPvOBHxZ/XwycFhEBJGCviNgdGA28BvxhF16DJElSQ6kkZE0Enu233FNcV3aflNIOYDMwlkLg+iPwG+AZ4JsppRdLnyAiFkZEV0R0bdq0qeoXIUmSlDeVhKwos660jXGgfY4DeoEJwBTg8xFx2Jt2TOm6lFJHSqlj3LhxZQsxVLOmXue5kiSp/ioJWT3Awf2WJwEbBtqn2DS4D/AicA7wryml7Sml3wI/B8q2Ww5m1KhRvPDCC4aHCqSUeOGFFxg1alS9iyJJ0shYsxTu+kLuJouu5O7CFcDUiJgCPAcsoBCe+lsCnAcsB84E7ksppYh4Bjg1Im4C9gROAK6utpCTJk2ip6cHmxIrM2rUKCZNmlTvYkiSlL01S+H2CwqTRa+6qTDlTk5Ggx8yZKWUdkTEZ4C7KQzhcH1K6dGIuALoSiktAb4P/Cgi1lGowVpQfPi3gR8Aqyk0Kf4gpfRItYVsb29nypQp1T5MkiQ1u/X3FQIWFH6uv69xQhZASmkpsLRk3WX9ft8GfKTM414ut16SJKkmDj+1UIO1fSu0jy4s50RTjvguSZJaxBHzCk2E6+8rBKyc1GKBIUuSJDW6I+blKlz1qeTuQkmSJFXJkCVJkvItp0M0DMWQJUmS8qtviIYV3y38bKCgZciSJEn5VW6IhgZhyJIkSfl1+KmFoRkgd0M0DMW7CyVJUn7leIiGoRiyJElSvuV0iIah2FwoSZKUAUOWJElSBgxZkiRJGTBkSZIkZcCO75IkqWl0PtPJ8g3LmTVhFrMnz65rWazJkiRJTaHzmU4uuv8iFj2+iIvuv4jOZzrrWh5DliRJagrLNyxnW+82ALb1bmP5huV1LY8hS5IkNYVZE2Yxqm0UAKPaRjFrwqy6lsc+WZIkqSnMnjybr5/y9dz0yTJkSZKkpjF78uy6h6s+hixJDW9Z90YeWLuJk6eOY860g+pdHEkC7JMlqcEt697IhYse5sblT3PhoodZ1r2x3kWSJMCQJanBPbB2E1u39wKwdXsvD6zdVOcSSVKBIUtSQzt56jhGt7cBMLq9jZOnjqtziSSpwD5ZkhranGkHcc3ZM+2TJSl3DFmSGt6caQcZriTljs2FkiRJGTBkSZIkZcDmQkkDcvwpSdp11mRJKsvxpyRpeAxZkspy/ClJGh5DlqSyHH9KkobHPlmSynL8KUkjZs1SWH8fHH4qHDGv3qWpGUOWpAE5/pSkzK1ZCrdfANu3wqqb4MPXF9Y3QegyZEkNzjsAJeVRxZ9N6+8rBCwo/Oz6ATz9wBtDV4MGLftkSQ2s1ncALuveyGV3rvZOwhrxfKpVVfXZdPip0D668Hvfz/6ha/192RY2Q4YsqYHV8g5Ah2yoLc+nWllVn01HzCvUVr37k4WfHee/MXQdfuoIlDgbhiypgdXyDkCHbKgtz6daWdWfTUfMgz//ZuFnaehq0KZCsE+W1NCGewdg/z4TJ08dx21dPWzd3uuQDTXg+VQrG/bdyX1hq8FFSqneZXiDjo6O1NXVVe9iSE2vrzmrLwRcc/ZMADvR15A3JUjNLyJWppQ6ym2zJktqUeWas66Yf3QuwkCzhBOHwJBaW0V9siLi9Ih4PCLWRcQlZbbvERE/Lm7/RUQc2m/b9IhYHhGPRsR/RsSo2hVf0q7K64judhiX1CyGrMmKiDbg28AcoAdYERFLUkrd/Xb7BPD7lNLbImIB8DXgrIjYHbgJ+O8ppV9FxFhge81fhdRCalXLk9cR3cvVsOWlbJJUjUpqso4D1qWUnkgpvQbcAswv2Wc+8MPi74uB0yIigPcBj6SUfgWQUnohpdRbm6JLrafWtTxzph2UmybCPnmtYZOkalUSsiYCz/Zb7imuK7tPSmkHsBkYC7wdSBFxd0Q8FBEXlXuCiFgYEV0R0bVpk7c5SwNphWEB+mrYPj7rEK45e2auAqAkVaOSju9RZl3pLYkD7bM7cBLwbuAV4KfFXvg/fcOOKV0HXAeFuwsrKJPUklplWAA7jEtqBpWErB7g4H7Lk4ANA+zTU+yHtQ/wYnH9z1JKvwOIiKXAscBPkVS1vPaj2hXNcgehJA2kkpC1ApgaEVOA54AFwDkl+ywBzgOWA2cC96WUUkTcDVwUEXsCrwH/FfhWrQovtaJmqOXpP0bXbV09NgtKrWbN0sKchIef2hSDjg5kyD5ZxT5WnwHuBh4Dbk0pPRoRV0TEGcXdvg+MjYh1wN8AlxQf+3vgKgpBbRXwUErprtq/DEmNpBX6lkkawJqlcPsFsOK7hZ9rlta7RJmpaDDSlNJSYGnJusv6/b4N+MgAj72JwjAOkgSU71tm86HUItbfB9u3Fn7fvrWw3KS1WY74LrWQvASZ0r5lgM2HUqs4/FRYdVMhYLWPLiw3KUOW1MT6hyrIV5Dp37fssjtXOwCp1CqOmAcfvr4l+mQZsqQmVdq5/ITD9q9rkBmsFq1VhqaQVHTEvKYOV30MWVKTKu1cDoUR1OsRZIa6m7BZhqbIS3OsVA9e/29myJKaVGnt0DnHH8I5xx9Slw/BSuYjbPShKRyWQq3M6788Q5bUpAaqHarHB18rNAc6sbVamdd/eYYsqYnlpXaoWZoDB1NJkLQ5Rc2qFf4jtSsipXxNFdjR0ZG6urrqXQxJqtpgIap/c8ro9jabU9R0WvU/EcU5mTvKbbMmS8q5Vv3gakSD1RzanKJml5ea8zwZclodSfXTV/tx4/KnuXDRwyzr3ljvImkXnTx1HKPb2wBsTpFahDVZUo7lrfbDWrVdV+t+ab4XUv4ZsqQcy1NnUm/RHr5aNaf4XkiNweZCKcf6aj8+PuuQun+RlqtVq5dl3Ru57M7VLdt8mqf3QtLADFlSzs2ZdhBXzD+67jUVeelTZD+1/LwXam2t/p+dSthcKKkipX2KoDCxcx5Hj292rTDumPLNJuvKWJMlqWJ9tWpA3WqT8laLU6//zeelhlOtySbryhiypJxphCr4en7A5qmfWqs0XTbCNamRlbf/7OSVzYVSjjRKFXy9p5DJctDDasqdp6bLrM53o1yTGlk2WVfGkCXlSJ6+tAcz1Adso34xV1vuvAyxkeX5bpRrUiPPEd6HZnOhlCONVAU/WJ+gvPXXqLS5q9py56XpMsvz3UjXpJQ31mRJOdIsVfB5qeGB6mp5dqXcefjffJbnu1muSakeIqVU7zK8QUdHR+rq6qp3MSQNU16mfbnsztXcuPzpncsfn3XIzjsky8lLuavVqOWWGl1ErEwpdZTdZsiS1Mz612SNbm9rmP5hUtNZsxTW3weHnwpHzKt3aWpmsJBlc6GkptaqzV1Z1mxZa9Y4St+r4bx3w3rf1yyF2y+A7Vth1U3w4eubKmgNxJosqcn4Bagsa++yrhms1/XbjH83pe/VBSdN4fp/f3KX3rthv+93fQFWfPf15Xd/Ev78m1W+onwarCbLuwulJlLPwTEdsDI/srzbMMtj1+v6HennHam/ldL36t7u53f5vRv2+374qdA+uvB7++jCcgswZEl1VssP3HoNndAqI5/XWlZftlkOu5Dlset1/Y7k847k30rpe/XeaW/d5fdu2O/7EfMKTYTv/mTLNBWCfbKkuqr1IJL1Gjoh6wErm70pp9YDiGbZDy3LY9fr+h3J5x3JwV3LvVczDt53l967mrzvR8xrmXDVxz5ZUh1VO7xAJeoRSBq5D1C9ZPHe10str7lm75PVrNdzK/PuQimnsvgfdD0Gx8yydiNv07rU6ss4TwO2Dketa+RKr9+RCj8j9XfTqne7tiprsqQ6a8amsFrK0//8a12Wat77vF4nWdbI5em9lwZiTZaUY3mYliXP8vQ//1rXqg313vcFq71Hte+89T5vE25nWSOXt1rMVpPXYN9IDFmSci8vQXQkm/j61+K0BfQWGx3yFjaasSO8sr0xo5UYsiSpQiNZq9a/Fqc3QdtuQe+fUi7DxnBC8GC1JfWsxWz1WhxrEWvDkCVJVRipWrXSWpwLTprClm3bm+pLv5LaknrUYlqLYy1irRiyJCmH8tQXLSt5rS3Ja7kqUasauFa4/kaCIUuScqL0CzIvfdGyktfakryWayhZD6eh6hmyJGkQ9RikslWaqPJaW5LXcg2lkWvgmpUhS5IGMJLBp1W/IPNaWzKS5SoN8rsa7Bu1Bq6ZGbIkaQAjGXzq+QXZ6nfS1VNpkL/gpCm7PCZao9bANbPdKtkpIk6PiMcjYl1EXFJm+x4R8ePi9l9ExKEl2ydHxMsR8YXaFFuSsnfy1HGMbm8DyDz49H1BfnzWISPaVNj3JX/j8qe5cNHDLOveWPPjX3bn6poftxbyULbSIH9v9/NvCvbVmDPtIK6Yf7QBKyeGDFkR0QZ8G5gLTAPOjohpJbt9Avh9SultwLeAr5Vs/xbwk+EXV2oOefhw19BGOvjU4wuyXG1drWQd4IYjL2UrDfLvnfbWEQv2yl4lzYXHAetSSk8ARMQtwHygu98+84EvF39fDPxjRERKKUXEB4EngD/WrNRSA2vFDs55Um3TWF77DNVKq06Lk5eylWvim3Hwvjb5NYlKQtZE4Nl+yz3A8QPtk1LaERGbgbERsRW4GJgDDNhUGBELgYUAkydPrrjwUiPKy4d7KzLgvlkjTYtTy75jwy1bLSf3Lg3yzR7sW0klISvKrEsV7nM58K2U0ssR5XYp7pjSdcB1AB0dHaXHlpqKdwDVjwG3vKy+1GsZ4LIYA2pXy1ZNWQz2ra2SkNUDHNxveRKwYYB9eiJid2Af4EUKNV5nRsTXgX2BP0XEtpTSPw675FKD8g6g+jHgVqaWNUa1CnBZBORdLVs1Zck62HtnaL5VErJWAFMjYgrwHLAAOKdknyXAecBy4EzgvpRSAk7u2yEivgy8bMCSbA6oFwPu0PJa81IuINcrYFRTliyDfV7fK71uyJBV7GP1GeBuoA24PqX0aERcAXSllJYA3wd+FBHrKNRgLciy0JK0q1o14FYaSPLapFoakIG6BYxqypJlsM/re6XXVTQYaUppKbC0ZN1l/X7fBnxkiGN8eRfKJzUkq/CVJ9XUeGTdpDqcv43+AfmyO1fXNWBUU5asgn0W75WfXbVV0WCkkiqXl/F3pD7VjIWV5dhgtfzbGMmBYmHwse3KlWUkxsKr9XvlZ1ftOa2OVGNW4Stvqq3xyKrmpZZ/GyPZv26omsB6NmUO973qX3PlZ1ftGbKkGvMONuVNXjr81/pvY6T611USPvLUlFmpcvMmjm5v87OrhgxZUo3l5QtN6i8PHf6z/tvIqj9RteGwUf6jVRoet2zb7mdXjUVhpIX86OjoSF1dXfUuhiRpF9Sr43T/WpnR7W1cc/ZMgJqVpdrXVcvzkNU5LXfODFbVi4iVKaWOstsMWdLweUeOVN8v7cvuXM2Ny5/euTz7HeN48IkXMwtdIyVv4VFvNljIsrlQGiYHBJQK6tlxurSJrq8MfT//+RdP7wxdjfR3WnpOa/068tCM3MwcwkEapkpujx+J27mlehvpYRX6Kx3O4JzjD3lDWYAh/07zqPScQmO+jlZlTZY0TEN1crWmS3mURTNRvW/6KK2VKR1WoX/zYV47o5cqNzxEI76OVmWfLKkGBvvCKu0r8vFZh3DF/KNHuojSTq3a4blZ+h81y+toFvbJkjI2WL+GRrmdu5n4JTS4Vh10sln6HzXL62gFhiwpY/VuQmk1Ns8OzeAvjQxDljQC/J/nyGnVWppqGPylkWHIktRUrKWpjMFfyp4hS9oF9vnJL2tpJOWFIUuqkn1+8s9aGkl54GCkUpUqGXxUkiRDllSleo5qLUlqHDYXSlWyz48kqRKGLGkX2OdHkjQUmwslSZIyYMiSJEnKgCFLkiQpA4YsSZKkDBiyJEmSMmDIkiRJyoBDOEgVcK5CSVK1rMmShtA3V+GNy5/mwkUPs6x7Y72LJElqAIYsaQjOVShJ2hWGLGkIzlUoSdoV9smShuBchZKkXWHIkirgXIWSpGrZXChJkpQBQ5YkSVIGDFmSJEkZMGRJkiRlwJAlSZKUAUOWJElSBgxZkiRJGagoZEXE6RHxeESsi4hLymzfIyJ+XNz+i4g4tLh+TkSsjIj/LP48tbbFl2pnWfdGLrtztXMTSpJqYsiQFRFtwLeBucA04OyImFay2yeA36eU3gZ8C/hacf3vgA+klI4BzgN+VKuCS7XkJNCSpFqrpCbrOGBdSumJlNJrwC3A/JJ95gM/LP6+GDgtIiKl9HBKaUNx/aPAqIjYoxYFl2rJSaAlSbVWSciaCDzbb7mnuK7sPimlHcBmYGzJPh8GHk4pvVr6BBGxMCK6IqJr0ya/3DTynARaklRrlcxdGGXWpWr2iYijKDQhvq/cE6SUrgOuA+jo6Cg9tpQ5J4GWJNVaJSGrBzi43/IkYMMA+/RExO7APsCLABExCbgD+HhKaf2wSyxlxEmgJUm1VElz4QpgakRMiYi3AAuAJSX7LKHQsR3gTOC+lFKKiH2Bu4BLU0o/r1WhJUmS8m7IkFXsY/UZ4G7gMeDWlNKjEXFFRJxR3O37wNiIWAf8DdA3zMNngLcBX4qIVcV/B9b8VUiSJOVMpJSvLlAdHR2pq6ur3sWQJEkaUkSsTCl1lNvmiO+SJEkZMGRJkiRloJK7C6WWtKx7o0M6SJJ2mSFLLWuwENU3zc7W7b3c1tXDNWfPNGhJkqpic6Fa0lBzFTrNjiRpuAxZaklDhSin2ZEkDZfNhWpJJ08dx21dPWzd3ls2RDnNjiRpuBwnSy3Lju2SpOEabJwsa7LUspyrUJKUJUOWWoY1V5KkkWTHd7WEoe4mlCSp1gxZagkOySBJGmmGLLUEh2SQJI00+2SpJTgkgyRppBmy1DK8m1CSNJJsLpQkScqAIUuSJCkDNheqaTkuliSpnqzJUlNyXCxJUr1Zk6Wm0b/mqty4WNZmSZJGkiFLTaGv5mrr9l5u6+rhgpOmMLq9ja3bex0XS5JUF4YsNYXSmqst27Y7LpYkqa4MWWoKJ08dx21dPW+ouXJcLElSPRmy1BQc0V2SlDeGLDUNa64kSXniEA6SJEkZMGRJkiRlwJAlSZKUAftkqSQmzXwAAAjOSURBVKE5dY4kKa+syVLDcuocSVKeGbLUsMpNnSNJUl4YstSwTp46jtHtbQBOnSNJyh37ZKlhOQCpJCnPDFlqKKUd3R2AVJKUVzYXqmHY0V2S1EgMWWoYdnSXJDUSQ5Yahh3dJUmNxD5ZyrXSPlh2dJckNQpDluqqNET1Xwa4cNHDbN3ey21dPVxz9kw7ukuSGoYhSyNqsBB1wUlTuP7fn9y5fMJh+7+pD5YBS5LUKCoKWRFxOvAPQBvwvZTSlSXb9wBuBN4FvACclVJ6qrjtUuATQC9wYUrp7pqVXnUxWO3TYMt7j2ofNETd2/38G5ah0Pdq6/Ze+2BJkhrOkCErItqAbwNzgB5gRUQsSSl199vtE8DvU0pvi4gFwNeAsyJiGrAAOAqYANwbEW9PKfXW+oVUY6gmql0JEJUsN8qxBzsWDF77NNhyW0BvKrwH5ULUe6e9lWdefHLn8jnHH8I5xx9iHyxJUkOKlNLgO0TMAr6cUnp/cflSgJTS3/fb5+7iPssjYnfgeWAccEn/ffvvN9DzdXR0pK6urmG9qMH0jbXU90XePwS8pa1ws+VrvX9607bhLjfKsYc61gmH7U/n468PnfCOg8bw+MaXK15u2y3o/VNidHsb15w9Exg8HEqSlGcRsTKl1FFuWyXNhROBZ/st9wDHD7RPSmlHRGwGxhbXP1jy2IllCrgQWAgwefLkCoq060rHWurfRPVa75927leu+Wo4y41y7KGOBYPXPg21fMFJU9iybfsbQlT/MGXHdklSs6gkZEWZdaXVXwPtU8ljSSldB1wHhZqsCsq0y06eOo7bunrKhoDSWpxqA8Rgy41y7KGOVa4Jb8bB+1a1LElSK6gkZPUAB/dbngRsGGCfnmJz4T7AixU+dkSVG2upfwgAhhUgBltulGMPday+89j/nFazLElSK6ikT9buwK+B04DngBXAOSmlR/vt82ngmJTSXxc7vv+3lNJHI+Io4J+B4yh0fP8pMHWwju9Z98mSJEmqlWH1ySr2sfoMcDeFIRyuTyk9GhFXAF0ppSXA94EfRcQ6CjVYC4qPfTQibgW6gR3Ap+t9Z6EkSdJIGLIma6RZkyVJkhrFYDVZThAtSZKUAUOWJElSBgxZkiRJGTBkSZIkZcCQJUmSlAFDliRJUgYMWZIkSRkwZEmSJGXAkCVJkpQBQ5YkSVIGDFmSJEkZMGRJkiRlwJAlSZKUAUOWJElSBgxZkiRJGTBkSZIkZcCQJUmSlAFDliRJUgYMWZIkSRkwZEmSJGXAkCVJkpSBSCnVuwxvEBGbgKdH4KkOAH43As/TLDxf1fOcVc9zVh3PV/U8Z9XxfA3tkJTSuHIbcheyRkpEdKWUOupdjkbh+aqe56x6nrPqeL6q5zmrjudreGwulCRJyoAhS5IkKQOtHLKuq3cBGoznq3qes+p5zqrj+aqe56w6nq9haNk+WZIkSVlq5ZosSZKkzBiyJEmSMtByISsiTo+IxyNiXURcUu/y5FFEHBwRnRHxWEQ8GhGfLa7fPyKWRcTa4s/96l3WPImItoh4OCL+X3F5SkT8oni+fhwRb6l3GfMkIvaNiMURsaZ4rc3yGhtcRHyu+De5OiIWRcQor7PXRcT1EfHbiFjdb13ZayoKril+FzwSEcfWr+T1M8A5+0bx7/KRiLgjIvbtt+3S4jl7PCLeX59SN46WClkR0QZ8G5gLTAPOjohp9S1VLu0APp9SOhI4Afh08TxdAvw0pTQV+GlxWa/7LPBYv+WvAd8qnq/fA5+oS6ny6x+Af00pHQG8k8K58xobQERMBC4EOlJKRwNtwAK8zvq7ATi9ZN1A19RcYGrx30Lg2hEqY97cwJvP2TLg6JTSdODXwKUAxe+BBcBRxcd8p/i9qgG0VMgCjgPWpZSeSCm9BtwCzK9zmXInpfSblNJDxd+3UPjym0jhXP2wuNsPgQ/Wp4T5ExGTgD8HvldcDuBUYHFxF89XPxHxZ8ApwPcBUkqvpZRewmtsKLsDoyNid2BP4Dd4ne2UUrofeLFk9UDX1HzgxlTwILBvRIwfmZLmR7lzllK6J6W0o7j4IDCp+Pt84JaU0qsppSeBdRS+VzWAVgtZE4Fn+y33FNdpABFxKDAT+AVwUErpN1AIYsCB9StZ7lwNXAT8qbg8Fnip3weV19obHQZsAn5QbGL9XkTshdfYgFJKzwHfBJ6hEK42AyvxOhvKQNeU3weVuQD4SfF3z1mVWi1kRZl1jmExgIgYA9wO/K+U0h/qXZ68ioi/AH6bUlrZf3WZXb3WXrc7cCxwbUppJvBHbBocVLEv0XxgCjAB2ItCk1cpr7PK+Dc6hIj4IoXuIzf3rSqzm+dsEK0WsnqAg/stTwI21KksuRYR7RQC1s0ppX8prt7YV51e/PnbepUvZ04EzoiIpyg0QZ9KoWZr32KzDnitleoBelJKvyguL6YQurzGBvZe4MmU0qaU0nbgX4D/gtfZUAa6pvw+GEREnAf8BXBuen1ATc9ZlVotZK0AphbvxnkLhQ58S+pcptwp9if6PvBYSumqfpuWAOcVfz8PuHOky5ZHKaVLU0qTUkqHUrim7kspnQt0AmcWd/N89ZNSeh54NiLeUVx1GtCN19hgngFOiIg9i3+jfefM62xwA11TS4CPF+8yPAHY3Nes2Ooi4nTgYuCMlNIr/TYtARZExB4RMYXCTQO/rEcZG0XLjfgeEfMo1DK0AdenlL5a5yLlTkScBDwA/Cev9zH63xT6Zd0KTKbwgf+RlFJpJ9OWFhHvAb6QUvqLiDiMQs3W/sDDwMdSSq/Ws3x5EhEzKNwo8BbgCeB8Cv/x8xobQERcDpxFoQnnYeB/UOgT43UGRMQi4D3AAcBG4O+A/0uZa6oYVP+Rwl1yrwDnp5S66lHuehrgnF0K7AG8UNztwZTSXxf3/yKFflo7KHQl+UnpMfW6lgtZkiRJI6HVmgslSZJGhCFLkiQpA4YsSZKkDBiyJEmSMmDIkiRJyoAhS5IkKQOGLEmSpAz8f9z+fnWcrQYlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "# ax.scatter(range(len(X)), X[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=30,color='k')\n",
    "ax.scatter(range(len(X_cnn_train)), X_cnn_train[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='train')\n",
    "ax.scatter(range(len(X_cnn_train), len(X_cnn_train)+len(X_cnn_validate)), X_cnn_validate[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='validate')\n",
    "ax.scatter(range(len(X_cnn_train)+len(X_cnn_validate), len(X)), X_cnn_test[:,np.where(data.location.unique()=='United States')[0],-1,new_cases_index], s=10, label='test')\n",
    "plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_3d = concatenate_4d_into_3d(scaled_splits)\n",
    "(X_cnn_train_model, y_cnn_train_model, X_cnn_validate_model,\n",
    " y_cnn_validate_model, X_cnn_test_model, y_cnn_test_model) = splits_3d\n",
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list = [50, 100, 200]\n",
    "batch_size_list = [64, 256, 1024]\n",
    "filter1_list = [16,32,64]\n",
    "filter2_list = [2,4,8]\n",
    "kernel1_size_list = [4,6,8]\n",
    "kernel2_size_list = [4,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations = list(itertools.product(epochs_list, batch_size_list, filter1_list,\n",
    "                                                filter2_list, kernel1_size_list, kernel2_size_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to both save the results as well as ensure that the run of this notebook can be broken up into sessions, create a DataFrame which logs the scores and saves them to a .csv file. During the exploration of hyperparameters, this csv will be overwritten with the inclusion of any new scores calculated. This will allow for comparison and analysis of the hyper parameter space, as well as ensure that we do not have to fit the model 729 times per run of this notebook. The scores that are saved are the ones generated by the validation set predictions. Also included for comparison purposes are the scores resulting from the naive baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('cnn_score_logging.csv'):\n",
    "    score_logging_df = pd.read_csv('cnn_score_logging.csv', index_col=0)\n",
    "else:\n",
    "    score_logging_df = pd.DataFrame(np.array(parameter_combinations), \n",
    "                                    columns=['epochs','batch_size','filter_1','filter_2','kernel_1','kernel_2'])\n",
    "    score_logging_df.loc[:, 'mean_squared_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_absolute_error'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_explained_variance'] = np.nan\n",
    "    score_logging_df.loc[:, 'naive_mean_squared_error'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "kernel0 = RandomNormal(seed=0)\n",
    "kernel1 = RandomNormal(seed=1)\n",
    "kernel2 = RandomNormal(seed=2)\n",
    "kernel3 = RandomNormal(seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>filter_1</th>\n",
       "      <th>filter_2</th>\n",
       "      <th>kernel_1</th>\n",
       "      <th>kernel_2</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>naive_mean_absolute_error</th>\n",
       "      <th>naive_explained_variance</th>\n",
       "      <th>naive_mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>278.091060</td>\n",
       "      <td>7.199402</td>\n",
       "      <td>0.920333</td>\n",
       "      <td>6.38194</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>283.717052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>200</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>100</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>50</td>\n",
       "      <td>1024</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>296.195258</td>\n",
       "      <td>8.795685</td>\n",
       "      <td>0.920441</td>\n",
       "      <td>6.38194</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>283.717052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch_size  filter_1  filter_2  kernel_1  kernel_2  \\\n",
       "253     100          64        16         4         4         6   \n",
       "597     200         256        32         2         6         4   \n",
       "430     100        1024        16         8         8         6   \n",
       "230      50        1024        64         4         6         8   \n",
       "519     200          64        32         2         8         4   \n",
       "\n",
       "     mean_squared_error  mean_absolute_error  explained_variance  \\\n",
       "253          278.091060             7.199402            0.920333   \n",
       "597                 NaN                  NaN                 NaN   \n",
       "430                 NaN                  NaN                 NaN   \n",
       "230          296.195258             8.795685            0.920441   \n",
       "519                 NaN                  NaN                 NaN   \n",
       "\n",
       "     naive_mean_absolute_error  naive_explained_variance  \\\n",
       "253                    6.38194                  0.915816   \n",
       "597                        NaN                       NaN   \n",
       "430                        NaN                       NaN   \n",
       "230                    6.38194                  0.915816   \n",
       "519                        NaN                       NaN   \n",
       "\n",
       "     naive_mean_squared_error  \n",
       "253                283.717052  \n",
       "597                       NaN  \n",
       "430                       NaN  \n",
       "230                283.717052  \n",
       "519                       NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_logging_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################### 50 runs completed\n",
      "################################################## 100 runs completed\n",
      "################################################## 150 runs completed\n",
      "################################################## 200 runs completed\n",
      "################################################## 250 runs completed\n",
      "################################################## 300 runs completed\n",
      "################################################## 350 runs completed\n",
      "################################################## 400 runs completed\n",
      "#########################"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  elif not isinstance(value, collections.Sized):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### 450 runs completed\n",
      "#####"
     ]
    }
   ],
   "source": [
    "for i, hyper_parameters in enumerate(parameter_combinations):\n",
    "    \n",
    "    (epochs, batch_size, filter1, \n",
    "     filter2, kernel1_size, kernel2_size) = hyper_parameters\n",
    "    \n",
    "    if score_logging_df.isna().loc[i,'mean_squared_error']:\n",
    "        cnn = Sequential()\n",
    "        cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                         padding='valid',\n",
    "                         input_shape=X_cnn_train.shape[2:],\n",
    "                         use_bias=False,\n",
    "                        kernel_initializer=kernel0\n",
    "                        )\n",
    "                 )\n",
    "        cnn.add(Conv1D(filters=int(filter2), \n",
    "                         kernel_size=int(kernel2_size), \n",
    "                         padding='valid',\n",
    "                         use_bias=False,\n",
    "                        kernel_initializer=kernel1\n",
    "                        )\n",
    "                 )\n",
    "        cnn.add(Flatten())\n",
    "        cnn.add(Dense(cnn.output.shape[1], \n",
    "                      kernel_initializer=kernel2\n",
    "                       )\n",
    "                 )\n",
    "        cnn.add(Dense(1, \n",
    "                        activation='relu',\n",
    "                      kernel_initializer=kernel3\n",
    "                           ))\n",
    "        cnn.compile(loss='mse', optimizer=Adam())\n",
    "        history = cnn.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "                  batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        y_true = y_cnn_validate_model.ravel()\n",
    "        y_predict = cnn.predict(X_cnn_validate_model).ravel()\n",
    "        y_naive = y_validate_naive.ravel()\n",
    "\n",
    "        score_logging_df.loc[i,'naive_mean_squared_error'] = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'mean_squared_error']  = mean_squared_error(y_true.ravel(), y_predict)\n",
    "        score_logging_df.loc[i,'naive_explained_variance']  = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'explained_variance']  = explained_variance_score(y_true.ravel(), y_predict)\n",
    "        score_logging_df.loc[i,'naive_mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_naive.ravel())\n",
    "        score_logging_df.loc[i,'mean_absolute_error']  = mean_absolute_error(y_true.ravel(), y_predict)\n",
    "        # every time a new score is calculated, overwrite the original file, to save space but also save progress scoring.\n",
    "        score_logging_df.to_csv('cnn_score_logging.csv')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print('#',end='')\n",
    "    if (i % 50 == 0) & (i>0):\n",
    "          print(' {} runs completed'.format(str(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_parameters = score_logging_df.loc[score_logging_df.mean_squared_error.idxmin(),:].iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(epochs, batch_size, filter1, \n",
    " filter2, kernel1_size, kernel2_size) = best_model_parameters\n",
    "\n",
    "best_cnn = Sequential()\n",
    "best_cnn.add(Conv1D(filters=int(filter1), kernel_size=int(kernel1_size),\n",
    "                 padding='valid',\n",
    "                 input_shape=X_cnn_train.shape[2:],\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel0\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Conv1D(filters=int(filter2), \n",
    "                 kernel_size=int(kernel2_size), \n",
    "                 padding='valid',\n",
    "                 use_bias=False,\n",
    "                kernel_initializer=kernel1\n",
    "                )\n",
    "         )\n",
    "best_cnn.add(Flatten())\n",
    "best_cnn.add(Dense(best_cnn.output.shape[1], \n",
    "              kernel_initializer=kernel2\n",
    "               )\n",
    "         )\n",
    "best_cnn.add(Dense(1, \n",
    "                activation='relu',\n",
    "              kernel_initializer=kernel3\n",
    "                   ))\n",
    "best_cnn.compile(loss='mse', optimizer=Adam())\n",
    "history = best_cnn.fit(X_cnn_train_model, y_cnn_train_model, epochs=epochs, validation_data=(X_cnn_validate_model, y_cnn_validate_model), \n",
    "          batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we are not overtraining the CNN, let's look at the plots of validation loss and training loss as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'][-100:], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'][-100:], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overtraining would be signalled by the validation loss increasing while the training loss decreases; there does not seem to be substantial evidence for this so we can move onto the prediction results. As a \"sanity check\", predict the outcomes of the training data, to see whether or not the predictions perform better than the naive baseline; if not, then either something is wrong or the model is not worth pursuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_train_model.ravel()\n",
    "y_predict = cnn.predict(X_cnn_train_model).ravel()\n",
    "model_analysis(y_true, y_train_naive, y_predict, n_countries, title='CNN model', suptitle='Performance on training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check the efficacy of the model by predicting the validation data. Note that this is different from the \"testing\" set, or hold-out set, which is used for the final check of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_cnn_validate_model.ravel()\n",
    "y_predict = cnn.predict(X_cnn_validate_model).ravel()\n",
    "model_analysis(y_true, y_validate_naive, y_predict, n_countries, title='CNN model', suptitle='Performance on validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5),sharex=True)\n",
    "(ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "_ = ax1.hist(best_cnn.get_weights()[0].ravel())\n",
    "_ = ax2.hist(best_cnn.get_weights()[1].ravel())\n",
    "_ = ax3.hist(best_cnn.get_weights()[2].ravel())\n",
    "_ = ax4.hist(best_cnn.get_weights()[3].ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "future work\n",
    "\n",
    "different baseline\n",
    "more than 1-day step predictions. \n",
    "different time-frame or entire data set usages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over possible leading window dates (window right edge, not inclusive),\n",
    "a 4-d tensor with dimensions given by the following is created:\n",
    "    \n",
    "```(n_windows, n_countries, n_time_steps, n_features)```\n",
    "\n",
    "Even if I don't use the input of this form, it makes it much easier to slice into train, validate and test, by slicing along\n",
    "the ```n_windows``` axis. \n",
    "\n",
    "Note that if I also only wanted to include data from after the first case, that is possible by slicing data.time_index >= 1 \n",
    "\n",
    "Putting this all together:\n",
    "\n",
    "\n",
    "Now, splitting X intro train, validate, test, is as easy as slicing the first axis. Depending on how the CNN is set up, this\n",
    "axis can later be flattened by simply applying ```np.concatenate(X_train, axis=0)```\n",
    "\n",
    "Now, for the target data ```y```. To be as general as possible, I note that I should first shift the time series and combine them to store all of the data, and THEN manipulate it, i.e.\n",
    "\n",
    "Assuming that ```X``` is of the shape ```(n_windows, n_countries, n_time_steps, n_features)```, the target data ```y``` should\n",
    "be of shape ```(n_windows, n_countries)``` and the values for each slice of the first axis is then of shape ```(n_countries,)```, with the values equaling the ```model_data.n_cases_weighted``` value for ```time_index == window_right_edge_inclusive + n```, where n is the prediction step size. \n",
    "\n",
    "Because we need future values with which to measure error, the maximum that the leading window edge can be is always \n",
    "\n",
    "```model_data.time_index.max() - n```\n",
    "\n",
    "In other words, because the maximum date value is\n",
    "\n",
    "If I want to predict (and measure error) for 7 days in the future, the last window would have leading edge \n",
    "```model_data.time_index == 123```. \n",
    "\n",
    "For now, just test the waters with a 1-day prediction.\n",
    "To do so, first split the X and y data into train, validate, test.\n",
    "For the first attempt at a model, use Conv1D only, have to flatten the data with concatenations.\n",
    "Before this, however, I want to rescale the data. Now, because each different element in the first axis is a different time range, the correct action is to (if normalizing) take the mean with respect to axis=1 and axis=2 ```n_countries``` and  ```n_time_steps```. This leaves a total number of averages of shape ```(n_windows, n_features)```, i.e. the mean and standard deviations for each feature for each date range. This ensures that no data snooping has occurred. To actually subtract and divide by these values, need to reform the same shape array (or at least that seems to be the easiest method to me.\n",
    "\n",
    "Just to check, the slice [0, :, :, 0] should only contain one value.\n",
    "\n",
    "Need to normalize only using the values up until frame; in fact, I only use the values inside each frame to normalize.\n",
    "In the case of normalizing the validation and testing sets, use the most recent frame's means and standard deviation to normalize. This requires forming the correctly shaped array which is performed using np.tile()\n",
    "\n",
    "Unfortunately there are values (actually a single value) where the standard deviation equals 0. Luckily, this\n",
    "can be circumvented because the value of $X-\\bar{X}$ is zero in the same place; i.e. I set $\\sigma=1$ at that location, because\n",
    "it doesn't actually affect the value.\n",
    "\n",
    "Notes on results\n",
    "\n",
    "    1. kernel small->large seems to work ok.\n",
    "    2. filters\n",
    "    3. pooling seems to hurt.\n",
    "\n",
    "Create a 5-d tensor for convolution and FC layer parameters using itertools.\n",
    "The layers of the CNN will remain the same throughout testing for now. That is, \n",
    "\n",
    "    1. Conv1D(f1, k1)\n",
    "    2. Conv1D(f2, k2)\n",
    "    3. Flatten\n",
    "    4. Dense(d)\n",
    "    5. Dense(1)) \n",
    "\n",
    "I don't know enough yet, but my intuition tells me that the overall trend (macroscopic picture)\n",
    "is much more important to capture than microscopic. \n",
    "   \n",
    "    f1 : number of filters,  first convolutional layer\n",
    "    k1 : kernel size, first convolutional layer\n",
    "    f1 : number of filters,  second convolutional layer\n",
    "    k1 : kernel size, second convolutional layer\n",
    "    d : number of nodes in hidden fully connected layer\n",
    "    \n",
    "\n",
    "I believe the naive baseline is performing so much better simply because of the growth in number of new cases; i.e. the model is training on smaller values and so in every prediction its underestimating. It's also predicting nearly the same values for each separate validation date, likely indicative of overfitting. \n",
    "\n",
    "Changing the activations and the layers seems to work much better than the number of filters and kernel size.\n",
    "\n",
    "ReLU activation on both cd layers somehow increases the values, i.e. away from zero? how does that make sense.\n",
    "Predicting non-negative quantity -> ReLU at eachy layer, otherwise model will try to use negative values which never do anything?\n",
    "\n",
    "On the surface this performs worse but doesn't look like it's making ***terrible*** predictions. I'll show that I believe\n",
    "that the model is indeed over-fitting. \n",
    "\n",
    "Relation between X slicing and the original data: \n",
    "\n",
    "    X[-1, :, -1, 2] = model_data.time_index.max() - n_days_into_future \n",
    "    X[-t, :, -1, 2] = (model_data.time_index.max() - n_days_into_future + 1) - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will discuss the data format which I have found the most useful. The motivation behind this formatting is that is has to be compatible with keras' convolution layers, respect ordering in time and hopefully be easy to use.\n",
    "\n",
    "A \"sample\" or single input of the convolutional model is to be a single time window which I shall refer to as a frame. The output of the model will be a single day's worth of predictions, namely, the next day's number of cases, (with some experimentation into the forecasting range to be performed later). The Conv1D layers require that each sample be of two dimensional input ```(n_time_steps, n_features)```, but the training / testing sets themselves are passed as 3-D tensors of shape ```(all_train_or_test_frames, n_time_steps, n_features)```. In order to partition time in an easy manner, it is easier to first create a 4-D tensor, whose (first) axis can be thought of as indexing the leading edge of the time frames. Therefore, slicing into training, validation and testing, is a well ordered operation simply performed by slicing the tensor along this axis. (picture) ```(n_frames, n_countries, n_time_steps, n_features)```. It is also relatively straight forward how to manipulate the target variables and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape) \n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = (n_validation_frames + n_test_frames)\n",
    "# new_cases_weighted_index=0\n",
    "# ((np.exp(X[-t, :, -1, new_cases_weighted_index])-1) - data[data.time_index == ((data.time_index.max() - n_days_into_future + 1) - t)].new_cases_weighted).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so to test the baseline, the predictions are of course the date on the RHS + n_days_into_future, or \n",
    "\n",
    "This proves the case where n_validate_frames == 1. Now try it for 7. This means that the \"first\" validation frame is t==8\n",
    "\n",
    "Ok. Now to see if the baseline is actually what I think it is. There could be an issue into how I am calculating the loss.\n",
    "This is calculating the loss of all batch frames simulataneously...? Each frame for each country makes a single prediction.\n",
    "Should I be trying to group it only by frame though? I.e. 2-d input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_naive = X[-n_test_frames-1, :, -1,  new_cases_weighted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y - create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries)[1][-y.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the relation between X and y is upheld (because of for loop?) they are not changing between n_days_into_future_changes. \n",
    "\n",
    "Emulate the last iteration of for loop.\n",
    "\n",
    "What should be going on. For n_days_into_future = 7, X should be slicing data from time <= 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the data is reshaped to be an iterable of sequences of length max_date_in_window, which is then truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For y. What should be happening? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is made such that its maximum date is  ```model_data.time_index.max() - n_days_into_future```. So by the very creation of\n",
    "X, X and y are already staggered if y is sliced correctly. Which would be ```start_date + n_days_into_future : ```, or simply ```-X.shape[0]:```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test is the last frame. meaning that it currently contains t-14 information. y_test is the very last day, n_cases_per_million\n",
    "un-normalized, so the naive model would be simply to use y_predict = X_test_original[:, -1, 2], this takes the values of the feature, new_cases_per_million, on the last day of the frame, for all countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeperableConv2D model. I believe that time steps should be channels and then countries should be the rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, my first guess at the parameters was very close to the optimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The top 10 sets of parameters (k1,k2,f1,f2,d) are:\\n')\n",
    "\n",
    "# mae_rankings = pd.DataFrame(np.concatenate((parameter_grid[mae_list.argsort()],mae_list[mae_list.argsort()].reshape(-1,1)),axis=1), \n",
    "#              columns=['kernel_layer_1','kernel_layer_2','filter_layer_1','filter_layer_2','MAE'])\n",
    "# mae_rankings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae_rankings.to_csv('conv_parameter_rankings.csv')\n",
    "\n",
    "# best_param_predictions = predictions_list[mae_list.argsort()][0]\n",
    "\n",
    "# pd.DataFrame(predictions_list[mae_list.argsort()]).to_csv('model_param_grid_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('conv_parameter_rankings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because ```n_days_into_future == 1```, The most recent date in the last frame of X is data.time_index.max()-1.\n",
    "The maximum date in y is data.time_index().max(). Therefore, to convert, we have\n",
    "    \n",
    "    X[-1,:,-1,:] = y[-2, :]\n",
    "    \n",
    "or, more generally, \n",
    "\n",
    "    X[-t,:,-1,:] = y[-(n+t), :]\n",
    "    \n",
    "testing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_day_in_X = -1\n",
    "# if last_day_in_X == -1:\n",
    "#     last_day_in_X = None\n",
    "# X_test_original = X[-16:last_day_in_X, :, -1, new_cases_weighted_index] \n",
    "# if last_day_in_X is None:\n",
    "#     last_day_in_X = 0\n",
    "\n",
    "# X_test_original - y[-(n_test_frames+n_days_into_future+X_test_original.shape[0]+last_day_in_X-1):\n",
    "#                     -(n_days_into_future+last_day_in_X), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the staggering has been done correctly. That is, each entry along the first axis of X predicts.\n",
    "Need to make sure that the naive predictions are being chosen correctly. To do so, just look at the instance\n",
    "where n_frames_validation == 1 and n_frames_test == 1 (to make slicing easier) for n_days_into_future == 7.\n",
    "\n",
    "Specifically, the shifting and comparison between y_validate and X_train isn't so hard as it follows the above formula.\n",
    "This is prior to concatenation / squeezing / flattening the first axis.\n",
    "\n",
    "    The following holds but this is not the predictions relation:   X[-t,:,-1,:] = y[-(n+t), :]\n",
    "                                                  \n",
    "    y has already been shifted to account for the n_days_into_future prediction. Therefore,     \n",
    "    True future new_cases_weighted values : y[-t,:]. This is the n_days_into_future values.\n",
    "    \n",
    "    CNN predictions new_cases_weighted : model.predict(X[-t, :, :, :]). This predicts using the \"present day\" frame. \n",
    "    \n",
    "    Naive predictions new_cases_weighted values: X[-t,:,-1, 2]. This takes the present day value from the last values of the frames.\n",
    "\n",
    "Therefore, when n_validation_frames > 1.  are simply\n",
    "\n",
    "    Naive predictions = X[validation_indices, :, -1, 2]\n",
    "    CNN predictions = model.predict(X[validation_indices, :, :, :])\n",
    "    True future values = y[validation_indices, :]\n",
    "    \n",
    "    Where validation indices is assumed to be a correctly formatted array of negative indices. \n",
    "    \n",
    "    \n",
    "    The last issue is the fact that X_validate is normalized and y values are not. Therefore, for the prediction X_validate is used, but for the naive baseline either the inverse normalized X_validation or just the original X should be used. Original\n",
    "    X is actually easier because of 4-d index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train and \"test\" without using the holdout test set. i.e. just use train and validation sets to test\n",
    "This uses a single frame to validate (and test). Therefore, the naive model is formed by taking the frame of\n",
    "X_validate and using its values for the prediction. The true values are stored in y_validate, the predicted values\n",
    "are formed by using X_validate to predict, the naive values are the values in X_validate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when slicing, the n_days_into_future buffer has already been factored into account. i.e. test\n",
    "slicing is X[-n_test_frames:,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X time_index 130 is not used. X_test is 129, y_test is 130. X_validate is 126,127,128 y is 127,128,129."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(1./(X_train.shape[0]/32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf\n",
    "\n",
    "To achieve error $\\epsilon$ with filter number $m$ the number of samples needed is $\\mathcal{O}(m/\\epsilon^2)$\n",
    "\n",
    "Let $\\epsilon = 0.1$, then $100m$ = samples or $m = samples/100$. The number of samples is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that included information from the too distant past actually makes the model worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switching the order of countries and timesteps affects the following:\n",
    "reshape A\n",
    "kernel size B\n",
    "kernal size AB\n",
    "concatenate axis 1->2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a convolution model. How is choosing a smaller frame size + kernel size different from\n",
    "larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is the *only* one that hasn't underestimated values. There is still a lot of parameter tuning to be completed though:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is the same up to small differences attributable to the individual trainings. \n",
    "\n",
    "In both models, the key difference between the predictions and the true values is that the ConvNet is having a hard\n",
    "time picking up on dramatic spikes in the number of new cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
