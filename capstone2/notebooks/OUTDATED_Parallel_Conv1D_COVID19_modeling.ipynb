{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, model_data.time_index.max() - n_days_into_future + 1):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(model_data.time_index <= max_date_in_window) & (model_data.time_index > max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_and_normalize_Xy(X, y, n_time_steps, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "\n",
    "    X_means = X_train.mean(axis=(1,2))\n",
    "    X_stds = X_train.std(axis=(1,2))\n",
    "\n",
    "    # To avoid division by zero. This is a big assumption but this typically occurs when the frame's feature\n",
    "    # value is identically zero, which would result in x-x_mean / x_std = 0 / 1 = 0. So it doesn't matter what \n",
    "    # the x_std value is changed to as they are always divided into 0.\n",
    "    X_stds[np.where(X_stds==0.)] = 1\n",
    "\n",
    "#     # First two features are time_index and time_index (days_since_first_case)\n",
    "    if date_normalization==False:\n",
    "        X_means[:,:2] = 0\n",
    "        X_stds[:, :2] = 1\n",
    "\n",
    "    # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "    # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "    # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "    # be repeated for each validation, test frame for each country for each timestep.\n",
    "    latest_training_mean = X_means[-1,:][np.newaxis, np.newaxis, np.newaxis, :]\n",
    "    latest_training_std = X_stds[-1,:][np.newaxis, np.newaxis, np.newaxis, :]\n",
    "    latest_training_std[np.where(latest_training_std==0)] = 1\n",
    "    \n",
    "    if train_test_only:\n",
    "    # Normalize the training data by each frame's specific mean and std deviation. \n",
    "        X_train_means = np.tile(X_means[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))\n",
    "        X_train_stds =  np.tile(X_stds[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))                \n",
    "        X_test_means = np.tile(latest_training_mean, \n",
    "                               (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "        X_test_stds = np.tile(latest_training_std, \n",
    "                              (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))   \n",
    "        \n",
    "        X_train = ((X_train - X_train_means) /  X_train_stds)\n",
    "        X_test = ((X_test - X_test_means) /  X_test_stds)\n",
    "        \n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "        normalizing_values = (X_train_means, X_train_stds, X_test_means, X_test_stds)\n",
    "    else:\n",
    "        X_train_means = np.tile(X_means[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))\n",
    "        X_train_stds =  np.tile(X_stds[:, np.newaxis, np.newaxis, :],\n",
    "                                (1, n_countries, n_time_steps, 1))                \n",
    "        X_validate_means = np.tile(latest_training_mean, \n",
    "                                   (X_validate.shape[0],X_validate.shape[1],X_validate.shape[2],1))\n",
    "        X_validate_stds = np.tile(latest_training_std, \n",
    "                                  (X_validate.shape[0],X_validate.shape[1],X_validate.shape[2],1))\n",
    "        X_test_means = np.tile(latest_training_mean, \n",
    "                               (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "        X_test_stds = np.tile(latest_training_std, \n",
    "                              (X_test.shape[0],X_test.shape[1],X_test.shape[2],1))    \n",
    "\n",
    "        X_train = ((X_train - X_train_means) /  X_train_stds)\n",
    "        X_validate = ((X_validate - X_validate_means) / X_validate_stds)\n",
    "        X_test = ((X_test - X_test_means) /  X_test_stds)\n",
    "\n",
    "\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "        normalizing_values = (X_train_means, X_train_stds, X_validate_means, X_validate_stds,\n",
    "                              X_test_means, X_test_stds)\n",
    "                          \n",
    "    return splits, normalizing_values\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_test, y_naive, y_predict, title=''):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20,5))\n",
    "    ymax = np.max([y_test.max(), y_predict.max()])\n",
    "    axes[0].scatter(y_test, y_naive, s=5)\n",
    "    axes[0].plot([0, ymax], [0, ymax])\n",
    "\n",
    "    axes[1].scatter(y_test, y_predict, s=5)\n",
    "    axes[1].plot([0, ymax], [0, ymax])\n",
    "\n",
    "    axes[0].set_xlabel('True value')\n",
    "    axes[0].set_ylabel('Predicted value')\n",
    "    axes[0].set_title('Naive model')\n",
    "\n",
    "    axes[1].set_xlabel('True value')\n",
    "    axes[1].set_ylabel('Predicted value')\n",
    "    axes[1].set_title('CNN model')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test,y_predict,title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "#     plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_naive, y_predict, y_true, n_test_frames,n_days_into_future, n_countries):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    for i in range(n_test_frames):\n",
    "        xrange = range(n_countries*i, n_countries*(i+1))\n",
    "        ax1.plot(xrange, y_true.reshape(-1,n_countries)[i,:]-y_naive.reshape(-1,n_countries)[i,:])\n",
    "        ax2.plot(xrange, y_true.reshape(-1,n_countries)[i,:]-y_predict.reshape(-1,n_countries)[i,:])\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('True minus Naive baseline')\n",
    "    ax2.set_title('True minus CNN')\n",
    "    residual_plot(y_true,y_naive,title='Naive residual',ax=ax3)\n",
    "    residual_plot(y_true,y_predict,title='CNN residual',ax=ax4)\n",
    "    plt.show()\n",
    "\n",
    "def n_step_model_predictions(model_data, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, batch_size,\n",
    "                             train_test_only=False, Xy_truncation=None):\n",
    "    \n",
    "    \"\"\" wrapper for iteration loop \n",
    "    \n",
    "    data : DataFrame of very specific make\n",
    "    \n",
    "    model : one of my custom models, sequential_Conv1D_model, SeparableConv2D_model, parallel_Conv1D_model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    new_cases_weighted_index = column_search(model_data,'new_cases_weighted', return_style='iloc')[0]#-n_pruned\n",
    "    prediction = []\n",
    "    naive = []\n",
    "    test = []\n",
    "    mae_naive_list = []\n",
    "    mae_predict_list = []\n",
    "    model_list = []\n",
    "\n",
    "    for n_days_into_future in predict_steps:\n",
    "        X, y = create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries)\n",
    "        if Xy_truncation is not None:\n",
    "            X = X[:Xy_truncation,:,:,:]\n",
    "            y = y[:Xy_truncation,:]\n",
    "        splits, normalizing = split_and_normalize_Xy(X, y,frame_size, n_validation_frames,\n",
    "                                                     n_test_frames,train_test_only=train_test_only)\n",
    "        A_splits = concatenate_4d_into_3d(splits, train_test_only=train_test_only) \n",
    "        \n",
    "        if model_generator == SeparableConv2D_model:\n",
    "            B_splits = splits\n",
    "            if train_test_only:\n",
    "                X_train_A, y_train_A, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_test_B, y_test_B = B_splits\n",
    "                \n",
    "                # model building\n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A\n",
    "                X_validate = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A\n",
    "            else:\n",
    "                X_train_A, y_train_A, X_validate_A, y_validate, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_validate_B, y_validate, X_test_B, y_test_B = B_splits  \n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A.ravel()\n",
    "                X_validate = [X_validate_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A.ravel()\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A.ravel()\n",
    "        else: \n",
    "            if train_test_only:\n",
    "                X_train, y_train, X_test, y_test = A_splits\n",
    "                X_validate, y_validate = X_test, y_test\n",
    "            else:\n",
    "                X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "                \n",
    "\n",
    "        model = model_generator(X_train, f, k)\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        # fit network\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "                  batch_size=batch_size)\n",
    "        \n",
    "        ### analysis\n",
    "        y_test = y_test.ravel()\n",
    "        y_naive = X[-n_test_frames:, :, -1, new_cases_weighted_index].ravel()\n",
    "        y_predict = model.predict(X_test).ravel()\n",
    "        # evaluate model\n",
    "\n",
    "        mae_naive = mean_absolute_error(y_test, y_naive)\n",
    "        mae_predict = mean_absolute_error(y_test, y_predict)\n",
    "        r2_naive = explained_variance_score(y_test, y_naive)\n",
    "        r2_predict = explained_variance_score(y_test, y_predict)\n",
    "        mae_naive_list.append(mae_naive)\n",
    "        mae_predict_list.append(mae_predict)\n",
    "        model_list.append(model)\n",
    "        print('{}-step MAE [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, mae_naive, mae_predict))\n",
    "        print('{}-step R^2 [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, r2_naive, r2_predict))\n",
    "        \n",
    "        true_predict_plot(y_test, y_naive, y_predict, title='')\n",
    "        residual_diff_plots(y_naive, y_predict, y_test, n_test_frames, n_days_into_future, n_countries)\n",
    "        \n",
    "    return test, naive, prediction, mae_naive_list, mae_predict_list, model_list\n",
    "\n",
    "def parallel_Conv1D_model(X_train, f, k):\n",
    "    (f11,f21,f31,f41,f51,f61,f12,f22,f32,f42,f52,f62) = f\n",
    "    k1,k2,k3,k4,k5,k6 = k\n",
    "    model_input = Input(shape=X_train.shape[1:])\n",
    "\n",
    "    # the first branch operates on the first input\n",
    "    A1 = Conv1D(filters=int(f11),\n",
    "               kernel_size=int(k1),        \n",
    "               padding='valid',\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A1 = MaxPooling1D(pool_size=2)(A1)\n",
    "    A1 = Activation('relu')(A1)\n",
    "    A1 = Conv1D(filters=int(f12),\n",
    "               kernel_size=int(k1),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A1)\n",
    "    # A1 = MaxPooling1D(pool_size=2)(A1)\n",
    "    A1 = Activation('relu')(A1)\n",
    "    A1 = Flatten()(A1)\n",
    "    # A1 = Dense(A1.shape[-1], activation='relu')(A1)\n",
    "    A1 = Model(inputs=model_input, outputs=A1)\n",
    "\n",
    "\n",
    "\n",
    "    A2 = Conv1D(filters=int(f21),\n",
    "               kernel_size=int(k2),        \n",
    "               padding='valid',\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A2 = MaxPooling1D(pool_size=2)(A2)\n",
    "    A2 = Activation('relu')(A2)\n",
    "    A2 = Conv1D(filters=int(f22),\n",
    "               kernel_size=int(k2),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A2)\n",
    "    # A2 = MaxPooling1D(pool_size=2)(A2)\n",
    "    A2 = Activation('relu')(A2)\n",
    "    A2 = Flatten()(A2)\n",
    "    # A2 = Dense(A2.shape[-1], activation='relu')(A2)\n",
    "    A2 = Model(inputs=model_input, outputs=A2)\n",
    "\n",
    "\n",
    "\n",
    "    A3 = Conv1D(filters=int(f31),\n",
    "               kernel_size=int(k3),        \n",
    "               padding='valid',\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A3 = MaxPooling1D(pool_size=2)(A3)\n",
    "    A3 = Activation('relu')(A3)\n",
    "    A3 = Conv1D(filters=int(f32),\n",
    "               kernel_size=int(k3),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A3)\n",
    "    # A3 = MaxPooling1D(pool_size=2)(A3)\n",
    "    A3 = Activation('relu')(A3)\n",
    "    A3 = Flatten()(A3)\n",
    "    # A3 = Dense(A3.shape[-1], activation='relu')(A3)\n",
    "    A3 = Model(inputs=model_input, outputs=A3)\n",
    "\n",
    "\n",
    "\n",
    "    A4 = Conv1D(filters=int(f41),\n",
    "               kernel_size=int(k4),        \n",
    "               padding='valid',\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A4 = MaxPooling1D(pool_size=2)(A4)\n",
    "    A4 = Activation('relu')(A4)\n",
    "    A4 = Conv1D(filters=int(f42),\n",
    "               kernel_size=int(k4),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A4)\n",
    "    # A4 = MaxPooling1D(pool_size=2)(A4)\n",
    "    A4 = Activation('relu')(A4)\n",
    "    A4 = Flatten()(A4)\n",
    "    # A4 = Dense(A4.shape[-1], activation='relu')(A4)\n",
    "    A4 = Model(inputs=model_input, outputs=A4)\n",
    "\n",
    "\n",
    "\n",
    "    A5 = Conv1D(filters=int(f51),\n",
    "               kernel_size=int(k5),        \n",
    "               padding='valid',\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A5 = MaxPooling1D(pool_size=2)(A5)\n",
    "    A5 = Activation('relu')(A5)\n",
    "    A5 = Conv1D(filters=int(f52),\n",
    "               kernel_size=int(k5),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A5)\n",
    "    # A5 = MaxPooling1D(pool_size=2)(A5)\n",
    "    A5 = Activation('relu')(A5)\n",
    "    A5 = Flatten()(A5)\n",
    "    # A5 = Dense(A5.shape[-1], activation='relu')(A5)\n",
    "    A5 = Model(inputs=model_input, outputs=A5)\n",
    "\n",
    "\n",
    "    A6 = Conv1D(filters=int(f61),\n",
    "               kernel_size=int(k6),        \n",
    "               padding='valid',\n",
    "               #kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(model_input)\n",
    "    # A6 = MaxPooling1D(pool_size=2)(A6)\n",
    "    A6 = Activation('relu')(A6)\n",
    "    A6 = Conv1D(filters=int(f62),\n",
    "               kernel_size=int(k6),        \n",
    "               padding='valid',\n",
    "    #            dilation_rate=2,\n",
    "    #            kernel_regularizer=l2(0.001),\n",
    "    #            activation='relu'\n",
    "                )(A6)\n",
    "    # A6 = MaxPooling1D(pool_size=2)(A6)\n",
    "    A6 = Activation('relu')(A6)\n",
    "    A6 = Flatten()(A6)\n",
    "    # A6 = Dense(A6.shape[-1], activation='relu')(A6)\n",
    "    A6 = Model(inputs=model_input, outputs=A6)\n",
    "\n",
    "    # combine the output of the parallel branches\n",
    "    # combine the output of the parallel branches\n",
    "\n",
    "    combined = concatenate([A1.output, A2.output, A3.output, A4.output, A5.output, A6.output], axis=1)\n",
    "\n",
    "    # apply a FC layer and then a regression prediction on the\n",
    "    # combined outputs\n",
    "\n",
    "\n",
    "    # FC1 = SeparableConv2D(filters=int(f2),\n",
    "    # #                     kernel_size=[int(k2),2],\n",
    "    #                     kernel_size=[2, int(k2)],\n",
    "    #                     activation='relu',\n",
    "    #                     padding='valid'\n",
    "    #                    )(combined)\n",
    "\n",
    "    FC = Flatten()(combined)\n",
    "    FC = Dense(FC.shape[-1]//2, activation='relu')(FC)\n",
    "    FC = Dense(1, activation='relu')(FC)\n",
    "\n",
    "    # our model will accept the inputs of the two branches and\n",
    "    # then output a single value\n",
    "    model = Model(inputs=model_input, outputs=FC)\n",
    "    return model\n",
    "\n",
    "def n_step_model_predictions(model_data, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, batch_size,\n",
    "                             train_test_only=False, Xy_truncation=None):\n",
    "    \n",
    "    \"\"\" wrapper for iteration loop \n",
    "    \n",
    "    data : DataFrame of very specific make\n",
    "    \n",
    "    model : one of my custom models, sequential_Conv1D_model, SeparableConv2D_model, parallel_Conv1D_model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    new_cases_weighted_index = column_search(model_data,'new_cases_weighted', return_style='iloc')[0]#-n_pruned\n",
    "    prediction = []\n",
    "    naive = []\n",
    "    test = []\n",
    "    mae_naive_list = []\n",
    "    mae_predict_list = []\n",
    "    model_list = []\n",
    "\n",
    "    for n_days_into_future in predict_steps:\n",
    "        X, y = create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries)\n",
    "        if Xy_truncation is not None:\n",
    "            X = X[:Xy_truncation,:,:,:]\n",
    "            y = y[:Xy_truncation,:]\n",
    "        splits, normalizing = split_and_normalize_Xy(X, y,frame_size, n_validation_frames,\n",
    "                                                     n_test_frames,train_test_only=train_test_only)\n",
    "        A_splits = concatenate_4d_into_3d(splits, train_test_only=train_test_only) \n",
    "        \n",
    "        if model_generator == SeparableConv2D_model:\n",
    "            B_splits = splits\n",
    "            if train_test_only:\n",
    "                X_train_A, y_train_A, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_test_B, y_test_B = B_splits\n",
    "                \n",
    "                # model building\n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A\n",
    "                X_validate = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A\n",
    "            else:\n",
    "                X_train_A, y_train_A, X_validate_A, y_validate, X_test_A, y_test_A = A_splits\n",
    "                X_train_B, y_train_B, X_validate_B, y_validate, X_test_B, y_test_B = B_splits  \n",
    "                X_train = [X_train_A, np.tile(X_train_B, (n_countries, 1,1,1))]\n",
    "                y_train = y_train_A.ravel()\n",
    "                X_validate = [X_validate_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_validate = y_test_A.ravel()\n",
    "                X_test = [X_test_A, np.tile(X_test_B, (n_countries, 1,1,1))]\n",
    "                y_test = y_test_A.ravel()\n",
    "        else: \n",
    "            if train_test_only:\n",
    "                X_train, y_train, X_test, y_test = A_splits\n",
    "                X_validate, y_validate = X_test, y_test\n",
    "            else:\n",
    "                X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "                \n",
    "\n",
    "        model = model_generator(X_train, f, k)\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        # fit network\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "                  batch_size=batch_size)\n",
    "        \n",
    "        ### analysis\n",
    "        y_test = y_test.ravel()\n",
    "        y_naive = X[-n_test_frames:, :, -1, new_cases_weighted_index].ravel()\n",
    "        y_predict = model.predict(X_test).ravel()\n",
    "        # evaluate model\n",
    "\n",
    "        mae_naive = mean_absolute_error(y_test, y_naive)\n",
    "        mae_predict = mean_absolute_error(y_test, y_predict)\n",
    "        r2_naive = explained_variance_score(y_test, y_naive)\n",
    "        r2_predict = explained_variance_score(y_test, y_predict)\n",
    "        mae_naive_list.append(mae_naive)\n",
    "        mae_predict_list.append(mae_predict)\n",
    "        model_list.append(model)\n",
    "        print('{}-step MAE [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, mae_naive, mae_predict))\n",
    "        print('{}-step R^2 [Naive, CNN] = [{},{}]'.format(\n",
    "        n_days_into_future, r2_naive, r2_predict))\n",
    "        \n",
    "        true_predict_plot(y_test, y_naive, y_predict, title='')\n",
    "        residual_diff_plots(y_naive, y_predict, y_test, n_test_frames, n_days_into_future, n_countries)\n",
    "        \n",
    "    return test, naive, prediction, mae_naive_list, mae_predict_list, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The more complex 1-D model\n",
    "\n",
    "Convolve a single input in parallel, using different kernel sizes. \n",
    "Send the same input to different kernel-size convolutional layers before aggregating. \n",
    "architecture from https://www.youtube.com/watch?v=nMkqWxMjWzg\n",
    "\n",
    "    different kernel sizes the entire point\n",
    "    combinations tested: \n",
    "    \n",
    "    1. equal filters\n",
    "    2. weighted n_filters\n",
    "    3. pooling (not good)\n",
    "    4. dilation (not good)\n",
    "    5. invidual dense layers on top of last 2 FC layers                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.iloc[:,1:]\n",
    "\n",
    "frame_size = 32\n",
    "start_date = n_dates - 2 * frame_size\n",
    "\n",
    "k = 1,2,3,4,5,6\n",
    "# first 6 values are first each of the first parallel layer, last 6 are second parallel layer.\n",
    "f = (8,8,8,8,8,8,16,16,16,16,16,16)\n",
    "\n",
    "n_validation_frames = 1\n",
    "n_test_frames = 3\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "predict_steps = [1, 7, 14]\n",
    "naive_scores = []\n",
    "predict_scores = []\n",
    "train_test_only=True\n",
    "model_generator = parallel_Conv1D_model\n",
    "results = n_step_model_predictions(model_data, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, batch_size,\n",
    "                             train_test_only=train_test_only)\n",
    "test, naive, prediction, mae_naive_list, mae_predict_list,model = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
