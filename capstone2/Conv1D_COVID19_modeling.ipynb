{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling1D, SeparableConv2D, Activation, concatenate, Conv2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN COVID-19 case number forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"files/architecture2.png\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n",
    "\n",
    "![title](conv_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    # can't include the max date because need at least 1 day in future to predict. +1 because of how range doesn't include endpoint\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 1):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window) & \n",
    "                                (time_index > max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    # y = model_data.new_cases_weighted.values.reshape(-1, model_data.time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                          train_test_only=False):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "                   \n",
    "    return splits\n",
    "\n",
    "\n",
    "def minmax(X, X_min, X_max):\n",
    "    # X_min and X_max need to have already been made into 4-d tensors with np.newaxis\n",
    "    tile_shape = np.array(np.array(X.shape) / np.array(X_min.shape), dtype=int)\n",
    "    denominator = np.tile(X_max, tile_shape) - np.tile(X_min, tile_shape)\n",
    "    denominator[denominator==0] = 1\n",
    "    X_scaled = (X - np.tile(X_min, tile_shape)) / denominator\n",
    "    return X_scaled\n",
    "\n",
    "def normal(X, X_mean, X_std):\n",
    "    tile_shape = np.array(np.array(X.shape) / np.array(X_mean.shape), dtype=int)\n",
    "    mean_ = np.tile(X_mean, tile_shape)\n",
    "    std_ =  np.tile(X_std, tile_shape)   \n",
    "    std_[np.where(std_==0.)] = 1\n",
    "    X_scaled = ((X - mean_) /  std_)\n",
    "    return X_scaled\n",
    "\n",
    "def normalize_Xy_splits(splits, normalization_method='minmax', train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    min_, max_ = (0, 1)\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "\n",
    "    if normalization_method=='minmax':\n",
    "        # To encapsulate the time-dependent nature of the problem and ignore the dramatic difference between current\n",
    "        # and initial behavior, only rescale the validation and testing frames by the most recent frame's values.\n",
    "        # There is only a single value per feature in this case, meaning that to rescale, the values need to\n",
    "        # be repeated for each validation, test frame for each country for each timestep.\n",
    "        X_min = X_train.min(axis=(2))\n",
    "        X_max = X_train.max(axis=(2))\n",
    "\n",
    "\n",
    "        X_train_scaled = minmax(X_train, X_min[:, :, np.newaxis, :],\n",
    "                        X_max[:, :, np.newaxis, :])\n",
    "        X_test_scaled = minmax(X_test, X_min[-1][np.newaxis, :, np.newaxis, :], \n",
    "                               X_max[-1][np.newaxis, :, np.newaxis, :])\n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = minmax(X_validate, X_min[-1,:][np.newaxis, :, np.newaxis, :], \n",
    "                                       X_max[-1,:][np.newaxis, :, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "    else:\n",
    "        \n",
    "        X_mean = X_train.mean(axis=(1,2))\n",
    "        X_std = X_train.std(axis=(1,2))\n",
    "\n",
    "\n",
    "        X_train_scaled = normal(X_train, \n",
    "                                X_mean[:, np.newaxis, np.newaxis, :],\n",
    "                                X_std[:, np.newaxis, np.newaxis, :])\n",
    "        X_test_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "        \n",
    "        if train_test_only:\n",
    "        # Normalize the training data by each frame's specific mean and std deviation. \n",
    "            splits = (X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "        else:\n",
    "            X_validate_scaled = normal(X_test, \n",
    "                               X_mean[-1,:][np.newaxis, np.newaxis, np.newaxis, :], \n",
    "                               X_std[-1,:][np.newaxis, np.newaxis, np.newaxis, :])\n",
    "            splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test)\n",
    "\n",
    "    return splits\n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_test, y_naive, y_predict, title=''):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    ymax = np.max([np.log(1+y_test).max(), np.log(1+y_predict).max()])\n",
    "    ax1.scatter(np.log(y_test+1), np.log(y_naive+1), s=5)\n",
    "    ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "\n",
    "    ax2.scatter(np.log(y_test+1), np.log(y_predict+1), s=5)\n",
    "    ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "\n",
    "    ax1.set_xlabel('Log(True value)')\n",
    "    ax1.set_ylabel('Log(Predicted value)')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('Log(True value)')\n",
    "    ax2.set_ylabel('Log(Predicted value)')\n",
    "    ax2.set_title('CNN model')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test,y_predict,title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('Log(True value)')\n",
    "    ax.grid(True)\n",
    "#     plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_naive, y_predict, y_true,n_days_into_future, n_countries):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    xrange = range(n_countries)\n",
    "    ax1.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_naive+1))\n",
    "    ax2.plot(xrange, np.log(y_true+1)\n",
    "             -np.log(y_predict+1))\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('Log(|True - Naive|)')\n",
    "    ax2.set_ylabel('Log(|True - CNN|)')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "    residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def Conv1D_model(X_train,f,k):\n",
    "    f1, f2 = f\n",
    "    k1, k2 = k\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=int(f1), kernel_size=int(k1),\n",
    "                     padding='valid',\n",
    "                     input_shape=X_train.shape[1:],\n",
    "                     activation='relu'\n",
    "                    )\n",
    "             )\n",
    "    model.add(Conv1D(filters=int(f2), \n",
    "                     kernel_size=int(k2), \n",
    "                     padding='valid',\n",
    "                     activation='relu'\n",
    "                    )\n",
    "             )\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(model.output.shape[1], \n",
    "                    activation='relu'\n",
    "                   )\n",
    "             )\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def n_step_model_predictions(model_data, target_data, time_index, model_generator, frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, predict_steps, f, k, epochs, learning_rate, batch_size,\n",
    "                             train_test_only=False, Xy_truncation=None,verbose=0, train_or_test='train'):\n",
    "    \n",
    "    \"\"\" wrapper for iteration loop \n",
    "    \n",
    "    data : DataFrame of very specific make\n",
    "    \n",
    "    model : one of my custom models, sequential_Conv1D_model, SeparableConv2D_model, parallel_Conv1D_model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for n_days_into_future in predict_steps:\n",
    "        X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "        splits = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=True,\n",
    "                                  train_test_only=train_test_only)\n",
    "        \n",
    "        A_splits = concatenate_4d_into_3d(splits, train_test_only=train_test_only) \n",
    "        if train_test_only:\n",
    "            X_train, y_train, X_test, y_test = A_splits\n",
    "            X_validate, y_validate = X_test, y_test\n",
    "        else:\n",
    "            X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "        model = model_generator(X_train, f, k)\n",
    "        model.compile(loss='mae', optimizer=Adam(learning_rate=learning_rate))\n",
    "        # fit network\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "                  batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "        ### analysis\n",
    "        if train_or_test == 'train':\n",
    "            y_true = y_validate.ravel()\n",
    "            y_naive = y[-(n_validation_frames+n_test_frames+n_days_into_future):-(n_test_frames+n_days_into_future), :].ravel()\n",
    "            y_predict = model.predict(X_validate).ravel()\n",
    "            # evaluate model\n",
    "        else:\n",
    "            y_true = y_test.ravel()\n",
    "            y_naive = y[-(n_test_frames+n_days_into_future):-n_days_into_future, :]\n",
    "            y_predict = model.predict(X_test).ravel()\n",
    "\n",
    "    return y_test, y_naive, y_predict, model, history, A_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use the new version of the data, filling zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick synopsis of the data. This data contains time series data for 10 features for 103 separate countries Because of how unfairly very small countries are weighted, the countries smaller than 25th percentile of population were removed.\n",
    "\n",
    "First course of action is to decide on the format of the inputs. For now, focus on 1-D convolution only.\n",
    "Need to decide on the dimension of the inputs. The typical format is (batch size, time steps, n_features). \n",
    "Two quick ideas :\n",
    "\n",
    "I think the simplest is to just find all windows of length $n$ for all countries such that the input dimension would be\n",
    "input_dim = (n_countries * n_windows, window_size, n_features) \n",
    "\n",
    "The other idea I had would be to do (n_windows, n_countries, window_size, n_features), and then the output would be the future values for each country.\n",
    "\n",
    "Just do the first for now. Also, just do a one-step model for now.\n",
    "\n",
    "Want to predict n_cases_weighted (weighted by the percentage of the population that the country consists of. Might be a dumb idea.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-942d6c9cb813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import cleaned data produced by other notebook.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnn_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Import cleaned data produced by other notebook. \n",
    "data = pd.read_csv('cnn_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0fc3c3acee29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mn_countries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mn_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_countries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "n_countries = data.location.nunique()\n",
    "n_dates = data.time_index.nunique()\n",
    "n_countries, n_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not enough countries have new_recovered_weighted values.\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.drop(columns=column_search(data, 'log'))\n",
    "# this is just a convenience thing for slicing later on.\n",
    "data.loc[:, 'time_index'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many params causes issues\n",
    "\n",
    "works well: \n",
    "\n",
    "    lower number of parameters, \n",
    "    log scaling\n",
    "    new_weighted_quantities only, \n",
    "    no minmax scaling\n",
    "    start date 32, \n",
    "    7 validation, \n",
    "    1 test\n",
    "    epochs 10\n",
    "    learning_rate 0.001\n",
    "    n_days_into_future = 1\n",
    "    n_validation_frames = 7\n",
    "    n_test_frames = 1\n",
    "    predict_steps = [1]\n",
    "    f = (32, 8)\n",
    "    k = (4, 4)\n",
    "    epochs = 10\n",
    "    batch_size = 128\n",
    "    \n",
    "    seems to do the best with fewer parameters and no contraction between last conv layer and dense layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-2-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.copy().iloc[:, [3,4,5,6]]\n",
    "model_data =  model_data.apply(lambda x :np.log(x+1))\n",
    "\n",
    "target_data = data.new_cases_weighted\n",
    "model_generator = Conv1D_model\n",
    "time_index = data.time_index\n",
    "\n",
    "frame_size = 14\n",
    "start_date = frame_size\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1\n",
    "f = (8, 2)\n",
    "k = (4, 4)\n",
    "epochs = 1000\n",
    "batch_size = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  elif not isinstance(value, collections.Sized):\n"
     ]
    }
   ],
   "source": [
    "# for n_days_into_future in predict_steps:\n",
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, date_normalization=False,\n",
    "                          )\n",
    "\n",
    "A_splits = concatenate_4d_into_3d(splits) \n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = A_splits\n",
    "\n",
    "model = model_generator(X_train, f, k)\n",
    "model.compile(loss='mae', optimizer=Adam(learning_rate=learning_rate))\n",
    "# fit network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15367, 14, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 1):\n",
    "    # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "    frame_data = model_data[(time_index <= max_date_in_window) & \n",
    "                            (time_index > max_date_in_window-frame_size)]\n",
    "    #     print(frame_data.shape)\n",
    "    # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "    reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "    #     print(reshaped_frame_data.shape)\n",
    "    # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "    # the first axis is always the default iteration axis. \n",
    "    # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "    resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "    frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15367 samples, validate on 889 samples\n",
      "Epoch 1/1000\n",
      "15367/15367 [==============================] - 1s 33us/sample - loss: 9.8041 - val_loss: 20.2992\n",
      "Epoch 2/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 7.7697 - val_loss: 17.0918\n",
      "Epoch 3/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 6.6947 - val_loss: 16.5824\n",
      "Epoch 4/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 6.2211 - val_loss: 15.5942\n",
      "Epoch 5/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 5.9533 - val_loss: 15.1400\n",
      "Epoch 6/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 5.8287 - val_loss: 15.0693\n",
      "Epoch 7/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.5897 - val_loss: 14.0424\n",
      "Epoch 8/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.4165 - val_loss: 14.1708\n",
      "Epoch 9/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.2925 - val_loss: 13.2595\n",
      "Epoch 10/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.2362 - val_loss: 13.7414\n",
      "Epoch 11/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.1411 - val_loss: 13.4382\n",
      "Epoch 12/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.0903 - val_loss: 12.6065\n",
      "Epoch 13/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.0396 - val_loss: 13.0471\n",
      "Epoch 14/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.0119 - val_loss: 12.3774\n",
      "Epoch 15/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 4.9122 - val_loss: 12.3335\n",
      "Epoch 16/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 4.8617 - val_loss: 13.1070\n",
      "Epoch 17/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 5.1843 - val_loss: 12.3657\n",
      "Epoch 18/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 4.7946 - val_loss: 12.8529\n",
      "Epoch 19/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 4.7718 - val_loss: 11.9780\n",
      "Epoch 20/1000\n",
      "15367/15367 [==============================] - 0s 17us/sample - loss: 4.6738 - val_loss: 11.8539\n",
      "Epoch 21/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 4.6752 - val_loss: 12.5019\n",
      "Epoch 22/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 4.6206 - val_loss: 11.5048\n",
      "Epoch 23/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.4774 - val_loss: 11.5554\n",
      "Epoch 24/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.4461 - val_loss: 11.6247\n",
      "Epoch 25/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.4017 - val_loss: 11.4627\n",
      "Epoch 26/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.4469 - val_loss: 10.9744\n",
      "Epoch 27/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.3081 - val_loss: 10.9296\n",
      "Epoch 28/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.3313 - val_loss: 11.0249\n",
      "Epoch 29/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.2889 - val_loss: 10.7975\n",
      "Epoch 30/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 4.2281 - val_loss: 10.6461\n",
      "Epoch 31/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.2075 - val_loss: 10.4392\n",
      "Epoch 32/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 4.1696 - val_loss: 10.4256\n",
      "Epoch 33/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.1448 - val_loss: 10.3413\n",
      "Epoch 34/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.1260 - val_loss: 10.1946\n",
      "Epoch 35/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.0949 - val_loss: 10.3905\n",
      "Epoch 36/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.1407 - val_loss: 10.4792\n",
      "Epoch 37/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.1016 - val_loss: 10.4055\n",
      "Epoch 38/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 4.1116 - val_loss: 9.9900\n",
      "Epoch 39/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 4.0973 - val_loss: 10.0106\n",
      "Epoch 40/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.0428 - val_loss: 10.0118\n",
      "Epoch 41/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.2029 - val_loss: 9.7619\n",
      "Epoch 42/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.9915 - val_loss: 9.9553\n",
      "Epoch 43/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.9798 - val_loss: 10.4183\n",
      "Epoch 44/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 4.0404 - val_loss: 9.5498\n",
      "Epoch 45/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.9680 - val_loss: 9.4852\n",
      "Epoch 46/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.9752 - val_loss: 9.3511\n",
      "Epoch 47/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.9368 - val_loss: 9.5078\n",
      "Epoch 48/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.9417 - val_loss: 9.5216\n",
      "Epoch 49/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.8843 - val_loss: 9.3584\n",
      "Epoch 50/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.9402 - val_loss: 9.1394\n",
      "Epoch 51/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.8841 - val_loss: 9.1140\n",
      "Epoch 52/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8496 - val_loss: 8.9559\n",
      "Epoch 53/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8732 - val_loss: 9.0931\n",
      "Epoch 54/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.9831 - val_loss: 9.2230\n",
      "Epoch 55/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.0460 - val_loss: 8.9501\n",
      "Epoch 56/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.7854 - val_loss: 9.2762\n",
      "Epoch 57/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8473 - val_loss: 8.8504\n",
      "Epoch 58/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7873 - val_loss: 9.0516\n",
      "Epoch 59/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8146 - val_loss: 8.6413\n",
      "Epoch 60/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.7925 - val_loss: 9.1657\n",
      "Epoch 61/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7566 - val_loss: 9.6588\n",
      "Epoch 62/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.8966 - val_loss: 8.5022\n",
      "Epoch 63/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7189 - val_loss: 8.9911\n",
      "Epoch 64/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.7997 - val_loss: 8.5113\n",
      "Epoch 65/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7070 - val_loss: 8.5101\n",
      "Epoch 66/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.6938 - val_loss: 8.5035\n",
      "Epoch 67/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.7023 - val_loss: 8.5731\n",
      "Epoch 68/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7550 - val_loss: 8.6454\n",
      "Epoch 69/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.7058 - val_loss: 8.5279\n",
      "Epoch 70/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.7355 - val_loss: 8.7388\n",
      "Epoch 71/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7698 - val_loss: 8.1156\n",
      "Epoch 72/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.7347 - val_loss: 8.2709\n",
      "Epoch 73/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.6550 - val_loss: 8.0628\n",
      "Epoch 74/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.6411 - val_loss: 8.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000\n",
      "15367/15367 [==============================] - 0s 23us/sample - loss: 3.6717 - val_loss: 8.0222\n",
      "Epoch 76/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.6232 - val_loss: 8.1064\n",
      "Epoch 77/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8110 - val_loss: 8.3623\n",
      "Epoch 78/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.6858 - val_loss: 8.1126\n",
      "Epoch 79/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.6472 - val_loss: 8.7972\n",
      "Epoch 80/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.6641 - val_loss: 8.2282\n",
      "Epoch 81/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.6152 - val_loss: 7.8886\n",
      "Epoch 82/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5979 - val_loss: 8.1171\n",
      "Epoch 83/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.6459 - val_loss: 8.1450\n",
      "Epoch 84/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.6054 - val_loss: 7.8151\n",
      "Epoch 85/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5967 - val_loss: 7.9158\n",
      "Epoch 86/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5655 - val_loss: 7.7797\n",
      "Epoch 87/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5880 - val_loss: 7.7090\n",
      "Epoch 88/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5513 - val_loss: 7.7868\n",
      "Epoch 89/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5916 - val_loss: 7.6839\n",
      "Epoch 90/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.6316 - val_loss: 7.7082\n",
      "Epoch 91/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5289 - val_loss: 9.4107\n",
      "Epoch 92/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.9313 - val_loss: 7.8843\n",
      "Epoch 93/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.5647 - val_loss: 7.7890\n",
      "Epoch 94/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5004 - val_loss: 7.4915\n",
      "Epoch 95/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5028 - val_loss: 7.6590\n",
      "Epoch 96/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5376 - val_loss: 7.5803\n",
      "Epoch 97/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5336 - val_loss: 7.3535\n",
      "Epoch 98/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.4786 - val_loss: 7.9444\n",
      "Epoch 99/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.8446 - val_loss: 7.2459\n",
      "Epoch 100/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.5300 - val_loss: 9.6766\n",
      "Epoch 101/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.8131 - val_loss: 7.2779\n",
      "Epoch 102/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.4809 - val_loss: 7.7223\n",
      "Epoch 103/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4574 - val_loss: 7.4349\n",
      "Epoch 104/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4527 - val_loss: 7.3740\n",
      "Epoch 105/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4406 - val_loss: 7.2395\n",
      "Epoch 106/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4473 - val_loss: 7.3970\n",
      "Epoch 107/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4803 - val_loss: 7.7797\n",
      "Epoch 108/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.8095 - val_loss: 7.2827\n",
      "Epoch 109/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.4763 - val_loss: 7.3386\n",
      "Epoch 110/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4308 - val_loss: 7.3575\n",
      "Epoch 111/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3981 - val_loss: 7.0574\n",
      "Epoch 112/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4284 - val_loss: 7.0285\n",
      "Epoch 113/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4630 - val_loss: 7.1618\n",
      "Epoch 114/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4475 - val_loss: 7.0777\n",
      "Epoch 115/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4087 - val_loss: 7.5481\n",
      "Epoch 116/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4195 - val_loss: 6.9518\n",
      "Epoch 117/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.3867 - val_loss: 7.0181\n",
      "Epoch 118/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3631 - val_loss: 7.0163\n",
      "Epoch 119/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4125 - val_loss: 7.0955\n",
      "Epoch 120/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.3584 - val_loss: 6.9398\n",
      "Epoch 121/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.4435 - val_loss: 7.1100\n",
      "Epoch 122/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4097 - val_loss: 6.8120\n",
      "Epoch 123/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4254 - val_loss: 7.7571\n",
      "Epoch 124/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.7850 - val_loss: 6.6896\n",
      "Epoch 125/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3999 - val_loss: 6.8578\n",
      "Epoch 126/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.3412 - val_loss: 6.9453\n",
      "Epoch 127/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3655 - val_loss: 7.0716\n",
      "Epoch 128/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3706 - val_loss: 7.0658\n",
      "Epoch 129/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.3344 - val_loss: 7.0159\n",
      "Epoch 130/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.3568 - val_loss: 7.7028\n",
      "Epoch 131/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5754 - val_loss: 7.2806\n",
      "Epoch 132/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.3504 - val_loss: 6.6895\n",
      "Epoch 133/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3822 - val_loss: 6.7206\n",
      "Epoch 134/1000\n",
      "15367/15367 [==============================] - 0s 22us/sample - loss: 3.3924 - val_loss: 6.7757\n",
      "Epoch 135/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.3221 - val_loss: 6.5683\n",
      "Epoch 136/1000\n",
      "15367/15367 [==============================] - 0s 20us/sample - loss: 3.2865 - val_loss: 8.6885\n",
      "Epoch 137/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 4.0358 - val_loss: 6.8813\n",
      "Epoch 138/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4699 - val_loss: 7.1285\n",
      "Epoch 139/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.5138 - val_loss: 6.4681\n",
      "Epoch 140/1000\n",
      "15367/15367 [==============================] - 0s 21us/sample - loss: 3.3253 - val_loss: 6.5787\n",
      "Epoch 141/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.4386 - val_loss: 6.5416\n",
      "Epoch 142/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.3796 - val_loss: 6.5477\n",
      "Epoch 143/1000\n",
      "15367/15367 [==============================] - 0s 18us/sample - loss: 3.3132 - val_loss: 6.4924\n",
      "Epoch 144/1000\n",
      "15367/15367 [==============================] - 0s 19us/sample - loss: 3.2749 - val_loss: 6.4358\n",
      "Epoch 145/1000\n",
      "11520/15367 [=====================>........] - ETA: 0s - loss: 3.3084"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d90e4cfeaff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n\u001b[1;32m----> 2\u001b[1;33m           batch_size=batch_size, verbose=1)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m### analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_validate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_validate, y_validate), \n",
    "          batch_size=batch_size, verbose=1)\n",
    "\n",
    "### analysis\n",
    "y_true = y_validate.ravel()\n",
    "y_naive = y[-(n_validation_frames+n_test_frames+n_days_into_future):-(n_test_frames+n_days_into_future), :].ravel()\n",
    "y_predict = model.predict(X_validate).ravel()\n",
    "# evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15367, 14, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_validate.ravel()\n",
    "y_naive = y[-(n_validation_frames+n_test_frames+n_days_into_future):-(n_test_frames+n_days_into_future), :].ravel()\n",
    "y_predict = model.predict(X_validate).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_naive = mean_absolute_error(y_test, y_naive)\n",
    "mae_predict = mean_absolute_error(y_test, y_predict)\n",
    "r2_naive = explained_variance_score(y_test, y_naive)\n",
    "r2_predict = explained_variance_score(y_test, y_predict)\n",
    "\n",
    "print('{}-step MAE [Naive, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, mae_naive, mae_predict))\n",
    "print('{}-step R^2 [Naive, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, r2_naive, r2_predict))\n",
    "\n",
    "true_predict_plot(y_test, y_naive, y_predict, title='')\n",
    "residual_diff_plots(y_naive, y_predict, y_test, n_days_into_future, n_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_days_into_future = [1]\n",
    "# y_test, y_naive, y_predict, model, history, split_data = \\\n",
    "#                              n_step_model_predictions(model_data, target_data, time_index, model_generator,\n",
    "#                                                       frame_size, start_date, n_countries,\n",
    "#                              n_validation_frames, n_test_frames, n_days_into_future, f, k, epochs, learning_rate, batch_size,\n",
    "#                                                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        y_true = y_test.ravel()\n",
    "        y_naive = y[-(n_test_frames+n_days_into_future):-n_days_into_future, :]\n",
    "        y_predict = model.predict(X_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old loss vs. val loss curve, pre-zero-filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history.history['loss'], label='loss')\n",
    "_ = plt.plot(history.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_into_future = [7]\n",
    "y_test7, y_naive7, y_predict7, model7, history7 = \\\n",
    "                             n_step_model_predictions(model_data, target_data, time_index, model_generator,\n",
    "                                                      frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, n_days_into_future, f, k, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_naive7 = mean_absolute_error(y_test7, y_naive7)\n",
    "mae_predict7 = mean_absolute_error(y_test7, y_predict7)\n",
    "r2_naive7 = explained_variance_score(y_test7, y_naive7)\n",
    "r2_predict7 = explained_variance_score(y_test7, y_predict7)\n",
    "\n",
    "print('{}-step MAE [naive7, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, mae_naive7, mae_predict7))\n",
    "print('{}-step R^2 [naive7, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, r2_naive7, r2_predict7))\n",
    "\n",
    "true_predict_plot(y_test7, y_naive7, y_predict7, title='')\n",
    "residual_diff_plots(y_naive7, y_predict7, y_test7, n_days_into_future, n_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history7.history['loss'], label='loss')\n",
    "_ = plt.plot(history7.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14-day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_into_future = [14]\n",
    "y_test14, y_naive14, y_predict14, model14, history14  = \\\n",
    "                             n_step_model_predictions(model_data, target_data, time_index, model_generator,\n",
    "                                                      frame_size, start_date, n_countries,\n",
    "                             n_validation_frames, n_test_frames, n_days_into_future, f, k, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_naive14 = mean_absolute_error(y_test14, y_naive14)\n",
    "mae_predict14 = mean_absolute_error(y_test14, y_predict14)\n",
    "r2_naive14 = explained_variance_score(y_test14, y_naive14)\n",
    "r2_predict14 = explained_variance_score(y_test14, y_predict14)\n",
    "\n",
    "print('{}-step MAE [naive14, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, mae_naive14, mae_predict14))\n",
    "print('{}-step R^2 [naive14, CNN] = [{},{}]'.format(\n",
    "n_days_into_future, r2_naive14, r2_predict14))\n",
    "\n",
    "true_predict_plot(y_test14, y_naive14, y_predict14, title='')\n",
    "residual_diff_plots(y_naive14, y_predict14, y_test14, n_days_into_future, n_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(history14.history['loss'], label='loss')\n",
    "_ = plt.plot(history14.history['val_loss'], label='val_loss')\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('CNN_covid_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over possible leading window dates (window right edge, not inclusive),\n",
    "a 4-d tensor with dimensions given by the following is created:\n",
    "    \n",
    "```(n_windows, n_countries, n_time_steps, n_features)```\n",
    "\n",
    "Even if I don't use the input of this form, it makes it much easier to slice into train, validate and test, by slicing along\n",
    "the ```n_windows``` axis. \n",
    "\n",
    "Note that if I also only wanted to include data from after the first case, that is possible by slicing data.time_index >= 1 \n",
    "\n",
    "Putting this all together:\n",
    "\n",
    "\n",
    "Now, splitting X intro train, validate, test, is as easy as slicing the first axis. Depending on how the CNN is set up, this\n",
    "axis can later be flattened by simply applying ```np.concatenate(X_train, axis=0)```\n",
    "\n",
    "Now, for the target data ```y```. To be as general as possible, I note that I should first shift the time series and combine them to store all of the data, and THEN manipulate it, i.e.\n",
    "\n",
    "Assuming that ```X``` is of the shape ```(n_windows, n_countries, n_time_steps, n_features)```, the target data ```y``` should\n",
    "be of shape ```(n_windows, n_countries)``` and the values for each slice of the first axis is then of shape ```(n_countries,)```, with the values equaling the ```model_data.n_cases_weighted``` value for ```time_index == window_right_edge_inclusive + n```, where n is the prediction step size. \n",
    "\n",
    "Because we need future values with which to measure error, the maximum that the leading window edge can be is always \n",
    "\n",
    "```model_data.time_index.max() - n```\n",
    "\n",
    "In other words, because the maximum date value is\n",
    "\n",
    "If I want to predict (and measure error) for 7 days in the future, the last window would have leading edge \n",
    "```model_data.time_index == 123```. \n",
    "\n",
    "For now, just test the waters with a 1-day prediction.\n",
    "To do so, first split the X and y data into train, validate, test.\n",
    "For the first attempt at a model, use Conv1D only, have to flatten the data with concatenations.\n",
    "Before this, however, I want to rescale the data. Now, because each different element in the first axis is a different time range, the correct action is to (if normalizing) take the mean with respect to axis=1 and axis=2 ```n_countries``` and  ```n_time_steps```. This leaves a total number of averages of shape ```(n_windows, n_features)```, i.e. the mean and standard deviations for each feature for each date range. This ensures that no data snooping has occurred. To actually subtract and divide by these values, need to reform the same shape array (or at least that seems to be the easiest method to me.\n",
    "\n",
    "Just to check, the slice [0, :, :, 0] should only contain one value.\n",
    "\n",
    "Need to normalize only using the values up until frame; in fact, I only use the values inside each frame to normalize.\n",
    "In the case of normalizing the validation and testing sets, use the most recent frame's means and standard deviation to normalize. This requires forming the correctly shaped array which is performed using np.tile()\n",
    "\n",
    "Unfortunately there are values (actually a single value) where the standard deviation equals 0. Luckily, this\n",
    "can be circumvented because the value of $X-\\bar{X}$ is zero in the same place; i.e. I set $\\sigma=1$ at that location, because\n",
    "it doesn't actually affect the value.\n",
    "\n",
    "Notes on results\n",
    "\n",
    "    1. kernel small->large seems to work ok.\n",
    "    2. filters\n",
    "    3. pooling seems to hurt.\n",
    "\n",
    "Create a 5-d tensor for convolution and FC layer parameters using itertools.\n",
    "The layers of the CNN will remain the same throughout testing for now. That is, \n",
    "\n",
    "    1. Conv1D(f1, k1)\n",
    "    2. Conv1D(f2, k2)\n",
    "    3. Flatten\n",
    "    4. Dense(d)\n",
    "    5. Dense(1)) \n",
    "\n",
    "I don't know enough yet, but my intuition tells me that the overall trend (macroscopic picture)\n",
    "is much more important to capture than microscopic. \n",
    "   \n",
    "    f1 : number of filters,  first convolutional layer\n",
    "    k1 : kernel size, first convolutional layer\n",
    "    f1 : number of filters,  second convolutional layer\n",
    "    k1 : kernel size, second convolutional layer\n",
    "    d : number of nodes in hidden fully connected layer\n",
    "    \n",
    "\n",
    "I believe the naive baseline is performing so much better simply because of the growth in number of new cases; i.e. the model is training on smaller values and so in every prediction its underestimating. It's also predicting nearly the same values for each separate validation date, likely indicative of overfitting. \n",
    "\n",
    "Changing the activations and the layers seems to work much better than the number of filters and kernel size.\n",
    "\n",
    "ReLU activation on both cd layers somehow increases the values, i.e. away from zero? how does that make sense.\n",
    "Predicting non-negative quantity -> ReLU at eachy layer, otherwise model will try to use negative values which never do anything?\n",
    "\n",
    "On the surface this performs worse but doesn't look like it's making ***terrible*** predictions. I'll show that I believe\n",
    "that the model is indeed over-fitting. \n",
    "\n",
    "Relation between X slicing and the original data: \n",
    "\n",
    "    X[-1, :, -1, 2] = model_data.time_index.max() - n_days_into_future \n",
    "    X[-t, :, -1, 2] = (model_data.time_index.max() - n_days_into_future + 1) - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will discuss the data format which I have found the most useful. The motivation behind this formatting is that is has to be compatible with keras' convolution layers, respect ordering in time and hopefully be easy to use.\n",
    "\n",
    "A \"sample\" or single input of the convolutional model is to be a single time window which I shall refer to as a frame. The output of the model will be a single day's worth of predictions, namely, the next day's number of cases, (with some experimentation into the forecasting range to be performed later). The Conv1D layers require that each sample be of two dimensional input ```(n_time_steps, n_features)```, but the training / testing sets themselves are passed as 3-D tensors of shape ```(all_train_or_test_frames, n_time_steps, n_features)```. In order to partition time in an easy manner, it is easier to first create a 4-D tensor, whose (first) axis can be thought of as indexing the leading edge of the time frames. Therefore, slicing into training, validation and testing, is a well ordered operation simply performed by slicing the tensor along this axis. (picture) ```(n_frames, n_countries, n_time_steps, n_features)```. It is also relatively straight forward how to manipulate the target variables and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape) \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (n_validation_frames + n_test_frames)\n",
    "new_cases_weighted_index=0\n",
    "((np.exp(X[-t, :, -1, new_cases_weighted_index])-1) - data[data.time_index == ((data.time_index.max() - n_days_into_future + 1) - t)].new_cases_weighted).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so to test the baseline, the predictions are of course the date on the RHS + n_days_into_future, or \n",
    "\n",
    "This proves the case where n_validate_frames == 1. Now try it for 7. This means that the \"first\" validation frame is t==8\n",
    "\n",
    "Ok. Now to see if the baseline is actually what I think it is. There could be an issue into how I am calculating the loss.\n",
    "This is calculating the loss of all batch frames simulataneously...? Each frame for each country makes a single prediction.\n",
    "Should I be trying to group it only by frame though? I.e. 2-d input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_naive = X[-n_test_frames-1, :, -1,  new_cases_weighted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y - create_Xy(model_data, start_date, frame_size, n_days_into_future, n_countries)[1][-y.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow the relation between X and y is upheld (because of for loop?) they are not changing between n_days_into_future_changes. \n",
    "\n",
    "Emulate the last iteration of for loop.\n",
    "\n",
    "What should be going on. For n_days_into_future = 7, X should be slicing data from time <= 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the data is reshaped to be an iterable of sequences of length max_date_in_window, which is then truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For y. What should be happening? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is made such that its maximum date is  ```model_data.time_index.max() - n_days_into_future```. So by the very creation of\n",
    "X, X and y are already staggered if y is sliced correctly. Which would be ```start_date + n_days_into_future : ```, or simply ```-X.shape[0]:```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test is the last frame. meaning that it currently contains t-14 information. y_test is the very last day, n_cases_per_million\n",
    "un-normalized, so the naive model would be simply to use y_predict = X_test_original[:, -1, 2], this takes the values of the feature, new_cases_per_million, on the last day of the frame, for all countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeperableConv2D model. I believe that time steps should be channels and then countries should be the rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly, my first guess at the parameters was very close to the optimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The top 10 sets of parameters (k1,k2,f1,f2,d) are:\\n')\n",
    "\n",
    "# mae_rankings = pd.DataFrame(np.concatenate((parameter_grid[mae_list.argsort()],mae_list[mae_list.argsort()].reshape(-1,1)),axis=1), \n",
    "#              columns=['kernel_layer_1','kernel_layer_2','filter_layer_1','filter_layer_2','MAE'])\n",
    "# mae_rankings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mae_rankings.to_csv('conv_parameter_rankings.csv')\n",
    "\n",
    "# best_param_predictions = predictions_list[mae_list.argsort()][0]\n",
    "\n",
    "# pd.DataFrame(predictions_list[mae_list.argsort()]).to_csv('model_param_grid_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('conv_parameter_rankings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because ```n_days_into_future == 1```, The most recent date in the last frame of X is data.time_index.max()-1.\n",
    "The maximum date in y is data.time_index().max(). Therefore, to convert, we have\n",
    "    \n",
    "    X[-1,:,-1,:] = y[-2, :]\n",
    "    \n",
    "or, more generally, \n",
    "\n",
    "    X[-t,:,-1,:] = y[-(n+t), :]\n",
    "    \n",
    "testing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_day_in_X = -1\n",
    "# if last_day_in_X == -1:\n",
    "#     last_day_in_X = None\n",
    "# X_test_original = X[-16:last_day_in_X, :, -1, new_cases_weighted_index] \n",
    "# if last_day_in_X is None:\n",
    "#     last_day_in_X = 0\n",
    "\n",
    "# X_test_original - y[-(n_test_frames+n_days_into_future+X_test_original.shape[0]+last_day_in_X-1):\n",
    "#                     -(n_days_into_future+last_day_in_X), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the staggering has been done correctly. That is, each entry along the first axis of X predicts.\n",
    "Need to make sure that the naive predictions are being chosen correctly. To do so, just look at the instance\n",
    "where n_frames_validation == 1 and n_frames_test == 1 (to make slicing easier) for n_days_into_future == 7.\n",
    "\n",
    "Specifically, the shifting and comparison between y_validate and X_train isn't so hard as it follows the above formula.\n",
    "This is prior to concatenation / squeezing / flattening the first axis.\n",
    "\n",
    "    The following holds but this is not the predictions relation:   X[-t,:,-1,:] = y[-(n+t), :]\n",
    "                                                  \n",
    "    y has already been shifted to account for the n_days_into_future prediction. Therefore,     \n",
    "    True future new_cases_weighted values : y[-t,:]. This is the n_days_into_future values.\n",
    "    \n",
    "    CNN predictions new_cases_weighted : model.predict(X[-t, :, :, :]). This predicts using the \"present day\" frame. \n",
    "    \n",
    "    Naive predictions new_cases_weighted values: X[-t,:,-1, 2]. This takes the present day value from the last values of the frames.\n",
    "\n",
    "Therefore, when n_validation_frames > 1.  are simply\n",
    "\n",
    "    Naive predictions = X[validation_indices, :, -1, 2]\n",
    "    CNN predictions = model.predict(X[validation_indices, :, :, :])\n",
    "    True future values = y[validation_indices, :]\n",
    "    \n",
    "    Where validation indices is assumed to be a correctly formatted array of negative indices. \n",
    "    \n",
    "    \n",
    "    The last issue is the fact that X_validate is normalized and y values are not. Therefore, for the prediction X_validate is used, but for the naive baseline either the inverse normalized X_validation or just the original X should be used. Original\n",
    "    X is actually easier because of 4-d index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train and \"test\" without using the holdout test set. i.e. just use train and validation sets to test\n",
    "This uses a single frame to validate (and test). Therefore, the naive model is formed by taking the frame of\n",
    "X_validate and using its values for the prediction. The true values are stored in y_validate, the predicted values\n",
    "are formed by using X_validate to predict, the naive values are the values in X_validate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when slicing, the n_days_into_future buffer has already been factored into account. i.e. test\n",
    "slicing is X[-n_test_frames:,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X time_index 130 is not used. X_test is 129, y_test is 130. X_validate is 126,127,128 y is 127,128,129."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(1./(X_train.shape[0]/32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://papers.nips.cc/paper/7320-how-many-samples-are-needed-to-estimate-a-convolutional-neural-network.pdf\n",
    "\n",
    "To achieve error $\\epsilon$ with filter number $m$ the number of samples needed is $\\mathcal{O}(m/\\epsilon^2)$\n",
    "\n",
    "Let $\\epsilon = 0.1$, then $100m$ = samples or $m = samples/100$. The number of samples is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that included information from the too distant past actually makes the model worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switching the order of countries and timesteps affects the following:\n",
    "reshape A\n",
    "kernel size B\n",
    "kernal size AB\n",
    "concatenate axis 1->2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a convolution model. How is choosing a smaller frame size + kernel size different from\n",
    "larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is the *only* one that hasn't underestimated values. There is still a lot of parameter tuning to be completed though:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is the same up to small differences attributable to the individual trainings. \n",
    "\n",
    "In both models, the key difference between the predictions and the true values is that the ConvNet is having a hard\n",
    "time picking up on dramatic spikes in the number of new cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archived results\n",
    "\n",
    "## 8-2//4\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,9.683242379442238]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.7953289844554481]\n",
    "\n",
    "\n",
    "## 8-2//full\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,8.567139680099809]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.8559826950197971]\n",
    "\n",
    "## 8-2//8\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,11.227516786392403]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.7102283800705309]\n",
    "\n",
    "## 8-2//2\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,8.873572600787506]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.7925219254301763]\n",
    "\n",
    "## 8-4//full\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,9.362758538127283]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.7841815468693817]\n",
    "\n",
    "## 4-2//full\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,9.758042888093435]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.8573291582800996]\n",
    "\n",
    "## 8-2//2 10k\n",
    "\n",
    "1-step MAE [Naive, CNN] = [10.181423289216637,8.977670771914049]\n",
    "1-step R^2 [Naive, CNN] = [0.5095423321692748,0.8255246499845381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
