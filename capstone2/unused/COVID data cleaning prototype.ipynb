{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "from matplotlib.patches import Rectangle\n",
    "from datetime import datetime\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook cleans and wrangles numerous data sets, making them uniform\n",
    "so that they can be used in a data-driven model for COVID-19 prediction.\n",
    "\n",
    "The key cleaning measures are those which find the most viable set of countries and date ranges\n",
    "such that the maximal amount of data can be used. In other words, different datasets can have data\n",
    "on a different set of countries; to avoid introducing large quantities of missing values\n",
    "the intersection of these countries is taken. For the date ranges, depending on the quantity,\n",
    "extrapolation/interpolation is used to ensure that each time series is defined to be non-zero\n",
    "on all dates. This process is kept track of by encoding the dates which have interpolated values.\n",
    "There are two measures to do so. Essentially its one hot encoding for the categories ['extrapolated', 'interpolated', 'actual']. The other measure is to track the \"days since infection\" where 0 represents the first day with a recorded\n",
    "case of COVID within that country. I leave the more complex feature creation to the exploratory data analysis portion\n",
    "of this project.\n",
    "\n",
    "Some of the data is currently not used but may be incorporated later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a id='toc'></a>\n",
    "\n",
    "## [Data wrangling function definitions](#generalfunctions)\n",
    "\n",
    "# Data <a id='data'></a>\n",
    "\n",
    "<!-- ## [The COVID tracking project testing data.](#source1)\n",
    "[https://covidtracking.com/api](https://covidtracking.com/api)\n",
    "            -->\n",
    "## [JHU CSSE case data.](#csse)\n",
    "[https://systems.jhu.edu/research/public-health/ncov/](https://systems.jhu.edu/research/public-health/ncov/)\n",
    "\n",
    "**Data available at:**\n",
    "[https://github.com/CSSEGISandData/COVID-19](https://github.com/CSSEGISandData/COVID-19)\n",
    "\n",
    "This data is split between a collection of .csv files of two different formats; first, the daily reports (global) are\n",
    "separated by day, each residing in their own .csv. Additionally, the daily report files have three different formats that need to be taken into account when compiling the data. The daily report data itself contains values on the number of confirmed cases, deceased, active cases, recovered cases.\n",
    "\n",
    "For the other format, .csv files with 'timeseries' in their filename, the data contains values for confirmed, deceased, recovered and are split between global numbers (contains United States as a whole) and numbers for the united states (statewide).\n",
    "           \n",
    "## [IHME hospital data](#ihme)\n",
    "\n",
    "**Data available at:**\n",
    "[http://www.healthdata.org/covid/data-downloads](http://www.healthdata.org/covid/data-downloads)\n",
    "\n",
    "The IHME hospital data is one of the more unique datasets I've discovered with \n",
    "           \n",
    "## [OWID case and test data](#owid)\n",
    "\n",
    "**Data available via github**\n",
    "[https://github.com/owid/covid-19-data](https://github.com/owid/covid-19-data)\n",
    "\n",
    "[https://ourworldindata.org/covid-testing](https://ourworldindata.org/covid-testing)\n",
    "\n",
    "The OWID dataset contains information regarding case and test numbers; it overlaps with the JHU CSSE \n",
    "and Testing Tracker datasets but I am going to attempt to use it in conjunction with those two because\n",
    "of how there is unreliable reporting. In other words to get the bigger picture I'm looking to stitch together\n",
    "multiple datasets.\n",
    "           \n",
    "## [OxCGRT government response data](#oxcgrt)\n",
    "\n",
    "**Data available at:**\n",
    "[https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv](https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv)\n",
    "\n",
    "\n",
    "**If API used to pull data (I elect not to because the datasets are different)**\n",
    "[https://covidtracker.bsg.ox.ac.uk/about-api](https://covidtracker.bsg.ox.ac.uk/about-api)\n",
    "\n",
    "The OxCGRT dataset contains information regarding different government responses in regards to social\n",
    "distancing measures. It measures the type of social distancing measure, whether or not they are recommended\n",
    "or mandated, whether they are targeted or broad (I think geographically). \n",
    "           \n",
    "## [Testing tracker data](#testtrack)\n",
    "<!-- **Website which lead me to dataset**\n",
    "[https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/](https://www.statista.com/statistics/1109066/coronavirus-testing-in-europe-by-country/) -->\n",
    "\n",
    "**Data available at:**\n",
    "[https://finddx.shinyapps.io/FIND_Cov_19_Tracker/](https://finddx.shinyapps.io/FIND_Cov_19_Tracker/)\n",
    "\n",
    "This dataset contains a time series of testing information: e.g. new (daily) tests, cumulative tests, etc. \n",
    "\n",
    "## [Delphi-epidata (currently not used)**](#delphi) which contains \n",
    "       Facebook surveys, google surveys, doctor visits, google health trends, quidel test data\n",
    "[https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html](https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html)\n",
    "\n",
    "I have not dove into this dataset too thoroughly but it contains information from facebook and google\n",
    "surveys regarding COVID as well as doctor visits; the doctor visit data attempts to make distinctions between\n",
    "those sick with the annual influenza and those with COVID.\n",
    "\n",
    "\n",
    "# [Data regularization: making things uniform](#uniformity)\n",
    "\n",
    "### [Intersection of countries](#country)\n",
    "  \n",
    "### [Time series date ranges](#time)\n",
    "\n",
    "### [Missing Values](#missingval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling function declaration <a id='generalfunctions'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Helper Functions for cleaning ----------------------#\n",
    "\n",
    "\n",
    "def column_or_index_string_reformat(df, columns=True, index=False, dt_formats=('%m/%d/%y', '%Y-%m-%d')):\n",
    "    \"\"\" Reformat column and index names. \n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    columns : bool\n",
    "    index : bool\n",
    "    \n",
    "    Notes :\n",
    "    -----\n",
    "    Change headers of columns; this needs to be updated to account for their formatting changes. \n",
    "    This function converts strings with CamelCase, underscore and space separators to lowercase words uniformly\n",
    "    separated with underscores. I.e. (hopefully!) following the correct python identifier syntax so that each column\n",
    "    can be reference as an attribute if desired. \n",
    "\n",
    "    For more on valid Python identifiers, see:\n",
    "    https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "    \"\"\"\n",
    "    if columns:\n",
    "        reformatted_column_names = []\n",
    "        for c in df.columns:\n",
    "            # handle labels which can be cast to datetime objects\n",
    "            try:\n",
    "                reformatted_column_names.append(datetime.strftime(\n",
    "                    datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "            except ValueError:\n",
    "                reformatted_column_names.append('_'.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c)\n",
    "                                                                .lower()).split()))\n",
    "        df.columns = reformatted_column_names        \n",
    "        \n",
    "    if index:\n",
    "        # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "        \n",
    "        \n",
    "        reformatted_country_names = []\n",
    "        for c in df.index.get_level_values(0):\n",
    "            reformatted_country_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "                                                        re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "                                                        .split()).title())\n",
    "        \n",
    "        reformatted_dates = pd.to_datetime(df.index.get_level_values(1)).normalize()\n",
    "        restored_columns = df.index.names\n",
    "        df = df.reset_index()\n",
    "        df.loc[:, restored_columns[0]] = reformatted_country_names\n",
    "        df.loc[:, restored_columns[1]] = reformatted_dates\n",
    "        df = df.set_index(restored_columns).sort_index()\n",
    "        \n",
    "#     if index:\n",
    "#         # only use only multi index dataframes where level=0 is country and level=1 is date. \n",
    "#         reformatted_index_names = []\n",
    "#         for c in df.index.get_level_values(0):\n",
    "#             # handle labels which can be cast to datetime objects\n",
    "#             try:\n",
    "#                 reformatted_index_names.append(datetime.strftime(\n",
    "#                     datetime.strptime(c, dt_formats[0]), format=dt_formats[1]))\n",
    "#             except ValueError:\n",
    "#                 reformatted_index_names.append(' '.join(re.sub('([A-Z][a-z]+)', r' \\1', \n",
    "#                                                         re.sub('([A-Z]+)|_|\\/', r' \\1', c).lower())\n",
    "#                                                         .split()).title())\n",
    "#         restored_column = df.index.names[0]\n",
    "#         df = df.reset_index(level=0)\n",
    "#         df.loc[:, restored_column] = reformatted_index_names\n",
    "#         df = df.set_index([restored_column, df.index]).sort_index()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def csse_daily_reports_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE daily report data from local machine. \n",
    "    \"\"\"\n",
    "    csv_different_formats_list = []\n",
    "    \n",
    "    # the actual format difference is being covered up by pd.concat which fills with Nans\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_daily_reports/*'):\n",
    "        if os.path.isdir(x):\n",
    "            df_list = []\n",
    "            for days in glob.glob(x+'/*'):\n",
    "                df = pd.read_csv(days)\n",
    "                df_list.append(df)\n",
    "            csv_different_formats_list.append(column_or_index_string_reformat(pd.concat(df_list, axis=0).reset_index(drop=True)))\n",
    "    \n",
    "    # concatenate the data\n",
    "    daily_reports_df = pd.concat(csv_different_formats_list).reset_index(drop=True)\n",
    "    # convert the date-like variable to datetime\n",
    "    daily_reports_df.loc[:, 'last_update'] = pd.to_datetime(daily_reports_df.last_update).dt.normalize()\n",
    "    # In the reporting there are duplicate values. Also, I'm aggregating by country because the other datasets\n",
    "    # are not nearly as detailed. Probably should flag this somehow. \n",
    "    daily_reports_df = daily_reports_df.drop_duplicates().groupby(['country_region','last_update']).sum()\n",
    "    # Reformat the location names and datetime index. Look at documentation above for details. \n",
    "    daily_reports_df = column_or_index_string_reformat(daily_reports_df, index=True, columns=True)\n",
    "    # name the indices and columns for later concatenation\n",
    "    daily_reports_df.index.names = ['location','date']\n",
    "    daily_reports_df.columns.names = ['csse_global_daily_reports']\n",
    "    return daily_reports_df\n",
    "    \n",
    "def csse_timeseries_reformat():\n",
    "    \"\"\" Import and concatenate all JHU CSSE time series data from local machine. \n",
    "    \"\"\"\n",
    "    global_df_list = []\n",
    "\n",
    "    for x in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_global.csv'):\n",
    "        global_tmp = column_or_index_string_reformat(pd.read_csv(x))\n",
    "        # only include the actual time series info; this removes latitude and \n",
    "        # longitude as well as other useless data.\n",
    "        global_specific_indice_list = [1] + list(range(4, global_tmp.shape[1]))\n",
    "        global_tmp = global_tmp.iloc[:,global_specific_indice_list].groupby(by='country_region').sum()\n",
    "        # keep the name of the data; i.e. 'confirmed', 'deaths', etc.\n",
    "        time_series_name = '_'.join(x.split('.')[0].split('_')[-2:][::-1])\n",
    "        global_df_list.append(global_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    # concatenate the data and name it to abide by my convention. \n",
    "    global_time_series_df = pd.concat(global_df_list, axis=1)#.reset_index(drop=True)\n",
    "    global_time_series_df.index.names = ['location','date']\n",
    "    global_time_series_df.columns.names = ['csse_global_timeseries']\n",
    "    global_time_series_df = column_or_index_string_reformat(global_time_series_df, index=True, columns=False)\n",
    "\n",
    "    # Repeat the steps above but for United States statewide data. \n",
    "    usa_df_list = []\n",
    "    for y in glob.glob('CSSEGIS_git_case_data/csse_covid_19_data/csse_covid_19_time_series/*_US.csv'):\n",
    "        usa_tmp = column_or_index_string_reformat(pd.read_csv(y))\n",
    "        try:\n",
    "            usa_tmp = usa_tmp.drop(columns='population')\n",
    "        except: \n",
    "            pass\n",
    "        usa_specific_indice_list = [6] + list(range(10, usa_tmp.shape[1]))\n",
    "        usa_tmp = usa_tmp.iloc[:,usa_specific_indice_list].groupby(\n",
    "            by='province_state').sum()\n",
    "        time_series_name = '_'.join(y.split('.')[0].split('_')[-2:][::-1])\n",
    "        usa_tmp.index.name = 'state'\n",
    "        usa_df_list.append(usa_tmp.stack().to_frame(name=time_series_name))    \n",
    "    \n",
    "    usa_time_series_df = pd.concat(usa_df_list,axis=1)#.reset_index(drop=True)\n",
    "    usa_time_series_df.index.names = ['location','date']\n",
    "    usa_time_series_df.columns.names = ['csse_us_timeseries']\n",
    "    usa_time_series_df = column_or_index_string_reformat(usa_time_series_df, index=True, columns=False)\n",
    "    \n",
    "    return global_time_series_df, usa_time_series_df\n",
    "\n",
    "\n",
    "def regularize_country_names(df):\n",
    "    \"\"\" Reformat column and index names. only works with with pandas MultiIndex for level=0.\n",
    "    \n",
    "    Parameters :\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    Notes :\n",
    "    -----\n",
    "    Different datasets have different naming conventions (for countries that go by multiple names and abbreviations).\n",
    "    This function imposes a convention on a selection of these country names.  \n",
    "    \"\"\"\n",
    "    # these lists are one-to-one. countries compared via manual inspection, unfortunately. \n",
    "    mismatch_labels_bad = ['Lao People\\'s Democratic Republic', 'Mainland China',\n",
    "                           'Occupied Palestinian Territory','Republic of Korea', 'Korea, South', \n",
    "                           'Gambia, The ', 'UK', \n",
    "                           'USA', 'Iran (Islamic Republic of)',\n",
    "                           'Bahamas, The', 'Russian Federation', 'Czech Republic', 'Republic Of Ireland',\n",
    "                          'Hong Kong Sar', 'Macao Sar', 'Uk','Us',\n",
    "                           'Congo ( Kinshasa)','Congo ( Brazzaville)',\n",
    "                           'Cote D\\' Ivoire', 'Viet Nam','Guinea- Bissau','Guinea','Usa']\n",
    "\n",
    "    mismatch_labels_good = ['Laos','China',\n",
    "                            'Palestine', 'South Korea', 'South Korea', \n",
    "                            'The Gambia', 'United Kingdom', \n",
    "                            'United States','Iran',\n",
    "                            'The Bahamas','Russia','Czechia','Ireland',\n",
    "                            'Hong Kong','Macao','United Kingdom', 'United States',\n",
    "                            'Democratic Republic Of The Congo','Republic Of The Congo',\n",
    "                            'Ivory Coast','Vietnam', 'Guinea Bissau','Guinea Bissau','United States']\n",
    "    \n",
    "    df = df.reset_index(level=0)\n",
    "    df.loc[:,'location'] = df.loc[:,'location'].replace(to_replace=mismatch_labels_bad, value=mismatch_labels_good)\n",
    "    df = df.set_index(['location', df.index])\n",
    "    return df\n",
    "\n",
    "#----------------- Helper Functions for regularization ----------------------#\n",
    "def intersect_country_index(df, country_intersection):\n",
    "    df_tmp = df.copy().reset_index(level=0)\n",
    "    df_tmp = df_tmp[df_tmp.location.isin(country_intersection)]\n",
    "    df_tmp = df_tmp.set_index(['location', df_tmp.index])\n",
    "    return df_tmp \n",
    "\n",
    "def resample_dates(df, dates):\n",
    "    df = df.loc[~df.index.duplicated(keep='first')]\n",
    "    return df.reindex(pd.MultiIndex.from_product([df.index.levels[0], dates], names=['location', 'date']), fill_value=np.nan)\n",
    "\n",
    "def make_multilevel_columns(df):\n",
    "    df.columns = pd.MultiIndex.from_product([[df.columns.name], df.columns], names=['dataset', 'features'])\n",
    "    return df\n",
    "\n",
    "def multiindex_to_table(df):\n",
    "    df_table = df.copy()\n",
    "    df_table.columns = df_table.columns.droplevel()\n",
    "    df_table.columns.names = ['']\n",
    "    df_table = df_table.reset_index()\n",
    "    return df_table\n",
    "\n",
    "#----------------- Manipulation flagging ----------------------#\n",
    "\n",
    "def flag_nan_differences(df, df_altered, suffix):\n",
    "    # Use bitwise XOR to flag the values which have been changed from NaN to something else.\n",
    "    # values which get mapped true -> false are those that are changed. \n",
    "    flag_df = df.isna() ^ df_altered.isna()\n",
    "    z1 = tuple(flag_df.columns.get_level_values(0).tolist())\n",
    "    z2 = tuple((flag_df.columns.get_level_values(1) + suffix).tolist())\n",
    "    flag_df.columns = pd.MultiIndex.from_tuples(list(zip(z1,z2)),names=['dataset', 'features'])\n",
    "    return flag_df\n",
    "\n",
    "\n",
    "#----------------- Currently Unused ----------------------#\n",
    "\n",
    "def pull_delphi_data(data_source=['fb-survey', 'google-survey', 'ght', 'quidel', 'quidelneg', 'doctor-visits'], \n",
    "                     daterange=pd.date_range(start=\"20200101\",\n",
    "                                             end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d'),\n",
    "                     **kwargs):\n",
    "    \"\"\" Pull data from https://cmu-delphi.github.io/delphi-epidata/api/\n",
    "        https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for data in data_source:\n",
    "        signal_dict = {'fb-survey':'smoothed_cli',\n",
    "                       'google-survey':'smoothed_cli',\n",
    "                       'ght':'smoothed_search',\n",
    "                       'quidel':'smoothed_tests_per_device',\n",
    "                       'quidelneg':'smoothed_pct_negative',\n",
    "                       'doctor-visits':'smoothed_cli'}\n",
    "        \n",
    "        signal = signal_dict[data]\n",
    "        if data=='quidelneg':\n",
    "            #change the proxy for the quidel signal\n",
    "            data = 'quidel'\n",
    "        for days in daterange:\n",
    "            resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "            day_data = resp.json().get('epidata', None)\n",
    "            if day_data is None:\n",
    "                pass\n",
    "            else:\n",
    "                var_number += pd.json_normalize(day_data).size\n",
    "                print(pd.json_normalize(day_data).shape)    \n",
    "                \n",
    "                \n",
    "# date_range_2020 = pd.date_range(start=\"20200101\",end=''.join(str(datetime.now().date()).split('-'))).strftime('%Y%m%d')\n",
    "# var_number = 0 \n",
    "# for days in date_range_2020:\n",
    "# #     days='20200302'\n",
    "#     resp = requests.get('https://delphi.cmu.edu/epidata/api.php?source=covidcast&data_source=doctor-visits&signal=smoothed_cli&time_type=day&geo_type=county&geo_value=*&time_values='+days)\n",
    "#     day_data = resp.json().get('epidata', None)\n",
    "#     if day_data is None:\n",
    "#         pass\n",
    "#     else:\n",
    "#         var_number += pd.json_normalize(day_data).size\n",
    "#         print(pd.json_normalize(day_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reformatting\n",
    "\n",
    "The following sections take the corresponding data set and reformat them such that the data\n",
    "is stored in a pandas DataFrame with a multiindex; level=0 -> 'location' (country or region) and\n",
    "level=1 -> date. Due to the nature of the data this is done separately for country-wide and united states-wide locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU CSSE case data\n",
    "<a id='csse'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks / to-do for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United States COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using function declared for this purpose, import and reform JHU CSSE data. Likewise, for\n",
    "the time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csse_global_daily_reports_df = csse_daily_reports_reformat().loc[:, ['confirmed','active','deaths','recovered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csse_global_timeseries_df, csse_us_timeseries_df = csse_timeseries_reformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csse_global_timeseries</th>\n",
       "      <th>global_confirmed</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Botswana</th>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea, South</th>\n",
       "      <th>2020-04-04</th>\n",
       "      <td>10156</td>\n",
       "      <td>177</td>\n",
       "      <td>6325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Sudan</th>\n",
       "      <th>2020-04-11</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angola</th>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belize</th>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "csse_global_timeseries   global_confirmed  global_deaths  global_recovered\n",
       "location     date                                                         \n",
       "Botswana     2020-02-03                 0              0                 0\n",
       "Korea, South 2020-04-04             10156            177              6325\n",
       "South Sudan  2020-04-11                 4              0                 0\n",
       "Angola       2020-04-30                27              2                 7\n",
       "Belize       2020-05-03                18              2                13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csse_global_timeseries_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHME hospital data\n",
    "<a id='ihme'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "[JHU CSSE](#csse) \n",
    "<font color='red'>\n",
    "### Has all USA states but only 32 countries which overlap with other data; stash this dataset for now. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihme_df = column_or_index_string_reformat(pd.read_csv(\n",
    "    './IHME_hospital_data/2020_04_12.02/Hospitalization_all_locs.csv').rename(columns={'location_name':'location'}))\n",
    "ihme_df.loc[:, 'date'] = pd.to_datetime(ihme_df.loc[:,'date']).dt.normalize()\n",
    "ihme_df = ihme_df.set_index(['location', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>allbed_mean</th>\n",
       "      <th>allbed_lower</th>\n",
       "      <th>allbed_upper</th>\n",
       "      <th>icubed_mean</th>\n",
       "      <th>icubed_lower</th>\n",
       "      <th>icubed_upper</th>\n",
       "      <th>inv_ven_mean</th>\n",
       "      <th>inv_ven_lower</th>\n",
       "      <th>inv_ven_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>new_icu_upper</th>\n",
       "      <th>totdea_mean</th>\n",
       "      <th>totdea_lower</th>\n",
       "      <th>totdea_upper</th>\n",
       "      <th>bedover_mean</th>\n",
       "      <th>bedover_lower</th>\n",
       "      <th>bedover_upper</th>\n",
       "      <th>icuover_mean</th>\n",
       "      <th>icuover_lower</th>\n",
       "      <th>icuover_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Latvia</th>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>129</td>\n",
       "      <td>270.894632</td>\n",
       "      <td>1.79875</td>\n",
       "      <td>1638.953846</td>\n",
       "      <td>76.680726</td>\n",
       "      <td>0.99875</td>\n",
       "      <td>458.288462</td>\n",
       "      <td>65.983892</td>\n",
       "      <td>0.6</td>\n",
       "      <td>399.817308</td>\n",
       "      <td>...</td>\n",
       "      <td>47.364161</td>\n",
       "      <td>258.945</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1425.225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.680726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>394.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slovakia</th>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>133</td>\n",
       "      <td>4.200572</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>35.165000</td>\n",
       "      <td>1.388743</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10.956250</td>\n",
       "      <td>0.994601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.401250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>69.830</td>\n",
       "      <td>6.0</td>\n",
       "      <td>378.150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United States of America</th>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>53</td>\n",
       "      <td>18.261490</td>\n",
       "      <td>15.90000</td>\n",
       "      <td>21.150000</td>\n",
       "      <td>6.203150</td>\n",
       "      <td>6.05000</td>\n",
       "      <td>6.401250</td>\n",
       "      <td>6.132350</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.401250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      v1  allbed_mean  allbed_lower  \\\n",
       "location                 date                                         \n",
       "Latvia                   2020-05-10  129   270.894632       1.79875   \n",
       "South Dakota             2020-02-27   56     0.000000       0.00000   \n",
       "Slovakia                 2020-02-10   39     0.000000       0.00000   \n",
       "West Virginia            2020-05-14  133     4.200572       0.00000   \n",
       "United States of America 2020-02-24   53    18.261490      15.90000   \n",
       "\n",
       "                                     allbed_upper  icubed_mean  icubed_lower  \\\n",
       "location                 date                                                  \n",
       "Latvia                   2020-05-10   1638.953846    76.680726       0.99875   \n",
       "South Dakota             2020-02-27      0.000000     0.000000       0.00000   \n",
       "Slovakia                 2020-02-10      0.000000     0.000000       0.00000   \n",
       "West Virginia            2020-05-14     35.165000     1.388743       0.00000   \n",
       "United States of America 2020-02-24     21.150000     6.203150       6.05000   \n",
       "\n",
       "                                     icubed_upper  inv_ven_mean  \\\n",
       "location                 date                                     \n",
       "Latvia                   2020-05-10    458.288462     65.983892   \n",
       "South Dakota             2020-02-27      0.000000      0.000000   \n",
       "Slovakia                 2020-02-10      0.000000      0.000000   \n",
       "West Virginia            2020-05-14     10.956250      0.994601   \n",
       "United States of America 2020-02-24      6.401250      6.132350   \n",
       "\n",
       "                                     inv_ven_lower  inv_ven_upper  ...  \\\n",
       "location                 date                                      ...   \n",
       "Latvia                   2020-05-10            0.6     399.817308  ...   \n",
       "South Dakota             2020-02-27            0.0       0.000000  ...   \n",
       "Slovakia                 2020-02-10            0.0       0.000000  ...   \n",
       "West Virginia            2020-05-14            0.0       8.401250  ...   \n",
       "United States of America 2020-02-24            6.0       6.300000  ...   \n",
       "\n",
       "                                     new_icu_upper  totdea_mean  totdea_lower  \\\n",
       "location                 date                                                   \n",
       "Latvia                   2020-05-10      47.364161      258.945          21.0   \n",
       "South Dakota             2020-02-27       0.000000        0.000           0.0   \n",
       "Slovakia                 2020-02-10       0.000000        0.000           0.0   \n",
       "West Virginia            2020-05-14       0.150000       69.830           6.0   \n",
       "United States of America 2020-02-24       1.401250        0.000           0.0   \n",
       "\n",
       "                                     totdea_upper  bedover_mean  \\\n",
       "location                 date                                     \n",
       "Latvia                   2020-05-10      1425.225           0.0   \n",
       "South Dakota             2020-02-27         0.000           0.0   \n",
       "Slovakia                 2020-02-10         0.000           0.0   \n",
       "West Virginia            2020-05-14       378.150           0.0   \n",
       "United States of America 2020-02-24         0.000           0.0   \n",
       "\n",
       "                                     bedover_lower  bedover_upper  \\\n",
       "location                 date                                       \n",
       "Latvia                   2020-05-10            0.0            0.0   \n",
       "South Dakota             2020-02-27            0.0            0.0   \n",
       "Slovakia                 2020-02-10            0.0            0.0   \n",
       "West Virginia            2020-05-14            0.0            0.0   \n",
       "United States of America 2020-02-24            0.0            0.0   \n",
       "\n",
       "                                     icuover_mean  icuover_lower  \\\n",
       "location                 date                                      \n",
       "Latvia                   2020-05-10     12.680726            0.0   \n",
       "South Dakota             2020-02-27      0.000000            0.0   \n",
       "Slovakia                 2020-02-10      0.000000            0.0   \n",
       "West Virginia            2020-05-14      0.000000            0.0   \n",
       "United States of America 2020-02-24      0.000000            0.0   \n",
       "\n",
       "                                     icuover_upper  \n",
       "location                 date                       \n",
       "Latvia                   2020-05-10     394.288462  \n",
       "South Dakota             2020-02-27       0.000000  \n",
       "Slovakia                 2020-02-10       0.000000  \n",
       "West Virginia            2020-05-14       0.000000  \n",
       "United States of America 2020-02-24       0.000000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihme_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWID case and test data\n",
    "<a id='source5'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Our World in Data\" dataset contains time series information on the cases, tests, and deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "owid_df = column_or_index_string_reformat(pd.read_csv('./OWID_git_and_manual_case_and_test_data/owid-covid-data.csv'))\n",
    "owid_df.loc[:, 'date'] = pd.to_datetime(owid_df.loc[:, 'date']).dt.normalize()\n",
    "owid_df = owid_df.set_index(['location','date']).sort_index()\n",
    "owid_df = regularize_country_names(owid_df)\n",
    "owid_df.columns.names = ['owid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owid</th>\n",
       "      <th>iso_code</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_cases_per_million</th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>total_tests_per_thousand</th>\n",
       "      <th>new_tests_per_thousand</th>\n",
       "      <th>tests_units</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>South Korea</th>\n",
       "      <th>2020-04-08</th>\n",
       "      <td>KOR</td>\n",
       "      <td>10384</td>\n",
       "      <td>53</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "      <td>202.539</td>\n",
       "      <td>1.034</td>\n",
       "      <td>3.901</td>\n",
       "      <td>0.156</td>\n",
       "      <td>486003.0</td>\n",
       "      <td>8699.0</td>\n",
       "      <td>9.436</td>\n",
       "      <td>0.169</td>\n",
       "      <td>cases tested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Africa</th>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>ZAF</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>units unclear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kuwait</th>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>KWT</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.088</td>\n",
       "      <td>3.512</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belarus</th>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>BLR</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.857</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Africa</th>\n",
       "      <th>2020-04-17</th>\n",
       "      <td>ZAF</td>\n",
       "      <td>2605</td>\n",
       "      <td>99</td>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>43.923</td>\n",
       "      <td>1.669</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.236</td>\n",
       "      <td>100827.0</td>\n",
       "      <td>5767.0</td>\n",
       "      <td>1.717</td>\n",
       "      <td>0.098</td>\n",
       "      <td>units unclear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "owid                    iso_code  total_cases  new_cases  total_deaths  \\\n",
       "location     date                                                        \n",
       "South Korea  2020-04-08      KOR        10384         53           200   \n",
       "South Africa 2020-03-14      ZAF           24          7             0   \n",
       "Kuwait       2020-02-27      KWT           26         15             0   \n",
       "Belarus      2020-03-16      BLR           27          6             0   \n",
       "South Africa 2020-04-17      ZAF         2605         99            48   \n",
       "\n",
       "owid                     new_deaths  total_cases_per_million  \\\n",
       "location     date                                              \n",
       "South Korea  2020-04-08           8                  202.539   \n",
       "South Africa 2020-03-14           0                    0.405   \n",
       "Kuwait       2020-02-27           0                    6.088   \n",
       "Belarus      2020-03-16           0                    2.857   \n",
       "South Africa 2020-04-17          14                   43.923   \n",
       "\n",
       "owid                     new_cases_per_million  total_deaths_per_million  \\\n",
       "location     date                                                          \n",
       "South Korea  2020-04-08                  1.034                     3.901   \n",
       "South Africa 2020-03-14                  0.118                     0.000   \n",
       "Kuwait       2020-02-27                  3.512                     0.000   \n",
       "Belarus      2020-03-16                  0.635                     0.000   \n",
       "South Africa 2020-04-17                  1.669                     0.809   \n",
       "\n",
       "owid                     new_deaths_per_million  total_tests  new_tests  \\\n",
       "location     date                                                         \n",
       "South Korea  2020-04-08                   0.156     486003.0     8699.0   \n",
       "South Africa 2020-03-14                   0.000       1017.0       93.0   \n",
       "Kuwait       2020-02-27                   0.000          NaN        NaN   \n",
       "Belarus      2020-03-16                   0.000          NaN        NaN   \n",
       "South Africa 2020-04-17                   0.236     100827.0     5767.0   \n",
       "\n",
       "owid                     total_tests_per_thousand  new_tests_per_thousand  \\\n",
       "location     date                                                           \n",
       "South Korea  2020-04-08                     9.436                   0.169   \n",
       "South Africa 2020-03-14                     0.017                   0.002   \n",
       "Kuwait       2020-02-27                       NaN                     NaN   \n",
       "Belarus      2020-03-16                       NaN                     NaN   \n",
       "South Africa 2020-04-17                     1.717                   0.098   \n",
       "\n",
       "owid                       tests_units  \n",
       "location     date                       \n",
       "South Korea  2020-04-08   cases tested  \n",
       "South Africa 2020-03-14  units unclear  \n",
       "Kuwait       2020-02-27            NaN  \n",
       "Belarus      2020-03-16            NaN  \n",
       "South Africa 2020-04-17  units unclear  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owid_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OxCGRT government response data\n",
    "<a id='oxcgrt'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual importation of data (for whatever reason this data set is different from pulling using API). This\n",
    "dataset contains time series information for the different social distancing and quarantine measures. The time\n",
    "series are recorded using flags which indicate whether or not a measure is in place, recommended, or not considered.\n",
    "In addition, there are addition flags which augment these time series; indicating whether or not the measures are targeted\n",
    "or general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df = column_or_index_string_reformat(pd.read_csv('./OxCGRT_response_data/OxCGRT_20200504.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the data, making it a multiindex dataframe which matches the others in this notebook. Also, cast\n",
    "the date-like variable as a datetime feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_df.loc[:,'date'] = pd.to_datetime(oxcgrt_df.date,format='%Y%m%d').dt.normalize()\n",
    "oxcgrt_df = oxcgrt_df.set_index(['country_name', 'date']).sort_index()\n",
    "oxcgrt_df.index.names = ['location','date']\n",
    "oxcgrt_df.columns.names = ['oxcgrt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oxcgrt</th>\n",
       "      <th>country_code</th>\n",
       "      <th>c1_school_closing</th>\n",
       "      <th>c1_flag</th>\n",
       "      <th>c2_workplace_closing</th>\n",
       "      <th>c2_flag</th>\n",
       "      <th>c3_cancel_public_events</th>\n",
       "      <th>c3_flag</th>\n",
       "      <th>c4_restrictions_on_gatherings</th>\n",
       "      <th>c4_flag</th>\n",
       "      <th>c5_close_public_transport</th>\n",
       "      <th>...</th>\n",
       "      <th>h3_contact_tracing</th>\n",
       "      <th>h4_emergency_investment_in_healthcare</th>\n",
       "      <th>h5_investment_in_vaccines</th>\n",
       "      <th>m1_wildcard</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>confirmed_deaths</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>stringency_index_for_display</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>South Sudan</th>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>SSD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.71</td>\n",
       "      <td>94.71</td>\n",
       "      <td>97.14</td>\n",
       "      <td>97.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mongolia</th>\n",
       "      <th>2020-04-15</th>\n",
       "      <td>MNG</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.88</td>\n",
       "      <td>56.88</td>\n",
       "      <td>69.76</td>\n",
       "      <td>69.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myanmar</th>\n",
       "      <th>2020-02-12</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.72</td>\n",
       "      <td>10.72</td>\n",
       "      <td>12.38</td>\n",
       "      <td>12.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Israel</th>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>ISR</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.77</td>\n",
       "      <td>86.77</td>\n",
       "      <td>85.24</td>\n",
       "      <td>85.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Serbia</th>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>SRB</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>97.14</td>\n",
       "      <td>97.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "oxcgrt                 country_code  c1_school_closing  c1_flag  \\\n",
       "location    date                                                  \n",
       "South Sudan 2020-04-16          SSD                3.0      1.0   \n",
       "Mongolia    2020-04-15          MNG                3.0      1.0   \n",
       "Myanmar     2020-02-12          MMR                0.0      NaN   \n",
       "Israel      2020-03-21          ISR                3.0      1.0   \n",
       "Serbia      2020-03-27          SRB                3.0      1.0   \n",
       "\n",
       "oxcgrt                  c2_workplace_closing  c2_flag  \\\n",
       "location    date                                        \n",
       "South Sudan 2020-04-16                   3.0      1.0   \n",
       "Mongolia    2020-04-15                   2.0      1.0   \n",
       "Myanmar     2020-02-12                   0.0      NaN   \n",
       "Israel      2020-03-21                   2.0      1.0   \n",
       "Serbia      2020-03-27                   3.0      1.0   \n",
       "\n",
       "oxcgrt                  c3_cancel_public_events  c3_flag  \\\n",
       "location    date                                           \n",
       "South Sudan 2020-04-16                      2.0      1.0   \n",
       "Mongolia    2020-04-15                      2.0      1.0   \n",
       "Myanmar     2020-02-12                      0.0      NaN   \n",
       "Israel      2020-03-21                      2.0      1.0   \n",
       "Serbia      2020-03-27                      2.0      1.0   \n",
       "\n",
       "oxcgrt                  c4_restrictions_on_gatherings  c4_flag  \\\n",
       "location    date                                                 \n",
       "South Sudan 2020-04-16                            4.0      1.0   \n",
       "Mongolia    2020-04-15                            0.0      NaN   \n",
       "Myanmar     2020-02-12                            0.0      NaN   \n",
       "Israel      2020-03-21                            4.0      1.0   \n",
       "Serbia      2020-03-27                            4.0      1.0   \n",
       "\n",
       "oxcgrt                  c5_close_public_transport  ...  h3_contact_tracing  \\\n",
       "location    date                                   ...                       \n",
       "South Sudan 2020-04-16                        2.0  ...                 1.0   \n",
       "Mongolia    2020-04-15                        1.0  ...                 2.0   \n",
       "Myanmar     2020-02-12                        0.0  ...                 0.0   \n",
       "Israel      2020-03-21                        1.0  ...                 2.0   \n",
       "Serbia      2020-03-27                        2.0  ...                 2.0   \n",
       "\n",
       "oxcgrt                  h4_emergency_investment_in_healthcare  \\\n",
       "location    date                                                \n",
       "South Sudan 2020-04-16                                    0.0   \n",
       "Mongolia    2020-04-15                                    0.0   \n",
       "Myanmar     2020-02-12                                    0.0   \n",
       "Israel      2020-03-21                                    0.0   \n",
       "Serbia      2020-03-27                                    0.0   \n",
       "\n",
       "oxcgrt                  h5_investment_in_vaccines  m1_wildcard  \\\n",
       "location    date                                                 \n",
       "South Sudan 2020-04-16                        0.0          NaN   \n",
       "Mongolia    2020-04-15                        0.0          NaN   \n",
       "Myanmar     2020-02-12                        0.0          NaN   \n",
       "Israel      2020-03-21                        0.0          NaN   \n",
       "Serbia      2020-03-27                        0.0          NaN   \n",
       "\n",
       "oxcgrt                  confirmed_cases  confirmed_deaths  stringency_index  \\\n",
       "location    date                                                              \n",
       "South Sudan 2020-04-16              4.0               0.0             94.71   \n",
       "Mongolia    2020-04-15             30.0               0.0             56.88   \n",
       "Myanmar     2020-02-12              NaN               NaN             10.72   \n",
       "Israel      2020-03-21            712.0               1.0             86.77   \n",
       "Serbia      2020-03-27            384.0               3.0            100.00   \n",
       "\n",
       "oxcgrt                  stringency_index_for_display  legacy_stringency_index  \\\n",
       "location    date                                                                \n",
       "South Sudan 2020-04-16                         94.71                    97.14   \n",
       "Mongolia    2020-04-15                         56.88                    69.76   \n",
       "Myanmar     2020-02-12                         10.72                    12.38   \n",
       "Israel      2020-03-21                         86.77                    85.24   \n",
       "Serbia      2020-03-27                        100.00                    97.14   \n",
       "\n",
       "oxcgrt                  legacy_stringency_index_for_display  \n",
       "location    date                                             \n",
       "South Sudan 2020-04-16                                97.14  \n",
       "Mongolia    2020-04-15                                69.76  \n",
       "Myanmar     2020-02-12                                12.38  \n",
       "Israel      2020-03-21                                85.24  \n",
       "Serbia      2020-03-27                                97.14  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxcgrt_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused\n",
    "#Pull the data using their API (for whatever reason this data set is different from the manual download).\n",
    "# url_to_present_date = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-01-02/' \\\n",
    "#                         + str(datetime.now().date())\n",
    "# response = requests.get(url_to_present_date)\n",
    "# response_json = response.json()\n",
    "# response_json_nested_dict = response_json['data']\n",
    "\n",
    "# response_api_df = pd.DataFrame.from_dict({(i,j): response_json_nested_dict[i][j] \n",
    "#                            for i in response_json_nested_dict.keys() \n",
    "#                            for j in response_json_nested_dict[i].keys()},\n",
    "#                        orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing tracker data\n",
    "<a id='testtrack'></a>\n",
    "[Return to table of contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset only pertains to testing data of different locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_tracker</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>penalty</th>\n",
       "      <th>population</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <th>2020-04-05</th>\n",
       "      <td>21401</td>\n",
       "      <td>311971</td>\n",
       "      <td>0.6</td>\n",
       "      <td>37742000</td>\n",
       "      <td>826.6</td>\n",
       "      <td>826.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <th>2020-04-10</th>\n",
       "      <td>0</td>\n",
       "      <td>1317887</td>\n",
       "      <td>0.8</td>\n",
       "      <td>83784000</td>\n",
       "      <td>1573.0</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>0</td>\n",
       "      <td>39000</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5850000</td>\n",
       "      <td>666.7</td>\n",
       "      <td>666.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Guatemala</th>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>0</td>\n",
       "      <td>208</td>\n",
       "      <td>1.3</td>\n",
       "      <td>17916000</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ireland</th>\n",
       "      <th>2020-03-30</th>\n",
       "      <td>12221</td>\n",
       "      <td>30213</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4938000</td>\n",
       "      <td>611.8</td>\n",
       "      <td>611.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_tracker          new_tests  tests_cumulative  penalty  population  \\\n",
       "location  date                                                           \n",
       "Canada    2020-04-05      21401            311971      0.6    37742000   \n",
       "Germany   2020-04-10          0           1317887      0.8    83784000   \n",
       "Singapore 2020-03-28          0             39000      1.3     5850000   \n",
       "Guatemala 2020-05-03          0               208      1.3    17916000   \n",
       "Ireland   2020-03-30      12221             30213      0.8     4938000   \n",
       "\n",
       "test_tracker          per100k  testsPer100k  \n",
       "location  date                               \n",
       "Canada    2020-04-05    826.6         826.6  \n",
       "Germany   2020-04-10   1573.0        1573.0  \n",
       "Singapore 2020-03-28    666.7         666.7  \n",
       "Guatemala 2020-05-03      1.2           1.2  \n",
       "Ireland   2020-03-30    611.8         611.8  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtrack_df = pd.read_csv('./TestTracker_data/Tests_20200504.csv')\n",
    "testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date']).dt.normalize()\n",
    "# testtrack_df.loc[:, 'date'] = pd.to_datetime(testtrack_df.loc[:, 'date'], format='%Y-%m-%d', errors='coerce')\n",
    "testtrack_df = testtrack_df.set_index(['country','date']).sort_index()\n",
    "testtrack_df.index.names = ['location','date']\n",
    "testtrack_df.columns.names = ['test_tracker']\n",
    "unused_columns = ['ind', 'jhu_ID.x', 'source', 'X.x', 'X.y', 'alpha2', 'alpha3',\n",
    "                  'numeric', 'latitude', 'longitude', 'jhu_ID.y', 'notes']\n",
    "\n",
    "testtrack_df = testtrack_df.drop(columns=unused_columns)\n",
    "testtrack_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delphi-epidata\n",
    "<a id='delphi'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "<font color='red'>\n",
    " ### currently unused\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_source\tname of upstream data source \n",
    "(e.g., fb-survey, google-survey, ght, quidel, doctor-visits)\tstring\n",
    "\n",
    "signal\tname of signal derived from upstream data (see notes below)\tstring\n",
    "\n",
    "time_type\ttemporal resolution of the signal (e.g., day, week)\tstring\n",
    "\n",
    "geo_type\tspatial resolution of the signal (e.g., county, hrr, msa, dma, state)\tstring\n",
    "\n",
    "time_values\ttime unit (e.g., date) over which underlying events happened\tlist of time values (e.g., 20200401)\n",
    "\n",
    "geo_value\tunique code for each location, depending on geo_type (county -> FIPS 6-4 code, HRR -> HRR number, MSA -> CBSA code,\n",
    "DMA -> DMA code, state -> two-letter state code), or * for all\tstring\n",
    "\n",
    "As of this writing, data sources have the following signals:\n",
    "\n",
    "fb-survey signal values include raw_cli, raw_ili, raw_wcli, raw_wili, and also four additional named with raw_* replaced by smoothed_* (e.g. smoothed_cli, etc).\n",
    "google-survey signal values include raw_cli and smoothed_cli.\n",
    "ght signal values include raw_search and smoothed_search.\n",
    "quidel signal values include smoothed_pct_negative and smoothed_tests_per_device.\n",
    "doctor-visits signal values include smoothed_cli.\n",
    "\n",
    "Delphi API data :\n",
    "doctor visits : 20200201-20200429 (as of 20200503)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data regularization: making things uniform <a id='uniformity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection of countries in all DataFrames\n",
    "<a id='country'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The data that will be used exists in the DataFrames : \n",
    "\n",
    "    csse_global_daily_reports_df\n",
    "    csse_global_timeseries_df\n",
    "    csse_us_timeseries_df\n",
    "    ihme_df\n",
    "    owid_df\n",
    "    oxcgrt_df\n",
    "    testtrack_df\n",
    "    \n",
    "The index (locations) were not reformatted by default; do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have all been formatted to have multi level indices and columns; the levels of the index are ```['location', 'date']``` which correspond to geographical location and day of record. I find it convenient to put these DataFrames into\n",
    "an iterable (list specifically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "    csse_us_timeseries_df, ihme_df, owid_df, oxcgrt_df, testtrack_df]\n",
    "\n",
    "global_data = [csse_global_daily_reports_df, csse_global_timeseries_df,\n",
    "                owid_df, oxcgrt_df, testtrack_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to correct the differences in naming conventions so that equivalent countries in fact have the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(all_data):\n",
    "    all_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(global_data):\n",
    "    global_data[i] = regularize_country_names(column_or_index_string_reformat(df, index=True, columns=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the subset of all countries which exist in all of the DataFrames. It is possible to\n",
    "simply concatenate the data and introduce missing values, however, I am electing to take the intersection of countries as\n",
    "to take the most \"reliable\" subset. On the contrary, for the dates I take the union; that is, the dates that exist in all datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_intersection = global_data[0].index.levels[0]\n",
    "dates_union =  global_data[0].index.levels[1].unique()\n",
    "for i in range(len(global_data)-1):\n",
    "    country_intersection = country_intersection.intersection(global_data[i+1].index.levels[0])\n",
    "    dates_union = dates_union.union(global_data[i+1].index.levels[1].unique())\n",
    "\n",
    "global_data_intersected = [intersect_country_index(df, country_intersection) for df in global_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of all dates is from 2019-12-31 00:00:00 to 2020-05-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('The range of all dates is from {} to {}'.format(dates_union.min(), dates_union.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final number of countries included is 110\n"
     ]
    }
   ],
   "source": [
    "print('The final number of countries included is {}'.format(len(country_intersection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense, because of the intersections between data; to us the u.s. time series and ihme data together but not with\n",
    "the global data. The hospital data is very useful and so it may be important to look specifically at the small number of countries it contains. Regardless; by using only the global data we can keep 110 countries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of time series dates\n",
    "<a id='time'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Want to have all time dependent data defined on the same time ranges for convenience;\n",
    "this involves two steps. 1. Initialize the new dates, 2. deal with the missing values. \n",
    "These missing values references the ones introduced by resampling or redefining the range of \n",
    "each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This redefines the time series for all variables as from December 31st 2019 to the day with most recent data\n",
    "normalized_global_data = [resample_dates(df, dates_union) for df in global_data_intersected]\n",
    "# To keep track of which data came from where, make the columns multi level with the first level labelling the dataset.\n",
    "data = pd.concat([make_multilevel_columns(df) for df in normalized_global_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id='missingval'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "The next section is concerned with the handling and imputation of missing values. The key consideration is\n",
    "to not contaminate the time series with information from the future. Because I am filling in the missing values here,\n",
    "I will be flagging the original missing values and keeping these flags as new features. Before I can compute these new features I need to think ahead towards the modeling phase of this project, that is, to take into consideration the features which\n",
    "are to be predicted.\n",
    "\n",
    "Specifically, I will be modelling and predicting case numbers. In order to not introduce linearly dependent features, I first aggregate the different case number time series and average them. I also drop other case-number-related features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = multiindex_to_table(data)\n",
    "case_features = data_table.columns[data_table.columns.str.contains('confirmed') | data_table.columns.str.contains('cases')].tolist()\n",
    "case_features_to_drop = case_features[4:6]\n",
    "case_features_to_avg = case_features[:3] + [case_features[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These features are dropped because of how similar they are to the target ['total_cases_per_million', 'new_cases_per_million']\n",
      "These features are being averaged and constitute the target variable ['confirmed', 'global_confirmed', 'total_cases', 'confirmed_cases']\n"
     ]
    }
   ],
   "source": [
    "print('These features are dropped because of how similar they are to the target', case_features_to_drop)\n",
    "print('These features are being averaged and constitute the target variable', case_features_to_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_groupby_indices = [data_table[data_table.location==country].index for country in data_table.location.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_averages = data_table.loc[country_groupby_indices[0], case_features_to_avg].mean(axis=1)\n",
    "for indices in country_groupby_indices[1:]:\n",
    "    case_averages = pd.concat((case_averages, data_table.loc[indices, case_features_to_avg].mean(axis=1)),axis=0)\n",
    "    \n",
    "data_table.loc[:, 'cases_average'] = case_averages\n",
    "data_table = data_table.drop(columns=case_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was planning on using a \"days since first case\" variable, which would equal zero until the date of the\n",
    "first case, but I believe this would correlate too strongly with the target variable. To test this assumption I'll compute it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_number_of_cases = data_table.cases_average.replace(to_replace=0., value=np.nan).dropna().index\n",
    "no_cases_dropped = data_table.loc[positive_number_of_cases,:]\n",
    "country_groupby_indices_dropped_nan = [no_cases_dropped[no_cases_dropped.location==country].index for country in no_cases_dropped.location.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I actually need this so that I can make predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32276230521120547\n"
     ]
    }
   ],
   "source": [
    "days_since = []\n",
    "for i, c in enumerate(country_groupby_indices_dropped_nan):\n",
    "    nonzero_list = list(range(len(c)))\n",
    "    zero_list = 0*np.array(list(range(len(country_groupby_indices[i])-len(c))))\n",
    "    days_since += list(zero_list)+nonzero_list\n",
    "    \n",
    "data_table.loc[:, 'days_since_first_case'] = days_since\n",
    "print(data_table.days_since_first_case.corr(data_table.cases_average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I misinterpreted the fact that days since first case is linear growth by defininition (really has the shape of a ReLU) and number of cases is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another peculiarity is the existence of two different features both called 'new_tests'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tests_tmp = data_table.loc[:, 'new_tests'].mean(1)\n",
    "data_table = data_table.drop(columns=['new_tests'])\n",
    "data_table.loc[:, 'new_tests'] = new_tests_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have aggregated and dropped the respective features, the missing values of the remaining data can be flagged and\n",
    "created into new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which values are missing\n",
    "missing_flags = data_table.isna()\n",
    "# Add a suffix to label these flag variables\n",
    "missing_flags.columns = missing_flags.columns + '_missing_flag'\n",
    "# The first two features consist of location and date; they do not miss any values and so the flag columns would be all 0's. \n",
    "# therefore they are sliced out. \n",
    "missing_flags = missing_flags.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = data_table.columns.str.contains('flag')\n",
    "\n",
    "# 'tests_units' have string like values\n",
    "data_table.loc[:, 'tests_units'] = data_table.loc[:, 'tests_units'].fillna('Missing')\n",
    "# the 'flag' columns from oxcgrt data set already use 0 as a value, so fill them separately with -1\n",
    "data_table.loc[:, data_table.columns[flag_columns]] = data_table.loc[:, data_table.columns[flag_columns]].fillna(value=-1)\n",
    "# Population is a static number but some entries are missing; its ok to backfill this.\n",
    "data_table.loc[:, 'population'] = data_table.loc[:, ['location', 'population']].replace(\n",
    "                                    to_replace=0., value=np.nan).groupby('location').fillna(method='bfill')\n",
    "\n",
    "# The remainder of the columns are filled with interpolation, then ffill \n",
    "data_interp = data_table.loc[:, data_table.columns[~flag_columns]]\n",
    "\n",
    "# Cannot fill with interpolation, because it will \"look\" into the future. \n",
    "# interpolated = data_interp.groupby(by='location', as_index=False).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "forwardfill = data_interp.groupby(by='location', as_index=False).fillna(method='ffill')\n",
    "# forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "remaining_nan = forwardfill.fillna(value=0)\n",
    "# remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "data_table.loc[remaining_nan.index, remaining_nan.columns] = remaining_nan\n",
    "data_table = data_table.drop(columns=['country_code','iso_code','m1_wildcard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OxCGRT's \"flag\" columns (which indicate a target or general response) are numerical but I will cast them as categorical\n",
    "#so that they are not affected by the upcoming numerical feature manipulations. \n",
    "# flag_columns =  data.columns.levels[1][data.columns.levels[1].str.contains('flag')]\n",
    "# multiindex_for_flag_columns = pd.MultiIndex.from_product([['oxcgrt'], flag_columns], names=['dataset', 'features'])\n",
    "# data.loc[:, multiindex_for_flag_columns] = data.loc[:, multiindex_for_flag_columns].fillna(value=-1.).astype('category')\n",
    "# data_numerical = data.copy().select_dtypes(include='number')\n",
    "\n",
    "# # Flagging every step is probably overkill\n",
    "# interpolated = data_numerical.groupby(level=0).apply(lambda x : x.interpolate(limit_direction='forward'))\n",
    "# # interpolate_flagged = flag_nan_differences(data_numerical, interpolated, '_interpolated')\n",
    "\n",
    "# forwardfill = interpolated.groupby(level=0).fillna(method='ffill')\n",
    "# # forwardfill_flagged = flag_nan_differences(interpolated, forwardfill, '_ffill')\n",
    "\n",
    "# remaining_nan = forwardfill.fillna(value=0)\n",
    "# # remaining_flagged = flag_nan_differences(forwardfill, remaining_nan, '_remaining')\n",
    "\n",
    "# backfill with interpolation, forward fill the remainder; NaNs may remain if there are only missing values\n",
    "# in their group. Therefore, still need to replace the remainder with something. Because so many of the features\n",
    "# utilize 0, I'm going to fill the remainder of missing values with -1 because nowhere do negative values appear. \n",
    "# data.loc[data_numerical.index, data_numerical.columns] = remaining_nan\n",
    "# data.loc[:, ('owid', 'tests_units')] = data.loc[:, ('owid', 'tests_units')].fillna('Missing')\n",
    "\n",
    "# still_missing_values = data.loc[:, pd.IndexSlice['test_tracker',:]].isna().sum()#.loc[pd.IndexSlice[:, #.index.levels[1]\n",
    "# throw_out_these = still_missing_values.index[still_missing_values > 0]\n",
    "\n",
    "# data = data.drop(columns=[('owid','iso_code'),\n",
    "#                          ('oxcgrt','m1_wildcard'), ('oxcgrt','country_code')]\n",
    "#                           + throw_out_these.tolist())\n",
    "# only remaining missing values are not numerical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "active                                   0.947359\n",
       "deaths                                   0.908577\n",
       "recovered                                0.744410\n",
       "global_deaths                            0.921072\n",
       "global_recovered                         0.752272\n",
       "total_deaths                             0.873763\n",
       "new_deaths                               0.824199\n",
       "total_deaths_per_million                 0.414956\n",
       "new_deaths_per_million                   0.357447\n",
       "total_tests                              0.873393\n",
       "total_tests_per_thousand                 0.147243\n",
       "new_tests_per_thousand                   0.120735\n",
       "c1_school_closing                        0.166029\n",
       "c1_flag                                  0.075713\n",
       "c2_workplace_closing                     0.199429\n",
       "c2_flag                                  0.108015\n",
       "c3_cancel_public_events                  0.163117\n",
       "c3_flag                                  0.066015\n",
       "c4_restrictions_on_gatherings            0.200374\n",
       "c4_flag                                  0.078687\n",
       "c5_close_public_transport                0.119416\n",
       "c5_flag                                  0.094506\n",
       "c6_stay_at_home_requirements             0.190247\n",
       "c6_flag                                  0.088833\n",
       "c7_restrictions_on_internal_movement     0.172235\n",
       "c7_flag                                  0.086731\n",
       "c8_international_travel_controls         0.106202\n",
       "e1_income_support                        0.267876\n",
       "e1_flag                                  0.134020\n",
       "e2_debt_contract_relief                  0.199928\n",
       "e3_fiscal_measures                       0.046338\n",
       "e4_international_support                -0.000672\n",
       "h1_public_information_campaigns          0.106972\n",
       "h1_flag                                  0.048431\n",
       "h2_testing_policy                        0.196501\n",
       "h3_contact_tracing                       0.071124\n",
       "h4_emergency_investment_in_healthcare    0.070099\n",
       "h5_investment_in_vaccines               -0.000600\n",
       "stringency_index                         0.156153\n",
       "stringency_index_for_display             0.156153\n",
       "legacy_stringency_index                  0.153853\n",
       "legacy_stringency_index_for_display      0.152027\n",
       "tests_cumulative                         0.890265\n",
       "penalty                                  0.132354\n",
       "population                               0.147017\n",
       "per100k                                  0.058223\n",
       "testsPer100k                             0.058223\n",
       "cases_average                            1.000000\n",
       "days_since_first_case                    0.323670\n",
       "new_tests                                0.525405\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.corrwith(data_table.cases_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>active</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>legacy_stringency_index</th>\n",
       "      <th>legacy_stringency_index_for_display</th>\n",
       "      <th>tests_cumulative</th>\n",
       "      <th>penalty</th>\n",
       "      <th>population</th>\n",
       "      <th>per100k</th>\n",
       "      <th>testsPer100k</th>\n",
       "      <th>cases_average</th>\n",
       "      <th>days_since_first_case</th>\n",
       "      <th>new_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12075</th>\n",
       "      <td>Sudan</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43849000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>22.86</td>\n",
       "      <td>22.86</td>\n",
       "      <td>14856.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>25500000.0</td>\n",
       "      <td>58.3</td>\n",
       "      <td>58.3</td>\n",
       "      <td>126.666667</td>\n",
       "      <td>47</td>\n",
       "      <td>3268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>Bosnia And Herzegovina</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3281000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>Botswana</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.38</td>\n",
       "      <td>12.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2352000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9660000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     location       date  active  deaths  recovered  \\\n",
       "12075                   Sudan 2020-01-23     0.0     0.0        0.0   \n",
       "596                 Australia 2020-03-12     0.0     6.0       44.0   \n",
       "1738   Bosnia And Herzegovina 2020-02-04     0.0     0.0        0.0   \n",
       "1836                 Botswana 2020-01-02     0.0     0.0        0.0   \n",
       "5765                  Hungary 2020-01-01     0.0     0.0        0.0   \n",
       "\n",
       "       global_deaths  global_recovered  total_deaths  new_deaths  \\\n",
       "12075            0.0               0.0           0.0         0.0   \n",
       "596              3.0              21.0           3.0         0.0   \n",
       "1738             0.0               0.0           0.0         0.0   \n",
       "1836             0.0               0.0           0.0         0.0   \n",
       "5765             0.0               0.0           0.0         0.0   \n",
       "\n",
       "       total_deaths_per_million  ...  legacy_stringency_index  \\\n",
       "12075                     0.000  ...                     0.00   \n",
       "596                       0.118  ...                    22.86   \n",
       "1738                      0.000  ...                    14.29   \n",
       "1836                      0.000  ...                    12.38   \n",
       "5765                      0.000  ...                     0.00   \n",
       "\n",
       "       legacy_stringency_index_for_display  tests_cumulative  penalty  \\\n",
       "12075                                 0.00               0.0      0.0   \n",
       "596                                  22.86           14856.0      1.3   \n",
       "1738                                 14.29               0.0      0.0   \n",
       "1836                                 12.38               0.0      0.0   \n",
       "5765                                  0.00               0.0      0.0   \n",
       "\n",
       "       population  per100k  testsPer100k  cases_average  \\\n",
       "12075  43849000.0      0.0           0.0       0.000000   \n",
       "596    25500000.0     58.3          58.3     126.666667   \n",
       "1738    3281000.0      0.0           0.0       0.000000   \n",
       "1836    2352000.0      0.0           0.0       0.000000   \n",
       "5765    9660000.0      0.0           0.0       0.000000   \n",
       "\n",
       "       days_since_first_case  new_tests  \n",
       "12075                      0        0.0  \n",
       "596                       47     3268.0  \n",
       "1738                       0        0.0  \n",
       "1836                       0        0.0  \n",
       "5765                       0        0.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_names in data.columns.levels[0]:\n",
    "#     dataset_datatable = multiindex_to_table(data.loc[:, pd.IndexSlice[dataset_names, :]])\n",
    "#     dataset_datatable.to_csv(dataset_names+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_table = pd.concat((data_table, missing_flags), axis=1)\n",
    "data_table.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14410, 107)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat of the above calculations for United States only data.\n",
    "\n",
    "<font color='red'>\n",
    "unfinished as of now\n",
    "</font>\n",
    "\n",
    "The United States' data merits separate investigation 1. because of the case number 2. because the IHME dataset is only really\n",
    "properly defined for the statewide description of the U.S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = [\n",
    "    csse_us_timeseries_df,\n",
    "    ihme_df,\n",
    "    owid_df,\n",
    "    oxcgrt_df,\n",
    "    testtrack_df]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
