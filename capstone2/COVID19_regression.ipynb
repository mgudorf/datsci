{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, make_scorer\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the number of new COVID19 cases with a regression model \n",
    "\n",
    "# Table of contents\n",
    "<a id='toc'></a>\n",
    "## [Introduction](#intro)\n",
    "## [Data](#data)\n",
    "## [COVID-19 Case number modeling](#model)\n",
    "## [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction and function declarations\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_slice(data, locations):\n",
    "    if type(locations)==str:\n",
    "        return data[data.location==locations]\n",
    "    else:\n",
    "        return data[data.location.isin(locations)]\n",
    "    \n",
    "def time_slice(data, start, end, indexer='time_index'):\n",
    "    if start < 0 and end < 0:\n",
    "        if start == -1:\n",
    "            start = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            start = data.loc[:, indexer].max()+start\n",
    "        if end == -1:\n",
    "            end = data.loc[:, indexer].max()\n",
    "        else:\n",
    "            end = data.loc[:, indexer].max()+end\n",
    "    return data[(data.loc[:, indexer] >= start) & (data.loc[:, indexer] <= end)]\n",
    "\n",
    "def per_country_plot(data, feature, legend=True):\n",
    "    data.set_index(['time_index', 'location']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def per_time_plot(data, feature, legend=True):\n",
    "    data.set_index(['location','time_index']).loc[:, feature].unstack().plot(legend=legend)\n",
    "    return None\n",
    "\n",
    "def country_groupby(df):\n",
    "    return [df[df.location==country].index for country in df.location.unique()]\n",
    "\n",
    "def country_search(df, country):\n",
    "    return df[df.location==country].index\n",
    "\n",
    "def column_search(df, name, return_style='loc', threshold='contains'):\n",
    "    if threshold=='contains':\n",
    "        func = df.columns.str.contains\n",
    "    else:\n",
    "        func = df.columns.str.match\n",
    "        \n",
    "    if return_style == 'loc':\n",
    "        return df.columns[func(name)]\n",
    "    elif return_style== 'iloc':\n",
    "        return np.where(func(name))[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def concatenate_4d_into_3d(splits, train_test_only=False):\n",
    "    \n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.concatenate(X_train, axis=0)\n",
    "        y_train = np.concatenate(y_train, axis=0)\n",
    "        X_validate = np.concatenate(X_validate, axis=0)\n",
    "        y_validate = np.concatenate(y_validate, axis=0)\n",
    "        X_test = np.concatenate(X_test, axis=0)\n",
    "        y_test = np.concatenate(y_test, axis=0)\n",
    "        concat_splits = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return concat_splits\n",
    "\n",
    "def transpose_for_separable2d(splits, train_test_only=False):\n",
    "    if train_test_only:\n",
    "        (X_train, y_train, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_test, y_test) \n",
    "    else:\n",
    "        (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "        X_train = np.transpose(X_train, axes=[0,2,1,3])\n",
    "        X_validate = np.transpose(X_validate, axes=[0,2,1,3])\n",
    "        X_test = np.transpose(X_test, axes=[0,2,1,3])\n",
    "        transpose_split = (X_train, y_train, X_validate, y_validate, X_test, y_test) \n",
    "    return transpose_split\n",
    "\n",
    "    \n",
    "def true_predict_plot(y_true, y_naive, y_predict, title='', suptitle='', scale=None,s=None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(20,5))\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def residual_plot(y_test, y_predict, title='', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.scatter(y_test, y_test-y_predict.ravel(), s=5)\n",
    "    ax.set_ylabel('Residual')\n",
    "    ax.set_xlabel('True value')\n",
    "    ax.grid(True)\n",
    "    return None\n",
    "\n",
    "def residual_diff_plots(y_true, y_naive, y_predict,n_days_into_future, n_countries, scale=None):\n",
    "    \n",
    "    # plot residuals in addition to the residual per country as\n",
    "    # line plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20,5), sharey=True)\n",
    "    (ax1,ax2) = axes.flatten()\n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax1)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax2)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax1)\n",
    "        residual_plot(y_true,y_predict, ax=ax2)\n",
    "    fig.suptitle('{}-day-into-future predictions'.format(n_days_into_future))\n",
    "    ax1.set_title('Country-wise differences')\n",
    "    ax2.set_title('Country-wise differences')\n",
    "    ax1.set_ylabel('True - Naive')\n",
    "    ax2.set_ylabel('True - CNN')\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries):\n",
    "    for max_date_in_window in range(start_date, time_index.max() - n_days_into_future + 2):\n",
    "        # Take all model_data with date proxy less than numerical value, leading_window_date_not_included\n",
    "        frame_data = model_data[(time_index <= max_date_in_window-1) & \n",
    "                                (time_index >= max_date_in_window-frame_size)]\n",
    "        #     print(frame_data.shape)\n",
    "        # Reshape the array such that each element along axis=0 is a time series of all feature model_data of a specific country.\n",
    "        reshaped_frame_data = frame_data.values.reshape(n_countries, frame_size, -1)\n",
    "#         print(reshaped_frame_data.shape)\n",
    "        #     print(reshaped_frame_data.shape)\n",
    "        # Truncate / pad the windows along the \"time\" axis, axis=1. (pad_sequences takes in an iterable of iterables;\n",
    "        # the first axis is always the default iteration axis. \n",
    "        # *********************** WARNING: pad_sequences converts to integers by default *********************\n",
    "        resized_frame_data = pad_sequences(reshaped_frame_data, maxlen=frame_size, dtype=np.float64)\n",
    "        frame_data_4D = resized_frame_data[np.newaxis, :, :, :]\n",
    "        if max_date_in_window == start_date:\n",
    "            print('Starting with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "            X = frame_data_4D.copy()\n",
    "        else:\n",
    "            X = np.concatenate((X, frame_data_4D),axis=0)\n",
    "    print('Ending with frame ranging time_index values:', max_date_in_window-frame_size, max_date_in_window - 1)\n",
    "    y = target_data.values.reshape(-1, time_index.nunique()).transpose()[-X.shape[0]:,:]\n",
    "    return X, y\n",
    "\n",
    "def split_Xy(X, y, frame_size, n_validation_frames, n_test_frames, train_test_only=False, model_type='cnn'):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the indices for the train-validate-test splits for when the predictors are put in a 2-d format.\n",
    "    train_indices = list(range(n_countries*0, n_countries*(len(X)-(n_validation_frames+n_test_frames))))\n",
    "    validate_indices = list(range(n_countries*(len(X)-(n_validation_frames+n_test_frames)), n_countries*(len(X)-n_test_frames)))\n",
    "    test_indices = list(range(n_countries*(len(X)-n_test_frames), n_countries*len(X)))\n",
    "    indices = (train_indices, validate_indices, test_indices)\n",
    "\n",
    "    # Note that the last frame (date_range) that exists in X has already been determined by the choice of the number\n",
    "    # of steps to predict in the future, this is only slicing the frames. \n",
    "    if train_test_only:\n",
    "        X_train= X[:-n_test_frames,:,:,:]\n",
    "        y_train =  y[:-n_test_frames,:]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_train= X[:-(n_validation_frames+n_test_frames),:,:,:]\n",
    "        y_train =  y[:-(n_validation_frames+n_test_frames),:]\n",
    "        X_validate = X[-(n_validation_frames+n_test_frames):-n_test_frames, :, :, :]\n",
    "        y_validate = y[-(n_validation_frames+n_test_frames):-n_test_frames, :]\n",
    "        X_test = X[-n_test_frames:, :, :, :] \n",
    "        y_test = y[-n_test_frames:, :]\n",
    "        splits =  (X_train, y_train, X_validate, y_validate,\n",
    "                   X_test, y_test)\n",
    "\n",
    "    return splits, indices\n",
    "\n",
    "\n",
    "def flatten_Xy(splits):\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    X_train_flat =np.concatenate(X_train.reshape(X_train.shape[0], X_train.shape[1], -1), axis=0)\n",
    "    X_validate_flat =np.concatenate(X_validate.reshape(X_validate.shape[0], X_validate.shape[1], -1), axis=0)\n",
    "    X_test_flat =np.concatenate(X_test.reshape(X_test.shape[0], X_test.shape[1], -1), axis=0)\n",
    "    y_train_flat = y_train.ravel()\n",
    "    y_validate_flat = y_validate.ravel()\n",
    "    y_test_flat = y_test.ravel()\n",
    "    flat_splits = (X_train_flat , y_train_flat , X_validate_flat , y_validate_flat , X_test_flat , y_test_flat)\n",
    "    return flat_splits\n",
    "\n",
    "def model_analysis(y_true, y_naive, y_predict, n_countries, title='',suptitle='',figname=None, scale=None, s=None):\n",
    "    print('There were {} negative predictions'.format(len(y_predict[y_predict<0])))\n",
    "    #     y_predict[y_predict<0]=0\n",
    "    # compute scores \n",
    "    mse_naive = mean_squared_error(y_true.ravel(), y_naive.ravel())\n",
    "    mse_predict = mean_squared_error(y_true.ravel(), y_predict)\n",
    "    r2_naive = explained_variance_score(y_true.ravel(), y_naive.ravel())\n",
    "    r2_predict = explained_variance_score(y_true.ravel(), y_predict)\n",
    "\n",
    "    print('{}-step MSE [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, mse_naive, mse_predict))\n",
    "    print('{}-step R^2 [Naive, {}] = [{},{}]'.format(\n",
    "    n_days_into_future,title, r2_naive, r2_predict))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    (ax1,ax2,ax3,ax4) = axes.flatten()\n",
    "    if scale == 'log':\n",
    "        ymax = np.max([np.log(1+y_true).max(), np.log(1+y_predict).max()])\n",
    "        ax1.scatter(np.log(y_true+1), np.log(y_naive+1), s=s,alpha=0.7)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(np.log(y_true+1), np.log(y_predict+1), s=s,alpha=0.7)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    else:\n",
    "        ymax = np.max([y_true.max(), y_predict.max()])\n",
    "        ax1.scatter(y_true, y_naive, s=s)\n",
    "        ax1.plot([0, ymax], [0, ymax],color='r')\n",
    "        ax2.scatter(y_true, y_predict, s=s)\n",
    "        ax2.plot([0, ymax], [0, ymax],color='r')\n",
    "    \n",
    "    ax1.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_naive,3)),fontsize=14)\n",
    "    ax1.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_naive,3)),fontsize=14)\n",
    "    ax1.set_xlabel('True value')\n",
    "    ax1.set_ylabel('Predicted value')\n",
    "    ax1.set_title('Naive model')\n",
    "             \n",
    "    ax2.text(0.0, ymax,'$MSE$ = {}'.format(np.round(mse_predict,3)),fontsize=14)\n",
    "    ax2.text(0.0, 0.9*ymax,'$R^2$ = {}'.format(np.round(r2_predict,3)),fontsize=14)\n",
    "    ax2.set_xlabel('True value')\n",
    "    ax2.set_ylabel('Predicted value')\n",
    "    ax2.set_title(title)\n",
    "    ax1.grid(True)\n",
    "    ax2.grid(True)\n",
    "    plt.suptitle(suptitle)\n",
    "\n",
    "    \n",
    "    xrange = range(len(y_true))\n",
    "    if scale=='log':\n",
    "        residual_plot(np.log(y_true+1),np.log(y_naive+1), ax=ax3)\n",
    "        residual_plot(np.log(y_true+1),np.log(y_predict+1), ax=ax4)\n",
    "    else:\n",
    "        residual_plot(y_true,y_naive, ax=ax3)\n",
    "        residual_plot(y_true,y_predict, ax=ax4)\n",
    "    ax3.set_title('')\n",
    "    ax4.set_title('')\n",
    "    ax3.set_ylabel('Residual')\n",
    "    ax4.set_ylabel('Residual')\n",
    "    ax3.grid(True)\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    if figname is not None:\n",
    "        plt.savefig(figname, bbox_inches='tight')\n",
    "        \n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_Xy(splits, feature_range=(0., 1.0), normalization_method='minmax',\n",
    "                        train_test_only=False, feature_indices=None):\n",
    "    \"\"\" Split into training, validation and test data.\n",
    "    Normalize with respect to some absolute max, just choose 2*absolute max of training set. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    min_, max_ = feature_range\n",
    "    (X_train, y_train, X_validate, y_validate, X_test, y_test) = splits\n",
    "    feature_minima = X_train[:,:,:,:].min((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_maxima = 2*X_train[:,:,:,:].max((0,1,2))[np.newaxis,np.newaxis,np.newaxis,:]\n",
    "    feature_denominator = feature_maxima-feature_minima\n",
    "    # get the shape of each split's array, so that array arithmetic can be done. \n",
    "    train_tile_shape = np.array(np.array(X_train.shape)/np.array(feature_maxima.shape),int)\n",
    "    validate_tile_shape = np.array(np.array(X_validate.shape)/np.array(feature_maxima.shape),int)\n",
    "    test_tile_shape = np.array(np.array(X_test.shape)/np.array(feature_maxima.shape),int)\n",
    "\n",
    "    # using X - X_min / (X_max-X_min). Form denominator\n",
    "    train_denominator = np.tile(feature_denominator, train_tile_shape)\n",
    "    validate_denominator = np.tile(feature_denominator, validate_tile_shape)\n",
    "    test_denominator = np.tile(feature_denominator, test_tile_shape)\n",
    "    \n",
    "    # and then the minima.\n",
    "    train_minima = np.tile(feature_minima,train_tile_shape)\n",
    "    validate_minima = np.tile(feature_minima,validate_tile_shape)\n",
    "    test_minima = np.tile(feature_minima,test_tile_shape)\n",
    "    \n",
    "    # factor of 1/max_ accounts for absolute maximum that is outside of the data (i.e. potential future values). \n",
    "    X_train_scaled = (max_-min_)*(X_train - train_minima)/train_denominator\n",
    "    X_validate_scaled = (max_-min_)*(X_validate - validate_minima)/validate_denominator\n",
    "    X_test_scaled = (max_-min_)*(X_test - test_minima)/test_denominator\n",
    "\n",
    "    \n",
    "    scaled_splits = (X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test) \n",
    "    scaling_arrays =  (feature_maxima, feature_minima, feature_denominator)\n",
    "\n",
    "    return scaled_splits, scaling_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "<a id='data'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Originally I was using a single day's values to predict the next day. This did not sufficiently leverage the knowledge from the\n",
    "past and so I changed it per an outside recommendation to include recent past history (chosen to be 4 weeks) as the set of regressors or predictors. I decided to keep the different country data separate; in other words, an alternative choice of regressor formatting would be to use all of the countries single day data to predict the next day. Going even further I could\n",
    "have used all countries recent past to predict the future but this would increase the size of the dataset to a point where cloud computing would likely be necessary (I think).\n",
    "\n",
    "In regards to normalization, I had tested the usage of MinMax [0,1] normalization using only the in-frame values, using the entire history of separate countries values up to that time frame as well as normalizing using all country data. I did \n",
    "not notice any performance increase, in fact, I only ever saw performance decrease, albeit with a small sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I feel like it simply clutters the notebook, I have removed the exploration / testing with respect to which features to include. After this experimentation, I have found it is best to keep the number of features at a minimum, using only the ```new_cases_per_million``` feature, its logarithm, the ```government_response_index``` feature, and their rolling averages. The other time dependent features which I originally thought to be important, such as deaths, recovered, and tests actually hurt performance. I believe that there are two separate reasons for this: for deaths and recovered, these are delayed with respect to case numbers. For the tests, it is because the testing variable does not have uniform units with respect to different countries; some countries report the number of people tested, some report the total number of tests, etc. I could account for this difference by splitting the test variable by its categories, but I believe that this is an unwise decision.\n",
    "\n",
    "In regards to capturing the time dependence, I decided that I should use 28 days worth of time series data for each prediction, keeping the countries stratified. Alternatives would be to use single day, all countries or 28 days, all countries to predict. My choice was based on focusing on time dependence, but also keeping the column number to a minimum due to the relatively small number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbose way of dropping all but the three desired features and their rolling averages. \n",
    "data = pd.read_csv('regression_data.csv', index_col=0)\n",
    "tmp = pd.read_csv('regression_data_full.csv', index_col=0)\n",
    "data = data.drop(columns=['date'])\n",
    "data = pd.concat((data,tmp.loc[:, column_search(tmp,'government_response')]),axis=1)\n",
    "data = data.drop(columns=column_search(data,'test'))\n",
    "data = data.drop(columns=column_search(data,'deaths'))\n",
    "data = data.drop(columns=column_search(data,'recovered'))\n",
    "data = data.drop(columns=column_search(data,'missing'))\n",
    "data = data.drop(columns=column_search(data,'std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the quantities used for the target variable, the time index used for splitting, etc.\n",
    "model_data = data.copy().iloc[:, 1:]\n",
    "new_cases_index = column_search(model_data,'new_cases_per_million', threshold='match', return_style='iloc')[0]\n",
    "n_countries = data.location.nunique()\n",
    "target_data = data.new_cases_per_million\n",
    "time_index = data.time_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been decided upon, I begin splitting it into training, validation, and testing sets. Because this is the final performance test I technically only really need to split into training and testing, but I am recycling functions from other notebooks. The dimensionality of the data (columns) is decided by the number of days used to predict the next day. I use a 28 day window for this purpose, and use only the frames which entirely overlap with the dataset. In other words, I ensure that the entire frame overlaps with the data set. For example, for a 28-day window, if I started on day 1 then 27 days of data would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = 28\n",
    "start_date = frame_size + data.time_index.min()\n",
    "\n",
    "n_validation_frames = 7\n",
    "n_test_frames = 1\n",
    "n_days_into_future = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I elect not to rescale the data as I did in the neural network notebooks. It seemed to only ever hurt the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with frame ranging time_index values: 0 27\n",
      "Ending with frame ranging time_index values: 154 181\n"
     ]
    }
   ],
   "source": [
    "X, y = create_Xy(model_data, target_data, time_index, start_date, frame_size, n_days_into_future, n_countries)\n",
    "splits, indices = split_Xy(X, y, frame_size, n_validation_frames, n_test_frames)\n",
    "\n",
    "flat_splits = flatten_Xy(splits)\n",
    "(X_regression_train, y_regression_train, X_regression_validate,\n",
    " y_regression_validate, X_regression_test, y_regression_test) = flat_splits\n",
    "\n",
    "(train_indices, validate_indices, test_indices) = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the values for the naive baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_naive_slicing = np.concatenate(X.reshape(X.shape[0], X.shape[1], -1), axis=0)\n",
    "n_features = X.shape[-1]\n",
    "last_day_new_cases_index = np.ravel_multi_index([[frame_size-1],[new_cases_index]],(frame_size, n_features))\n",
    "y_train_naive = X_for_naive_slicing[train_indices, last_day_new_cases_index].ravel()\n",
    "y_validate_naive =  X_for_naive_slicing[validate_indices, last_day_new_cases_index].ravel()\n",
    "y_test_naive =  X_for_naive_slicing[test_indices, last_day_new_cases_index].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the validation and training sets into the \"final\" training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regression = np.concatenate((X_regression_train, X_regression_validate),axis=0)\n",
    "y_regression = np.concatenate((y_regression_train, y_regression_validate),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression model \n",
    "<a id='model'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "In order to use mean squared error as the metric, I need to use RidgeCV and pass the scoring as a function. The automatically performs cross validation for the regularization strength $\\alpha$ but because of the complications from time dependence I only use a single fold for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "ridge = RidgeCV(scoring=scorer, cv=[[train_indices, validate_indices]]) \n",
    "_ = ridge.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit new ridge regression instance with the best alpha value, using both the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = ridge.alpha_\n",
    "ridge = Ridge(alpha=best_alpha)\n",
    "_ = ridge.fit(X_regression, y_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and conclusion\n",
    "<a id='conclusion'></a>\n",
    "[Return to table of contents](#toc)\n",
    "\n",
    "Check the performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 5141 negative predictions\n",
      "1-step MSE [Naive, Ridge Regression model] = [120.41959446037183,85.41108391737305]\n",
      "1-step R^2 [Naive, Ridge Regression model] = [0.91597417503742,0.9403881624713997]\n"
     ]
    }
   ],
   "source": [
    "y_predict = ridge.predict(X_regression).ravel()\n",
    "y_true = y_regression\n",
    "y_naive = np.concatenate((y_train_naive, y_validate_naive),axis=0)\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge Regression model',\n",
    "               suptitle='Naive baseline vs. predictions of training set',\n",
    "              figname='regression_train_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything has been validated, we may perform the final prediction on the hold-out set. The idea here is to not use this to tune the model. I.e. this is the point of no return, otherwise I risk introducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_regression_test\n",
    "y_naive = y_test_naive\n",
    "y_predict = ridge.predict(X_regression_test).ravel()\n",
    "\n",
    "# model_analysis(y_regression_test, y_test_naive, y_predict_test, n_countries, title='Ridge', suptitle='Scaled predictor test set performance')\n",
    "model_analysis(y_true, y_naive, y_predict, n_countries, title='Ridge regression model',\n",
    "               suptitle='Naive baseline vs. predictions of testing set',\n",
    "              figname='regression_test_performance.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't perform exceptionally better, but I suspect the naive baseline's performance is due to the 1-day forecasting interval. The future work would be to see how these results generalize to longer forecasting intervals. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
