{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from matplotlib.patches import Rectangle\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a id='intro'></a>\n",
    "\n",
    "This notebook uses a variety of different COVID-19 related datasets to explore the behavior\n",
    "of the multiple time series'. This notebook also creates new features that attempt to encapsulate the\n",
    "time dependent (and time delayed) nature of the problem; these will be used during the model creation\n",
    "project which makes time dependent forecasting models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "## [Function definitions](#generalfunctions)\n",
    "\n",
    "## [Data](#imports)\n",
    "\n",
    "## [Exploratory Data Analysis](#EDA)\n",
    "\n",
    "## [Feature production](#newfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions <a id='generalfunctions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_rolling_values(data, features, roll_widths):\n",
    "    # only do the averaging over one country at a time, but store in single array so that\n",
    "    # we can assign it to j\n",
    "    datatmp = data.loc[:, features].copy()\n",
    "#     print(data.columns.get_level_values(1))\n",
    "    featureind = datatmp.columns.get_level_values(1)\n",
    "    new_feature_df_list = []\n",
    "\n",
    "    for window in roll_widths:\n",
    "        # order the dataframe so date is index, backfill in the first roll_width values \n",
    "        rollmean = pd.DataFrame(datatmp.reset_index(\n",
    "            level=0).groupby(by='location').rolling(window).mean().fillna(method='backfill'))\n",
    "        rollstd = pd.DataFrame(datatmp.reset_index(\n",
    "            level=0).groupby(by='location').rolling(window).std().fillna(method='backfill'))    \n",
    "        new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "        rmind = featureind+'_rolling_mean_'+str(window)\n",
    "        rsind = featureind+'_rolling_std_'+str(window)\n",
    "        window_ind = rmind.append(rsind)\n",
    "        zip_work_around = tuple(datatmp.columns.get_level_values(0).tolist()+datatmp.columns.get_level_values(0).tolist())\n",
    "        zip_work_around2 =  tuple(window_ind.tolist())\n",
    "        multind_tuples = list(zip(zip_work_around, zip_work_around2))\n",
    "        multind = pd.MultiIndex.from_tuples(multind_tuples, names=['dataset', 'features'])\n",
    "        new_features.columns = multind\n",
    "        new_feature_df_list.append(new_features)\n",
    "    return pd.concat([data, pd.concat(new_feature_df_list,axis=1)], axis=1)\n",
    "\n",
    "def tsplot(data, roll_width, **kw):\n",
    "    rollmean = datatmp.rolling(roll_width).mean().fillna(method='backfill').values.ravel()\n",
    "    rollstd  = datatmp.rolling(roll_width).std().fillna(method='backfill').values.ravel()\n",
    "    cis = (rollmean - rollstd, rollmean + rollstd)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(range(len(datatmp)), cis[0], cis[1], alpha=0.5)\n",
    "    ax.plot(range(len(datatmp)), rollmean, color='k', **kw)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data <a id='imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>active</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>global_deaths</th>\n",
       "      <th>global_recovered</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>...</th>\n",
       "      <th>legacy_stringency_index_missing_flag</th>\n",
       "      <th>legacy_stringency_index_for_display_missing_flag</th>\n",
       "      <th>tests_cumulative_missing_flag</th>\n",
       "      <th>penalty_missing_flag</th>\n",
       "      <th>population_missing_flag</th>\n",
       "      <th>per100k_missing_flag</th>\n",
       "      <th>testsPer100k_missing_flag</th>\n",
       "      <th>cases_average_missing_flag</th>\n",
       "      <th>days_since_first_case_missing_flag</th>\n",
       "      <th>new_tests_missing_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>Bosnia And Herzegovina</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12807</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10078</th>\n",
       "      <td>Qatar</td>\n",
       "      <td>2020-05-03</td>\n",
       "      <td>13326.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.777</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Bahrain</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8861</th>\n",
       "      <td>Norway</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     location        date   active  deaths  recovered  \\\n",
       "1685   Bosnia And Herzegovina  2020-02-16      0.0     0.0        0.0   \n",
       "12807    United Arab Emirates  2020-03-21      0.0     2.0       38.0   \n",
       "10078                   Qatar  2020-05-03  13326.0    12.0     1534.0   \n",
       "907                   Bahrain  2020-01-25      0.0     0.0        0.0   \n",
       "8861                   Norway  2020-02-10      0.0     0.0        0.0   \n",
       "\n",
       "       global_deaths  global_recovered  total_deaths  new_deaths  \\\n",
       "1685             0.0               0.0           0.0         0.0   \n",
       "12807            2.0              38.0           0.0         0.0   \n",
       "10078           12.0            1664.0           8.0         0.0   \n",
       "907              0.0               0.0           0.0         0.0   \n",
       "8861             0.0               0.0           0.0         0.0   \n",
       "\n",
       "       total_deaths_per_million  ...  legacy_stringency_index_missing_flag  \\\n",
       "1685                      0.000  ...                                 False   \n",
       "12807                     0.000  ...                                 False   \n",
       "10078                     2.777  ...                                 False   \n",
       "907                       0.000  ...                                 False   \n",
       "8861                      0.000  ...                                 False   \n",
       "\n",
       "       legacy_stringency_index_for_display_missing_flag  \\\n",
       "1685                                              False   \n",
       "12807                                             False   \n",
       "10078                                             False   \n",
       "907                                               False   \n",
       "8861                                              False   \n",
       "\n",
       "       tests_cumulative_missing_flag  penalty_missing_flag  \\\n",
       "1685                            True                  True   \n",
       "12807                          False                 False   \n",
       "10078                          False                 False   \n",
       "907                             True                  True   \n",
       "8861                            True                  True   \n",
       "\n",
       "      population_missing_flag  per100k_missing_flag  \\\n",
       "1685                     True                  True   \n",
       "12807                   False                 False   \n",
       "10078                   False                 False   \n",
       "907                      True                  True   \n",
       "8861                     True                  True   \n",
       "\n",
       "       testsPer100k_missing_flag  cases_average_missing_flag  \\\n",
       "1685                        True                       False   \n",
       "12807                      False                       False   \n",
       "10078                      False                       False   \n",
       "907                         True                       False   \n",
       "8861                        True                       False   \n",
       "\n",
       "       days_since_first_case_missing_flag  new_tests_missing_flag  \n",
       "1685                                False                    True  \n",
       "12807                               False                   False  \n",
       "10078                               False                   False  \n",
       "907                                 False                    True  \n",
       "8861                                False                    True  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # key words account for multilevel indices\n",
    "# data = pd.read_csv('data.csv', index_col=[0,1], header=[0,1])\n",
    "# data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = data.columns[data.columns.str.contains('flag')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = data.iloc[:,2:].columns.difference(flag_columns.tolist() + ['tests_units'])\n",
    "features_groupby = data.iloc[:,2:].columns.difference(flag_columns.tolist() + ['tests_units']).join(['location'],how='outer')\n",
    "roll_widths = [2, 3, 5, 7, 14]\n",
    "datatmp = data.loc[:, features_groupby]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_df_list = []\n",
    "for window in roll_widths:\n",
    "    # order the dataframe so date is index, backfill in the first roll_width values \n",
    "    rollmean = pd.DataFrame(datatmp.groupby(by='location').rolling(window).mean().fillna(value=0.))\n",
    "    rollstd = pd.DataFrame(datatmp.groupby(by='location').rolling(window).std().fillna(value=0.))    \n",
    "    new_features = pd.concat((rollmean, rollstd), axis=1)\n",
    "    rmind = features +'_rolling_mean_' + str(window)\n",
    "    rsind = features +'_rolling_std_' + str(window)\n",
    "    new_cols = rmind.append(rsind)\n",
    "    new_features.columns = new_cols\n",
    "    new_feature_df_list.append(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_features = pd.concat((data, pd.concat(new_feature_df_list, axis=1).reset_index(drop=True)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_features.to_csv('modeling_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis<a id='EDA'></a>\n",
    "Ideas for the inclusion or creation of new columns.\n",
    "\n",
    "Moving averages\n",
    "fourier\n",
    "signal\n",
    "flags for lots of different things\n",
    "\n",
    "hardest hit countries\n",
    "\n",
    "days since\n",
    "\n",
    "extrapolated, actual, interpolated\n",
    "\n",
    "which dataset it came from\n",
    "\n",
    "humans view, interpret and forecast things in a way which are not available to robots. \n",
    "data driven, time dependent manner of modeling. Really trying to encapsulate the time dependence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_rolling_values(data, features, roll_widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USA COVID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_response_dates = start_end_df.min(axis=1).sort_index()\n",
    "first_response_dates.head(10)\n",
    "\n",
    "first_case_dates = test_multiindex_df.reset_index(level=1).groupby(level=0).Date.min().sort_index()\n",
    "first_case_dates.head(10)\n",
    "\n",
    "dates_with_test_data = test_multiindex_df.tests_cumulative.dropna()\n",
    "dates_with_test_data.head()\n",
    "\n",
    "min_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "\n",
    "first_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.min()\n",
    "last_testing_dates = test_multiindex_df.tests_cumulative.dropna().reset_index(level=1).groupby(level=0).Date.max()\n",
    "\n",
    "first_testing_dates.reset_index()\n",
    "\n",
    "# convert entire dataframe to index so it can be used to slice testing data, dataframe\n",
    "first_tmp =  first_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "last_tmp =  last_testing_dates.reset_index().set_index(['Country','Date'])\n",
    "first_tmp.head()\n",
    "\n",
    "test_min = test_multiindex_df.loc[first_tmp.index, :]\n",
    "test_max = test_multiindex_df.loc[last_tmp.index, :]\n",
    "\n",
    "# reset index so we can subtract datetime variables.\n",
    "test_max_reset = test_max.reset_index(level=1)\n",
    "test_min_reset = test_min.reset_index(level=1)\n",
    "time_differential = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "testing_rates = np.log(test_max_reset.tests_cumulative / test_min_reset.tests_cumulative)# / time_intervals\n",
    "\n",
    "\n",
    "test_final_test_initial_time_intervals = (test_max_reset.Date - test_min_reset.Date).dt.days\n",
    "\n",
    "case_response_differential = (first_case_dates-first_response_dates).dt.days\n",
    "\n",
    "late_response = case_response_differential < 0\n",
    "late_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_inspect = ['Michigan', 'Georgia', 'New York', 'Texas']\n",
    "\n",
    "dead=us_deaths[us_deaths['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed=us_cases[us_cases['Province_State'].isin(states_to_inspect)].groupby(by='Province_State').sum()\n",
    "confirmed.head()\n",
    "\n",
    "\n",
    "since_first_case_normalized_u = u.replace(to_replace=[0,0.], value=np.nan)\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n",
    "death_rate_df = since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].copy()\n",
    "death_rate_normalized = 100 * since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:].values / since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "death_rate_df.loc[:, :] = death_rate_normalized\n",
    "\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].values\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:] / since_first_case_normalized_u.loc[(states_to_inspect,'Dead'), :].iloc[:,6:]\n",
    "\n",
    "since_first_case_normalized_u.loc[(states_to_inspect,'Confirmed'), :].iloc[:,6:].apply(np.log10).transpose().plot()\n",
    "\n",
    "time_series_df = since_first_case_normalized_u#.iloc[:, 6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_case_dates.astype('category').cat.codes.plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(new_feature_df_list,ignore_index=False).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "death_rate_df.transpose().plot().legend(bbox_to_anchor=(1, 1))\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "plt.grid(True, axis='both')\n",
    "plt.title('Death rate by state')\n",
    "plt.savefig('death_rate_NY_MI_GA.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax,ax2) = plt.subplots(1, 2, sharey=True,  figsize=(20,5), dpi=200)\n",
    "confirmed.loc[:, '2/21/20':].transpose().plot(ax=ax).legend(bbox_to_anchor=(0.2, 1))\n",
    "dead.loc[:, '2/21/20':].transpose().plot(ax=ax2).legend(bbox_to_anchor=(0.2, 1))\n",
    "ax.set_yscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax.set_title('Number of confirmed cases vs. time')\n",
    "ax2.set_title('Number of diseased vs. time')\n",
    "ax.grid(True, axis='both')\n",
    "ax2.grid(True, axis='both')\n",
    "plt.savefig('cases_vs_dead_comparison_GA_NY_MI.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5_counties(state_df, state_name):\n",
    "    state = state_df[(state_df.Province_State==state_name)]\n",
    "    state = state.drop(columns=['UID','iso2','iso3','code3','FIPS','Country_Region','Lat','Long_','Combined_Key','Province_State'])\n",
    "    top5_counties = state.groupby(by='Admin2').sum().sum(axis=1).sort_values(ascending=False)[:5].index.tolist()\n",
    "    state_info = state[state.Admin2.isin(top5_counties)].set_index('Admin2').transpose()\n",
    "    state_info.columns.name = 'County'\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global COVID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_recovered_dates_only = global_recovered.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_confirmed_dates_only = global_confirmed.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "global_dead_dates_only = global_dead.set_index('Country/Region').loc[:, '1/22/20':].groupby(level=0).sum()\n",
    "\n",
    "global_dead['type']='Dead'\n",
    "global_confirmed['type']='Confirmed'\n",
    "global_recovered['type']='Recovered'\n",
    "\n",
    "dead=global_dead[global_dead['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "confirmed=global_confirmed[global_confirmed['Country/Region'].isin(['Germany', 'Italy', 'US'])].set_index('Country/Region').loc[:,'1/22/20':]#.iloc[:, 4:].transpose().columns\n",
    "\n",
    "global_dead = global_dead.sort_index(axis=1)\n",
    "global_confirmed = global_confirmed.sort_index(axis=1)\n",
    "global_recovered = global_recovered.sort_index(axis=1)\n",
    "\n",
    "skr = global_confirmed.groupby('Country/Region').sum().iloc[143, :].loc['1/22/20':'4/28/20']\n",
    "skr.head()\n",
    "\n",
    "top10 = global_confirmed.groupby('Country/Region').sum().loc[:, '1/22/20':'4/28/20'].sort_values(by='4/28/20').iloc[-10:, :]\n",
    "skr = global_confirmed.groupby('Country/Region').sum().loc['Korea, South', '1/22/20':'4/28/20']\n",
    "\n",
    "top10_and_south_korea = pd.concat((top10, skr.to_frame(name='South Korea').transpose()),axis=0).sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "for i, country_time_series in enumerate(top10_and_south_korea.replace(to_replace=[0,0.], value=np.nan).values):\n",
    "    nan_count = np.sum(np.isnan(country_time_series))\n",
    "    days_since_first = np.roll(country_time_series, -nan_count)\n",
    "    plt.plot(days_since_first, label=top10_and_south_korea.index[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "global_dead_dates_only\n",
    "\n",
    "dsum = global_dead_dates_only.sum()\n",
    "csum = global_confirmed_dates_only.sum()\n",
    "drsum = 100*dsum/csum\n",
    "drsum.plot()\n",
    "_ = plt.xlabel('Date')\n",
    "_ = plt.ylabel('Death Rate (%)')\n",
    "_ = plt.title('Average global death rate vs. time')\n",
    "plt.grid(True, axis='both')\n",
    "plt.savefig('death_rate_global.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature production <a id='newfeatures'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function ```append_rolling_values``` is not working. Need to compute rolling averages for each\n",
    "countries time series' individually but want to store them in the multi index DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before interpolation and backfilling, I used to prune countries which did not have cases prior\n",
    "to responses (i.e. \"early responders\" were not included)\n",
    "To make my life easier, I'm only taking data which had cases before all government mandates so the rates before and after are well defined. We can think of these as being \"late responders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_case_dates = case_df.reset_index().set_index(['Country','date']).total_cases.replace(\n",
    "                           to_replace=0,value=np.nan).dropna().reset_index(level=1).groupby(level=0).date.min()\n",
    "\n",
    "first_response_dates = response_df.min(axis=1)\n",
    "tmp = response_df.copy()\n",
    "dt = pd.DataFrame(np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1])))\n",
    "diff_df = tmp - np.tile(first_case_dates.values.reshape(-1,1),(1, response_df.shape[1]))\n",
    "num_miss=diff_df.where(diff_df > pd.Timedelta(days=0)).isna().sum(1).sort_values(ascending=False)\n",
    "countries_with_cases_before_responses = num_miss.where(num_miss==0).dropna().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using the endpoints of each interval is not going to work as well, because if the endpoints represent outliers then they\n",
    "will not capture the overall trend. Therefore, I will do the following: average the two intervals before and after the quarantine measure (average the cases/((1M people)(100k tests)) and then compare the averages with the value at the quarantine date. I believe this is fair because it's being applied equally to both intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_list = []\n",
    "slice_list = []\n",
    "\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    before_list = []\n",
    "    after_list = []\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "#         after = effective_range.reset_index().Date.max()\n",
    "        slice_list += [before]   \n",
    "        \n",
    "enacted_ended_df = pd.DataFrame(np.array(slice_list).reshape(len(country_list), -1), index=country_list, columns=all_responses.columns)\n",
    "\n",
    "all_responses = response_df.iloc[:, [0, 1, 2, 3, 5, 6]]\n",
    "country_list = []\n",
    "minmax_list = []\n",
    "for j, (country, country_df) in enumerate(all_responses.groupby(level=0)):\n",
    "    active_dates = country_df.replace(to_replace=0., value=np.nan)\n",
    "    country_list += [country]\n",
    "    for i, single_response in enumerate(active_dates.columns):\n",
    "        effective_range = active_dates[single_response].dropna(axis=0)\n",
    "        before = effective_range.reset_index().Date.min()\n",
    "        after = effective_range.reset_index().Date.max()\n",
    "        minmax_list += [before, after]   \n",
    "\n",
    "start_end_columns = np.array([[x+'_start', x+'_end'] for x in all_responses.columns.tolist()]).ravel()\n",
    "start_end_df = pd.DataFrame(np.array(minmax_list).reshape(len(country_list), -1), index=country_list, columns=start_end_columns)\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]\n",
    "start_end_filtered_df = start_end_df.drop(columns=['Close_public_transport_start','Close_public_transport_end']).dropna(axis=0)\n",
    "filtered_countries = start_end_filtered_df.index\n",
    "enacted_ended_filtered_df = enacted_ended_df.drop(columns=['Close_public_transport']).loc[filtered_countries, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = case_multiindex_df.join(test_multiindex_df, lsuffix='_x', rsuffix='_y').sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that total cases is a cumulative variable, replace zeros with np.nan and then backwards interpolate\n",
    "Growth rate calculations require values greater than zero, so remove all dates where there are zero confirmed cases, per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the time series, fill in with missing values with nan. \n",
    "data = data.reindex(pd.MultiIndex.from_product([data.index.levels[0], \n",
    "                    data.index.get_level_values(1).unique().sort_values()], names=['Country', 'Date']), fill_value=np.nan)\n",
    "\n",
    "# Don't use zeros this messes things up.\n",
    "data.loc[:, 'total_cases'] = data.loc[:, 'total_cases'].replace(to_replace=[0,0.], value=np.nan)\n",
    "# instantiate with copy so that we can iterate over DataFrame groupby\n",
    "data.loc[:, 'total_cases_interpolated'] = data.loc[:, 'total_cases'].copy()\n",
    "data.loc[:, 'tests_cumulative_interpolated'] = data.loc[:, 'tests_cumulative'].copy()\n",
    "\n",
    "for country, country_df in data.groupby(level=0):\n",
    "    data.loc[country, 'total_cases_interpolated'] = country_df.loc[:, 'total_cases'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'tests_cumulative_interpolated'] = country_df.loc[:, 'tests_cumulative'].interpolate(limit_direction='backward').values\n",
    "    data.loc[country, 'population'] = country_df.loc[:, 'population'].fillna(method='backfill')\n",
    "\n",
    "data.loc[:, 'cases_per_1M_people_per_100k_tests'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).values\n",
    "data.loc[:, 'cases_per_1M_people'] = (data.total_cases_interpolated / ((data.population/1000000.))).values\n",
    "\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / ((data.population/1000000.) * (data.tests_cumulative_interpolated))).cumsum().apply(np.log)\n",
    "\n",
    "before_minus_after = response_multiindex_df.applymap(multiindex_response_date_to_average_rates).replace(to_replace=0., value=np.nan).sort_index()\n",
    "\n",
    "before_minus_after_residual_values =  before_minus_after.values - np.tile(before_minus_after.mean(1).values.reshape(-1,1), (1, 5))\n",
    "before_minus_after_residual_df = pd.DataFrame(before_minus_after_residual_values.reshape(-1, 5), columns=before_minus_after.columns, index=before_minus_after.index)\n",
    "before_minus_after_residual_df.head()\n",
    "\n",
    "data.loc[:, 'cumulative_normalized_case_test_ratio'] = (data.total_cases_interpolated / data.tests_cumulative_interpolated).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
